
@misc{noauthor_deep_nodate,
	title = {Deep {Residual} {Learning} for {Image} {Recognition} {\textbar} {IEEE} {Conference} {Publication} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore-ieee-org.ezproxy2.lib.gla.ac.uk/document/7780459},
	urldate = {2024-03-26},
}

@article{sola_importance_1997,
	title = {Importance of input data normalization for the application of neural networks to complex industrial problems},
	volume = {44},
	issn = {1558-1578},
	url = {https://ieeexplore.ieee.org/document/589532},
	doi = {10.1109/23.589532},
	abstract = {Recent advances in artificial intelligence have allowed the application of such technologies in real industrial problems. We have studied the application of backpropagation neural networks to several problems of estimation and identification in nuclear power plants. These problems often have been reported to be very time-consuming in the training phase. Among the different approaches suggested to ease the backpropagation training process, input data pretreatment has been pointed out, although no specific procedure has been proposed. We have found that input data normalization with certain criteria, prior to a training process, is crucial to obtain good results as well as to fasten significantly the calculations. This paper shows how data normalization affects the performance error of parameter estimators trained to predict the value of several variables of a PWR nuclear power plant. The criteria needed to accomplish such data normalization are also described.},
	number = {3},
	urldate = {2024-03-26},
	journal = {IEEE Transactions on Nuclear Science},
	author = {Sola, J. and Sevilla, J.},
	month = jun,
	year = {1997},
	note = {Conference Name: IEEE Transactions on Nuclear Science},
	keywords = {Artificial intelligence, Artificial neural networks, Backpropagation, Inductors, Industrial training, Multilayer perceptrons, Neural networks, Parameter estimation, Power generation, Power measurement},
	pages = {1464--1468},
}

@article{doneus_openness_2013,
	title = {Openness as {Visualization} {Technique} for {Interpretative} {Mapping} of {Airborne} {Lidar} {Derived} {Digital} {Terrain} {Models}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/5/12/6427},
	doi = {10.3390/rs5126427},
	abstract = {Openness is proposed as a visualization technique for the archaeological interpretation of digital terrain models derived from airborne laser scanning. In contrast to various shading techniques, openness is not subject to directional bias and relief features highlighted by openness do not contain any horizontal displacement. Additionally, it offers a clear distinction between relief features and the surrounding topography, while it highlights both the highest and lowest parts of features. This makes openness an ideal tool for mapping and outlining of archaeological features. A comparison with sky-view factor and local relief model visualizations helps to evaluate advantages and limits of the technique.},
	language = {en},
	number = {12},
	urldate = {2024-03-26},
	journal = {Remote Sensing},
	author = {Doneus, Michael},
	month = dec,
	year = {2013},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Lidar, Openness, airborne laser scanning, digital terrain model, local relief model, sky-view factor, visualization},
	pages = {6427--6442},
}

@article{kokalj_why_2019,
	title = {Why {Not} a {Single} {Image}? {Combining} {Visualizations} to {Facilitate} {Fieldwork} and {On}-{Screen} {Mapping}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Why {Not} a {Single} {Image}?},
	url = {https://www.mdpi.com/2072-4292/11/7/747},
	doi = {10.3390/rs11070747},
	abstract = {Visualization products computed from a raster elevation model still form the basis of most archaeological and geomorphological enquiries of lidar data. We believe there is a need to improve the existing visualizations and create meaningful image combinations that preserve positive characteristics of individual techniques. In this paper, we list the criteria a good visualization should meet, present five different blend modes (normal, screen, multiply, overlay, luminosity), which combine various images into one, discuss their characteristics, and examine how they can be used to improve the visibility (recognition) of small topographical features. Blending different relief visualization techniques allows for a simultaneous display of distinct topographical features in a single (enhanced) image. We provide a “recipe” and a tool for a mix of visualization techniques and blend modes, including all the settings, to compute a visualization for archaeological topography that meets all of the criteria of a good visualization.},
	language = {en},
	number = {7},
	urldate = {2024-03-26},
	journal = {Remote Sensing},
	author = {Kokalj, Žiga and Somrak, Maja},
	month = jan,
	year = {2019},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {airborne laser scanning, blend modes, digital elevation model, lidar, relief mapping, visualization},
	pages = {747},
}

@misc{facebook_pytorchpytorch_2024,
	title = {pytorch/pytorch: {Tensors} and {Dynamic} neural networks in {Python} with strong {GPU} acceleration},
	url = {https://github.com/pytorch/pytorch},
	urldate = {2024-03-25},
	author = {Facebook},
	year = {2024},
}

@misc{maintainers_torchvision_2016,
	title = {{TorchVision}: {PyTorch}'s {Computer} {Vision} library},
	copyright = {BSD-3-Clause},
	shorttitle = {{TorchVision}},
	url = {https://github.com/pytorch/vision},
	abstract = {Datasets, Transforms and Models specific to Computer Vision},
	urldate = {2024-03-25},
	author = {maintainers, TorchVision and {contributors}},
	month = nov,
	year = {2016},
	note = {original-date: 2016-11-09T23:11:43Z},
}

@article{bonhage_modified_2021,
	title = {A modified {Mask} region-based convolutional neural network approach for the automated detection of archaeological sites on high-resolution light detection and ranging-derived digital elevation models in the {North} {German} {Lowland}},
	volume = {28},
	copyright = {© 2021 The Authors. Archaeological Prospection published by John Wiley \& Sons Ltd.},
	issn = {1099-0763},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/arp.1806},
	doi = {10.1002/arp.1806},
	abstract = {Due to complicated backgrounds and unclear target orientation, automated object detection is difficult in the field of archaeology. Most of the current convolutional neural network (CNN) object-oriented detection techniques are based on a faster region-based CNN (R-CNN) and other one-stage detectors that often lack adequate processing speeds and detection accuracies. Recently, the two-stage detector Mask R-CNN technique achieved impressive results in object detection and instance segmentation problems and was successfully applied in the analysis of archaeological airborne laser scanning (ALS) data. In this study, we outline a modified Mask R-CNN technique that reliably and efficiently detects relict charcoal hearth (RCH) sites on light detection and ranging (LiDAR) data-based digital elevation models (DEMs). Using image augmentation and image preprocessing steps combined with the deep learning-based adaptive gradient method with a dynamic bound on the learning rate (AdaBound) optimization technique, we could improve the model's accuracy and significantly reduce its training time. We use DEMs based on high-resolution LiDAR data and the visualization for archaeological topography (VAT) technique that give images with a very strong contrast of the terrain and the outline of the sites of interest in the North German Lowland. Therefore, the model can identify RCH sites with an average recall of 83\% and an average precision of 87\%. Techniques such as the modified Mask R-CNN method outlined here will help to greatly improve our knowledge about archaeological site densities in the realm of historical charcoal production and past human-landscape interactions. This method provides an accurate, time-efficient and bias-free large-scale site mapping option not only for the North German Lowland but potentially for other landscapes as well.},
	language = {en},
	number = {2},
	urldate = {2024-03-25},
	journal = {Archaeological Prospection},
	author = {Bonhage, Alexander and Eltaher, Mahmoud and Raab, Thomas and Breuß, Michael and Raab, Alexandra and Schneider, Anna},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/arp.1806},
	keywords = {Mask R-CNN, anthropogenic geomorphology, automated detection, charcoal hearth, deep learning, object detection},
	pages = {177--186},
}

@misc{huang_tiling_2019,
	title = {Tiling and {Stitching} {Segmentation} {Output} for {Remote} {Sensing}: {Basic} {Challenges} and {Recommendations}},
	shorttitle = {Tiling and {Stitching} {Segmentation} {Output} for {Remote} {Sensing}},
	url = {http://arxiv.org/abs/1805.12219},
	doi = {10.48550/arXiv.1805.12219},
	abstract = {In this work we consider the application of convolutional neural networks (CNNs) for pixel-wise labeling (a.k.a., semantic segmentation) of remote sensing imagery (e.g., aerial color or hyperspectral imagery). Remote sensing imagery is usually stored in the form of very large images, referred to as "tiles", which are too large to be segmented directly using most CNNs and their associated hardware. As a result, during label inference, smaller sub-images, called "patches", are processed individually and then "stitched" (concatenated) back together to create a tile-sized label map. This approach suffers from computational ineffiency and can result in discontinuities at output boundaries. We propose a simple alternative approach in which the input size of the CNN is dramatically increased only during label inference. This does not avoid stitching altogether, but substantially mitigates its limitations. We evaluate the performance of the proposed approach against a vonventional stitching approach using two popular segmentation CNN models and two large-scale remote sensing imagery datasets. The results suggest that the proposed approach substantially reduces label inference time, while also yielding modest overall label accuracy increases. This approach contributed to our wining entry (overall performance) in the INRIA building labeling competition.},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Huang, Bohao and Reichman, Daniel and Collins, Leslie M. and Bradbury, Kyle and Malof, Jordan M.},
	month = feb,
	year = {2019},
	note = {arXiv:1805.12219 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{devunuri_gtfs_2024,
	title = {{GTFS} {Segments}: {A} {Fast} and {Efficient} {Library} to {Generate} {Bus} {Stop} {Spacings}},
	volume = {9},
	issn = {2475-9066},
	shorttitle = {{GTFS} {Segments}},
	url = {https://joss.theoj.org/papers/10.21105/joss.06306},
	doi = {10.21105/joss.06306},
	abstract = {Devunuri et al., (2024). GTFS Segments: A Fast and Efficient Library to Generate Bus Stop Spacings. Journal of Open Source Software, 9(95), 6306, https://doi.org/10.21105/joss.06306},
	language = {en},
	number = {95},
	urldate = {2024-03-20},
	journal = {Journal of Open Source Software},
	author = {Devunuri, Saipraneeth and Lehe, Lewis},
	month = mar,
	year = {2024},
	pages = {6306},
}

@article{gerry_mccartney_what_2013,
	title = {What (or who) causes health inequalities: {Theories}, evidence and implications?},
	volume = {113},
	doi = {10.1016/j.healthpol.2013.05.021},
	abstract = {Health inequalities are the unjust differences in health between groups of people occupying different positions in society. Since the Black Report of 1980 there has been considerable effort to understand what causes them, so as to be able to identify actions to reduce them. This paper revisits and updates the proposed theories, evaluates the evidence in light of subsequent epidemiological research, and underlines the political and policy ramifications.},
	number = {3},
	journal = {Health Policy},
	author = {{Gerry McCartney} and McCartney, Gerry and {Chik Collins} and Collins, Charles and {Mhairi Mackenzie} and Mackenzie, Mhairi},
	month = dec,
	year = {2013},
	doi = {10.1016/j.healthpol.2013.05.021},
	pmid = {23810172},
	note = {MAG ID: 1987714877},
	pages = {221--227},
}

@article{gerry_mccartney_how_2023,
	title = {How important is it to avoid indices of deprivation that include health variables in analyses of health inequalities?},
	volume = {221},
	doi = {10.1016/j.puhe.2023.06.028},
	abstract = {This study aimed to quantify the difference in mortality inequalities using the Scottish Index of Multiple Deprivation (SIMD) and the Income and Employment Index (IEI; a subindex of SIMD, which excludes health) as ranking measures in Scotland. This ecological study was a cross-sectional analysis of routine administrative data. Data from the 2020 SIMD and the subindex using data from only the Income and Employment domains, the IEI, were obtained. The correlation between data zones, percentage of data zones that changed deprivation tenth and differences in the Slope Index of Inequality (SII) and Relative Index of Inequality (RII) for Standardised Mortality Ratios (SMRs) across tenths were compared when data zones were ranked by SIMD and IEI. There was a close correlation between data zones ranked by SIMD and IEI (R2 = 0.96). When data zones were ranked by IEI, 18.7\% of data zones moved to a lower deprivation tenth, and 20.8\% of data zones moved to a higher deprivation tenth, compared with SIMD. However, only a negligible number of data zones moved two or more tenths. The SMRs across deprivation tenths were very similar between the SIMD and IEI, as were the summary health inequality measures of SII (87.3 compared with 85.7) and RII (0.88 and 0.86). Although there is a logical problem in using deprivation indices that include health outcomes to rank areas to calculate the scale of health inequalities, the impact of using an alternative subindex containing only data from the income and employment domains is minimal. For population-wide analyses of health inequalities in Scotland, the SIMD does not introduce a substantial bias in the health inequalities summary measures despite substantial movement of small areas between ranked population tenths. Although not examined here, this is likely to be relevant to other similar indices across the United Kingdom.},
	author = {{Gerry McCartney} and {R Hoggett} and {David Walsh} and {D Lee}},
	month = aug,
	year = {2023},
	doi = {10.1016/j.puhe.2023.06.028},
	pmid = {37473649},
	note = {MAG ID: 4384923137},
	pages = {175--180},
}

@article{andrew_sayer_economic_2021,
	title = {Economic relationships and health inequalities: improving public health recommendations},
	volume = {199},
	doi = {10.1016/j.puhe.2021.08.017},
	abstract = {Abstract   Policy recommendations, which aim to reduce health inequalities in society, often focus upon improving the incomes, working conditions and physical environments of the most deprived groups. We agree with these recommendations but argue that they are insufficient to reduce health inequalities because they fail to address the economic relationships between social groups that lead to health inequalities and which perpetuate them over time. A comprehensive programme to reduce health inequalities will require policies that address the numerous ways in which economic resources flow from poorer groups to richer groups through the design of the economy. In this commentary we describe key economic relationships between social groups that lead to inequalities, namely rent, interest, capital gains, profit, monopoly and speculation. Addressing these causes of economic inequality in recommendations to reduce health inequalities should be considered by future research in this area.},
	journal = {Public Health},
	author = {{Andrew Sayer} and {A. Sayer} and Sayer, Andrew and {Gerry McCartney} and McCartney, Gerry},
	month = oct,
	year = {2021},
	doi = {10.1016/j.puhe.2021.08.017},
	pmid = {34583201},
	note = {MAG ID: 3203318602},
	pages = {103--106},
}

@article{philip_mcloone_targeting_2001,
	title = {Targeting deprived areas within small areas in {Scotland}: population study.},
	volume = {323},
	doi = {10.1136/bmj.323.7309.374},
	abstract = {Geographical measures of deprivation show wide variations in the socioeconomic characteristics of populations who live in small areas. This variation has led governments over the years to target deprived areas within these small areas with the aim of improving the residents' circumstances. A large number of initiatives based on such areas, including health action zones, employment zones, and social inclusion partnerships, have recently been introduced in the United Kingdom. In Scotland, some health boards target resources towards areas at the most deprived extreme of the Carstairs deprivation scale, which ranges from −7.5 (most affluent) to 12.9 (most deprived).1

In 1979, Townsend argued that an area based approach should not be central to improving the conditions of people in poverty.2 Using Holtermann's earlier work,3 Townsend concluded that the spatial concentration of aspects of deprivation could be low. …},
	number = {7309},
	journal = {BMJ},
	author = {{Philip McLoone} and McLoone, Philip},
	month = aug,
	year = {2001},
	doi = {10.1136/bmj.323.7309.374},
	pmcid = {37397},
	pmid = {11509428},
	note = {MAG ID: 2015922025},
	pages = {374--375},
}

@article{gerry_mccartney_how_2023-1,
	title = {How well do area-based deprivation indices identify income- and employment-deprived individuals across {Great} {Britain} today?},
	volume = {217},
	doi = {10.1016/j.puhe.2023.01.020},
	abstract = {Area-based deprivation indices are used in many countries to target interventions and policies to populations with the greatest needs. Analyses of the Carstairs deprivation index applied to postcode sectors in 2001 identified that less than half of all deprived individuals lived in the most deprived areas.This article examines the specificity and sensitivity of deprivation indices across Great Britain in identifying individuals claiming income- and employment-related social security benefits.This was a descriptive analysis of cross-sectional administrative data.The data sets for the 2020 Scottish Index of Multiple Deprivation, Scottish Income and Employment Index, the 2019 English Index of Multiple Deprivation and the 2019 Welsh Index of Multiple Deprivation were obtained. For each data set, small areas were ranked by increasing overall deprivation, and the cumulative proportions of individuals who were income and employment deprived were calculated. Receiver operating characteristic curves were plotted to show the sensitivity and specificity of each index, and the percentages of income- and employment-deprived individuals captured at different overall deprivation thresholds were calculated.Across all indices, the sensitivity and specificity for detecting income- and employment-deprived individuals were low, with less than half living in the most deprived 20\% of areas. Between 55\% and 62\% of income-deprived people and between 56\% and 63\% of employment-deprived people were missed across the indices at the 20\% deprivation threshold. The sensitivity and specificity were slightly higher for income deprivation than employment deprivation across indices and slightly higher for the Scottish Index of Multiple Deprivation and Scottish Income and Employment Index than for the English Index of Multiple Deprivation and Welsh Index of Multiple Deprivation.Area-based deprivation measures in Great Britain have limited sensitivity and specificity for identifying individuals who are income or employment deprived. Place-based policies and interventions are unlikely to be effective at reducing inequalities as a result. Creation of individually linked data sets and interventions that recognise the social and economic relationships between social groups are likely to be more effective.},
	author = {{Gerry McCartney} and {R Hoggett} and {David Walsh} and {D Lee}},
	month = apr,
	year = {2023},
	doi = {10.1016/j.puhe.2023.01.020},
	pmid = {36841035},
	note = {MAG ID: 4321768014},
	pages = {22--25},
}

@article{ruslan_sadyrtdinov_social_2023,
	title = {Social stratification of households in the context of the digital divide},
	volume = {431},
	doi = {10.1051/e3sconf/202343107034},
	abstract = {To study issues of social deprivation and inequality, scientists explore factors, correlations and various indices and models. The deprivation indices are used in many countries to target interventions and policies to populations with the greatest needs. The aim of this research is to construct stratification scale of Russian households for the period of Covid-19 pandemic to study their social deprivation. The data source for household social deprivation is the Russian Longitudinal Monitoring Survey (RLMS-HSE). The results show that the digital divide of households in extreme poverty greatly increases their social exclusion and increases social deprivation. About 50\% of extremely poor households live in rural areas. It confirms the ongoing income stratification of urban and rural residents. About 30\% of households do not own a car. In the context of the restrictions of the self-isolation regime, this is a significant factor in social exclusion. Low incomes and material deprivation do not allow such households to change this situation. Even not all rich households have access to high-speed Internet. This may be due to the underdevelopment of high-speed Internet infrastructure.},
	author = {{Ruslan Sadyrtdinov} and {Svetlana Vladimirova}},
	month = jan,
	year = {2023},
	doi = {10.1051/e3sconf/202343107034},
	note = {MAG ID: 4387625423},
	pages = {07034--07034},
}

@article{mccartney_how_2023,
	title = {How well does the {Scottish} {Index} of {Multiple} {Deprivation} identify income and employment deprived individuals across the urban-rural spectrum and between local authorities?},
	volume = {217},
	issn = {0033-3506},
	url = {https://www.sciencedirect.com/science/article/pii/S0033350623000185},
	doi = {10.1016/j.puhe.2023.01.009},
	abstract = {Background
Area-based indices of deprivation are used to identify populations at need, to inform service planning and policy, to rank populations for monitoring trends in inequalities, and to evaluate the impacts of interventions. There is scepticism of the utility of area deprivation indices in rural areas because of the spatial heterogeneity of their populations.
Objective
To compare the sensitivity of the Scottish Index of Multiple Deprivation (SIMD) for detecting income and employment deprived individuals by urban-rural classification and across local authorities.
Study design
Descriptive analysis of cross-sectional data.
Methods
Data from the 2020 Scottish Index of Multiple Deprivation (SIMD) were used to calculate the number and percentage of income and employment deprived people missed within each of the six-fold urban-rural classification strata and each local authority using areas ranked by the national SIMD, within local authority rankings, and within urban-rural strata rankings, for deprivation thresholds between the 5\% most deprived areas and the 30\% most deprived areas. The Slope Index of Inequality (SII) and Relative Index of Inequality (RII) were calculated within local authorities and urban-rural classification strata to estimate the concentration of deprivation within ranked data zones.
Results
The number and percentage of income and employment deprived people is higher in urban than rural areas. However, using the national, local authority, and within urban-rural classification strata rankings of SIMD, and under all deprivation thresholds (from the 5\%–30\% most deprived areas), the percentage of income and employment deprived people missed by targeting the most deprived areas within urban-rural strata is higher in more remote and rural areas, and in island local authorities. The absolute number of income and employment deprived individuals is greater in urban areas across rankings and thresholds.
Conclusion
The SIMD misses a higher percentage of income and employment deprived people in remote, rural and island areas across deprivation thresholds and irrespective of whether national, local or within urban-rural classification strata are used. However, the absolute number of people missed is higher in urban areas.},
	urldate = {2024-03-15},
	journal = {Public Health},
	author = {McCartney, G. and Hoggett, R.},
	month = apr,
	year = {2023},
	keywords = {Employment, Income, Rurality, Scottish Index of Multiple Deprivation},
	pages = {26--32},
}

@incollection{kulkarni_building_2022,
	address = {Berkeley, CA},
	title = {Building an {Object} {Detection} {Model}},
	isbn = {978-1-4842-8272-4 978-1-4842-8273-1},
	url = {https://link.springer.com/10.1007/978-1-4842-8273-1_3},
	language = {en},
	urldate = {2024-03-08},
	booktitle = {Computer {Vision} {Projects} with {PyTorch}},
	publisher = {Apress},
	author = {Kulkarni, Akshay and Shivananda, Adarsha and Sharma, Nitin Ranjan},
	collaborator = {Kulkarni, Akshay and Shivananda, Adarsha and Sharma, Nitin Ranjan},
	year = {2022},
	doi = {10.1007/978-1-4842-8273-1_3},
	pages = {85--128},
}

@article{takatsuka_geovista_2002,
	title = {{GeoVISTA} {Studio}: a codeless visual programming environment for geoscientific data analysis and visualization},
	volume = {28},
	issn = {0098-3004},
	shorttitle = {{GeoVISTA} {Studio}},
	url = {https://ui.adsabs.harvard.edu/abs/2002CG.....28.1131T},
	doi = {10.1016/S0098-3004(02)00031-6},
	abstract = {The fundamental goal of the GeoVISTA Studio project is to improve geoscientific analysis by providing an environment that operationally integrates a wide range of analysis activities, including those both computationally and visually based. Improving the infrastructure used in analysis has far-reaching potential to better integrate human-based and computationally based expertise, and so ultimately improve scientific outcomes. To address these challenges, some difficult system design and software engineering problems must be overcome. This paper illustrates the design of a component-oriented system, GeoVISTA Studio, as a means to overcome such difficulties by using state-of-the-art component-based software engineering techniques. Advantages described include: ease of program construction (visual programming), an open (non-proprietary) architecture, simple component-based integration and advanced deployment methods. This versatility has the potential to change the nature of systems development for the geosciences, providing better mechanisms to coordinate complex functionality, and as a consequence, to improve analysis by closer integration of software tools and better engagement of the human expert. Two example applications are presented to illustrate the potential of the Studio environment for exploring and better understanding large, complex geographical datasets and for supporting complex visual and computational analysis.},
	urldate = {2024-03-07},
	journal = {Computers and Geosciences},
	author = {Takatsuka, Masahiro and Gahegan, Mark},
	month = dec,
	year = {2002},
	note = {ADS Bibcode: 2002CG.....28.1131T},
	pages = {1131--1144},
}

@article{weaver_visual_2007,
	title = {Visual {Exploration} and {Analysis} of {Historic} {Hotel} {Visits}},
	volume = {6},
	issn = {1473-8716},
	url = {https://doi.org/10.1057/palgrave.ivs.9500145},
	doi = {10.1057/palgrave.ivs.9500145},
	abstract = {Understanding the spatial and temporal characteristics of individual and group behavior in social networks is a critical component of visual tools for intelligence analysis, emergency management, consumer analysis, and human geography. Identification and analysis of patterns of recurring events is an essential feature of such tools. In this paper, we describe an interactive visual tool for exploring the visitation patterns of guests at two hotels in central Pennsylvania from 1894 to 1900. The centerpiece of the tool is a wrapping spreadsheet technique, called reruns, that reveals regular and irregular periodic patterns of events in multiple overlapping artificial and natural calendars. Implemented as a coordinated multiple view visualization in Improvise, the tool is in ongoing development through an iterative process of data collection, transcription, hypothesis, design, discovery, analysis, and evaluation in close collaboration with historical geographers. Numerous discoveries have driven additional data collection from archival newspaper and census sources, as well as plans to enhance analysis of spatial patterns using historic weather records and railroad schedules. Distributed online evaluations of usability and usefulness have resulted in feature and design recommendations that are being incorporated into the tool.},
	language = {en},
	number = {1},
	urldate = {2024-03-07},
	journal = {Information Visualization},
	author = {Weaver, Chris and Fyfe, David and Robinson, Anthony and Holdsworth, Deryck and Peuquet, Donna and MacEachren, Alan M.},
	month = mar,
	year = {2007},
	note = {Publisher: SAGE Publications},
	pages = {89--103},
}

@article{elliot_53_2017,
	title = {53 {Ways} to {Enhance} {Researcher} {Development53} {Ways} to {Enhance} {Researcher} {Development} {Edited} by {Daley} {Rob}, {Guccione} {Kay} and {Hutchinson} {Steve} {Suffolk} {Frontinus} {Ltd}., 2017 208 p. {ISBN} 978-1-907076-95-4},
	volume = {8},
	doi = {10.1108/SGPE-D-17-00037},
	journal = {Studies in Graduate and Postdoctoral Education},
	author = {Elliot, Dely Lazarte},
	month = nov,
	year = {2017},
	pages = {220--223},
}

@article{kandel_research_2011,
	title = {Research directions in data wrangling: {Visualizations} and transformations for usable and credible data},
	volume = {10},
	issn = {1473-8716},
	shorttitle = {Research directions in data wrangling},
	url = {https://doi.org/10.1177/1473871611415994},
	doi = {10.1177/1473871611415994},
	abstract = {In spite of advances in technologies for working with data, analysts still spend an inordinate amount of time diagnosing data quality issues and manipulating data into a usable form. This process of ‘data wrangling’ often constitutes the most tedious and time-consuming aspect of analysis. Though data cleaning and integration arelongstanding issues in the database community, relatively little research has explored how interactive visualization can advance the state of the art. In this article, we review the challenges and opportunities associated with addressing data quality issues. We argue that analysts might more effectively wrangle data through new interactive systems that integrate data verification, transformation, and visualization. We identify a number of outstanding research questions, including how appropriate visual encodings can facilitate apprehension of missing data, discrepant values, and uncertainty; how interactive visualizations might facilitate data transform specification; and how recorded provenance and social interaction might enable wider reuse, verification, and modification of data transformations.},
	language = {en},
	number = {4},
	urldate = {2024-03-05},
	journal = {Information Visualization},
	author = {Kandel, Sean and Heer, Jeffrey and Plaisant, Catherine and Kennedy, Jessie and van Ham, Frank and Riche, Nathalie Henry and Weaver, Chris and Lee, Bongshin and Brodbeck, Dominique and Buono, Paolo},
	month = oct,
	year = {2011},
	note = {Publisher: SAGE Publications},
	pages = {271--288},
}

@article{cao_z-glyph_2018,
	title = {Z-{Glyph}: {Visualizing} outliers in multivariate data},
	volume = {17},
	issn = {1473-8716},
	shorttitle = {Z-{Glyph}},
	url = {https://doi.org/10.1177/1473871616686635},
	doi = {10.1177/1473871616686635},
	abstract = {Outlier analysis techniques are extensively used in many domains such as intrusion detection. Today, even with the most advanced statistical learning techniques, human judgment still plays an important role in outlier analysis tasks due to the difficulty of defining and collecting outlier examples. This work seeks to tackle this problem by introducing a new visualization design, “Z-Glyph,” a family of glyphs designed to facilitate human judgment in outlier analysis of multivariate data. By employing a location-scale transformation, a Z-Glyph represents the “normal” data using regular shapes (e.g. straight line and circle), such that the abnormal data can be revealed when deviating from the regular shapes. Extensive controlled experiment and case studies based on real-world datasets indicate the superior performance of the Z-Glyph family, compared with the baselines, suggesting that the proposed design is able to leverage human perceptional features with statistical characterization. This study contributes to a more fundamental understanding about designing visual representations for revealing outliers in multivariate data, which can be applied as a building block in many domain-specific anomaly detection applications.},
	language = {en},
	number = {1},
	urldate = {2024-03-05},
	journal = {Information Visualization},
	author = {Cao, Nan and Lin, Yu-Ru and Gotz, David and Du, Fan},
	month = jan,
	year = {2018},
	note = {Publisher: SAGE Publications},
	pages = {22--40},
}

@article{laurier_graphic_2014,
	title = {The {Graphic} {Transcript}: {Poaching} {Comic} {Book} {Grammar} for {Inscribing} the {Visual}, {Spatial} and {Temporal} {Aspects} of {Action}},
	volume = {8},
	copyright = {© 2014 The Author(s). Geography Compass © 2014 John Wiley \& Sons Ltd},
	issn = {1749-8198},
	shorttitle = {The {Graphic} {Transcript}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/gec3.12123},
	doi = {10.1111/gec3.12123},
	abstract = {A brief review is presented of existing forms of transcription of talk that incorporate visual, spatial and temporal elements. The most common forms use text and a line-by-line-based system, and conversation analytic transcripts have been successful in making a number of other features of talk visible. The desire of geographers to draw upon video recordings and time-lapse photography has lead to time-series images being used to bring those visual materials into documents. The graphic transcript is proposed as an alternative form of transcription that hybridises the qualities and evidentiary criteria of the transcript with the representational conventions of the comic strip. The comic strip itself has recently been undergoing a period of experimentation and hybridisation with other forms. The graphic transcript brings together familiar comic strip features such as panels, guttering, speech bubbles and captions with the transcript's criteria of providing an evidentiary record of earlier events that is available for re-inspection and re-interpretation by other analysts.},
	language = {en},
	number = {4},
	urldate = {2024-03-05},
	journal = {Geography Compass},
	author = {Laurier, Eric},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/gec3.12123},
	pages = {235--248},
}

@article{kay_ggdist_2024,
	title = {ggdist: {Visualizations} of {Distributions} and {Uncertainty} in the {Grammar} of {Graphics}},
	volume = {30},
	issn = {1941-0506},
	shorttitle = {ggdist},
	url = {https://ieeexplore.ieee.org/document/10297592},
	doi = {10.1109/TVCG.2023.3327195},
	abstract = {The grammar of graphics is ubiquitous, providing the foundation for a variety of popular visualization tools and toolkits. Yet support for uncertainty visualization in the grammar graphics—beyond simple variations of error bars, uncertainty bands, and density plots—remains rudimentary. Research in uncertainty visualization has developed a rich variety of improved uncertainty visualizations, most of which are difficult to create in existing grammar of graphics implementations. ggdist, an extension to the popular ggplot2 grammar of graphics toolkit, is an attempt to rectify this situation. ggdist unifies a variety of uncertainty visualization types through the lens of distributional visualization, allowing functions of distributions to be mapped to directly to visual channels (aesthetics), making it straightforward to express a variety of (sometimes weird!) uncertainty visualization types. This distributional lens also offers a way to unify Bayesian and frequentist uncertainty visualization by formalizing the latter with the help of confidence distributions. In this paper, I offer a description of this uncertainty visualization paradigm and lessons learned from its development and adoption: ggdist has existed in some form for about six years (originally as part of the tidybayes R package for post-processing Bayesian models), and it has evolved substantially over that time, with several rewrites and API re-organizations as it changed in response to user feedback and expanded to cover increasing varieties of uncertainty visualization types. Ultimately, given the huge expressive power of the grammar of graphics and the popularity of tools built on it, I hope a catalog of my experience with ggdist will provide a catalyst for further improvements to formalizations and implementations of uncertainty visualization in grammar of graphics ecosystems. A free copy of this paper is available at https://osf.io/2gsz6. All supplemental materials are available at https://github.com/mjskay/ggdist-paper and are archived on Zenodo at doi:10.5281/zenodo.7770984.},
	number = {1},
	urldate = {2024-03-05},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Kay, Matthew},
	month = jan,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Bayes methods, Data visualization, Geometry, Grammar, Standards, Uncertainty, Uncertainty visualization, Visualization, confidence distributions, grammar of graphics, probability distributions},
	pages = {414--424},
}

@book{noauthor_grammar_2005,
	address = {New York},
	series = {Statistics and {Computing}},
	title = {The {Grammar} of {Graphics}},
	isbn = {978-0-387-24544-7},
	url = {http://link.springer.com/10.1007/0-387-28695-0},
	language = {en},
	urldate = {2024-03-05},
	publisher = {Springer-Verlag},
	year = {2005},
	doi = {10.1007/0-387-28695-0},
	keywords = {Data Visualization, geometry, presentation, statistical software, visualization},
}

@article{wills_autovis_2010,
	title = {{AutoVis}: {Automatic} {Visualization}},
	volume = {9},
	issn = {1473-8716},
	shorttitle = {{AutoVis}},
	url = {https://doi.org/10.1057/ivs.2008.27},
	doi = {10.1057/ivs.2008.27},
	abstract = {AutoVis is a data viewer that responds to content–text, relational tables, hierarchies, streams, images–and displays the information appropriately (that is, as an expert would). Its design rests on the grammar of graphics, scagnostics and a modeler based on the logic of statistical analysis. We distinguish an automatic visualization system (AVS) from an automated visualization system. The former automatically makes decisions about what is to be visualized. The latter is a programming system for automating the production of charts, graphs and visualizations. An AVS is designed to provide a first glance at data before modeling and analysis are done. AVS is designed to protect researchers from ignoring missing data, outliers, miscodes and other anomalies that can violate statistical assumptions or otherwise jeopardize the validity of models. The design of this system incorporates several unique features: (1) a spare interface–analysts simply drag a data source into an empty window, (2) a graphics generator that requires no user definitions to produce graphs, (3) a statistical analyzer that protects users from false conclusions, and (4) a pattern recognizer that responds to the aspects (density, shape, trend, and so on) that professional statisticians notice when investigating data sets.},
	language = {en},
	number = {1},
	urldate = {2024-03-05},
	journal = {Information Visualization},
	author = {Wills, Graham and Wilkinson, Leland},
	month = jan,
	year = {2010},
	note = {Publisher: SAGE Publications},
	pages = {47--69},
}

@article{byrohl_scida_2024,
	title = {scida: scalable analysis for scientific big data},
	volume = {9},
	issn = {2475-9066},
	shorttitle = {scida},
	url = {https://joss.theoj.org/papers/10.21105/joss.06064},
	doi = {10.21105/joss.06064},
	abstract = {Byrohl et al., (2024). scida: scalable analysis for scientific big data. Journal of Open Source Software, 9(94), 6064, https://doi.org/10.21105/joss.06064},
	language = {en},
	number = {94},
	urldate = {2024-02-29},
	journal = {Journal of Open Source Software},
	author = {Byrohl, Chris and Nelson, Dylan},
	month = feb,
	year = {2024},
	pages = {6064},
}

@inproceedings{hosseini_mapreader_2022,
	address = {Seattle Washington},
	title = {{MapReader}: a computer vision pipeline for the semantic exploration of maps at scale},
	isbn = {978-1-4503-9533-5},
	shorttitle = {{MapReader}},
	url = {https://dl.acm.org/doi/10.1145/3557919.3565812},
	doi = {10.1145/3557919.3565812},
	language = {en},
	urldate = {2024-02-29},
	booktitle = {Proceedings of the 6th {ACM} {SIGSPATIAL} {International} {Workshop} on {Geospatial} {Humanities}},
	publisher = {ACM},
	author = {Hosseini, Kasra and Wilson, Daniel C. S. and Beelen, Kaspar and McDonough, Katherine},
	month = nov,
	year = {2022},
	keywords = {historical maps, neural networks},
	pages = {8--19},
}

@article{hosseini_maps_2021,
	title = {Maps of a {Nation}? {The} {Digitized} {Ordnance} {Survey} for {New} {Historical} {Research}},
	volume = {26},
	issn = {1355-5502, 1750-0133},
	shorttitle = {Maps of a {Nation}?},
	url = {https://academic.oup.com/jvc/article/26/2/284/6232245},
	doi = {10.1093/jvcult/vcab009},
	abstract = {Abstract
            Although the Ordnance Survey has itself been the subject of historical research, scholars have not systematically used its maps as primary sources of information. This is partly for disciplinary reasons and partly for the technical reason that high-quality maps have not until recently been available digitally, geo-referenced, and in color. A final, and crucial, addition has been the creation of item-level metadata which allows map collections to become corpora which can for the first time be interrogated en masse as source material. By applying new Computer Vision methods leveraging machine learning, we outline a research pipeline for working with thousands (rather than a handful) of maps at once, which enables new forms of historical inquiry based on spatial analysis. Our ‘patchwork method’ draws on the longstanding desire to adopt an overall or ‘complete’ view of a territory, and in so doing highlights certain parallels between the situation faced by today’s users of digitized maps, and a similar inflexion point faced by their predecessors in the nineteenth century, as the project to map the nation approached a form of completion.},
	language = {en},
	number = {2},
	urldate = {2024-02-29},
	journal = {Journal of Victorian Culture},
	author = {Hosseini, Kasra and McDonough, Katherine and Van Strien, Daniel and Vane, Olivia and Wilson, Daniel C S},
	month = may,
	year = {2021},
	pages = {284--299},
}

@incollection{golledge_human_2003,
	title = {{HUMAN} {WAYFINDING} {AND} {COGNITIVE} {MAPS}},
	url = {https://www.taylorfrancis.com/chapters/edit/10.4324/9780203422908-13/human-wayfinding-cognitive-maps-reginald-golledge},
	abstract = {Wayfinding refers to the ability to determine a route, learn it, and retrace or reverse it
from memory. Wayfinding is universal to all cultures. It is involved in a myriad of
daily and longer-term episodic activities ranging from a search of local areas for food
sources to the large-scale and long-term international migrations that first populated
the world. Fundamental scenarios in which wayfinding takes place include (a)
wandering in search of, then finding and settling in, a new home environment; (b)
situations where the ultimate intent is to return home after traveling; (c) episodic food
searches; and (d) travel from and to home to achieve a specific purpose (e.g. health,
safety, recreation, socialization, communication, and interaction). For the successful
completion of a wayfinding trip, people must acquire and use environmental knowledge.},
	booktitle = {The {Colonization} of {Unfamiliar} {Landscapes}},
	author = {Golledge, Reginald G.},
	month = dec,
	year = {2003},
	doi = {10.4324/9780203422908-13},
	note = {MAG ID: 92077514},
	pages = {49--54},
}

@book{alan_m_maceachren_how_1995,
	title = {How {Maps} {Work}: {Representation}, {Visualization}, and {Design}},
	abstract = {Part 1 How meaning is derived from maps: taking a scientific approach in improving map representation and design an information processing view of vision and visual cognition - cartographic implications how maps are seen how maps are understood. Part 2 How maps are imbued with meaning: a Primer On Semiotics For Understanding Map Representation A Functional approach to map representation - semantics and syntactics of map signs a lexical approach to map representation - map pragmatics. Part 3 How maps are used - applications in geographical visualization: GVIS - facilitating visual thinking GVIS - relationship in space and time GVIS - should we believe what we see.},
	author = {{Alan M. MacEachren} and MacEachren, Alan M.},
	month = jun,
	year = {1995},
	note = {MAG ID: 1506583902},
}

@article{opach_choropleth_2014,
	title = {Do choropleth maps linked with parallel coordinates facilitate an understanding of multivariate spatial characteristics},
	volume = {41},
	doi = {10.1080/15230406.2014.953585},
	abstract = {Our study has three objectives. We want to investigate (1) whether choropleth maps linked with parallel coordinates help people understand the locations of vulnerable places and the factors making these places vulnerable, (2) whether sparklines that imitate the polylines from a parallel coordinate plot support the understanding of the information provided in that plot, and (3) whether a multiple-view geovisualization approach might be intuitive and useful also for nonexperts. Although we base our work on the functionalities available in the tool called ‘ViewExposed,’ we intend to outline more general conclusions on whether multiple linked views facilitate the understanding of multivariate spatial characteristics. An empirical study with 53 individuals was conducted to obtain insights on these objectives. Our task-based assessment considered the ways in which participants understood the dynamic linking capabilities. Some of the key findings are as follows: (1) even nonexpert users are able to use parallel ...},
	number = {5},
	journal = {Cartography and Geographic Information Science},
	author = {Opach, Tomasz and Rød, Jan Ketil},
	month = oct,
	year = {2014},
	doi = {10.1080/15230406.2014.953585},
	note = {MAG ID: 2067861124},
	pages = {413--429},
}

@phdthesis{robinson_design_2008,
	address = {University Park, PA},
	title = {Design for {Synthesis} in {Geovisualization}},
	abstract = {Visually-enabled analysis of geographic information with interactive geovisualization tools is increasingly common in domains like disease surveillance, crisis management, and intelligence analysis. As geovisualization tools evolve to support more sophisticated analytical capabilities, the results that emerge from these systems are becoming more abundant and intricate. Current tools provide basic mechanisms for collecting, organizing, and making sense out of multiple results, but little basic research has been done to characterize this task – the synthesis of geovisual information.

This study explores the topic of synthesis in the context of infectious disease surveillance. Expert analysts from the Pacific Northwest National Laboratory (PNNL) and experts from the Penn State Center for Infectious Disease Dynamics (CIDD), and the Geographic Visualization Science, Technology, and Applications Center (GeoVISTA) were recruited to take part in interviews and experiments to characterize geovisual synthesis. These participants are likely to use, or are already using geovisualization tools to develop analytical results – therefore they stand to benefit from new synthesis support tools. 

This research employs a mixed qualitative method study to characterize and design for geovisual synthesis. Interviews were conducted with analysts at PNNL to characterize how synthesis is conducted currently and to elicit opinions about how synthesis should be supported in the future. Individual and collaborative synthesis experiments were conducted with participants from PNNL, CIDD, and GeoVISTA to observe synthesis in a simulated real-world scenario. Analysis of experimental and interview data provides insight into the process of geovisual synthesis. Results show that synthesis involves the application of a wide range of organizational metaphors, and that it requires flexible tools that support creative approaches. These results are distilled into a set of empirically-derived design guidelines for new synthesis support tools.},
	school = {Pennsylvania State University},
	author = {Robinson, Anthony C.},
	month = jan,
	year = {2008},
	note = {MAG ID: 2304661400},
}

@article{openshaw_building_1990,
	title = {Building a prototype {Geographical} {Correlates} {Exploration} {Machine}},
	volume = {4},
	doi = {10.1080/02693799008941548},
	abstract = {The paper describes a exploratory procedure for data analysis for use within GIS. The objective is to search digital map databases for the presence of geographical relationships that may be useful for descriptive purposes, as a pointer towards areas for further investigation, and as a means of generating hypotheses for subsequent testing. A prototype Geographical Correlates Exploration Machine (GCEM) is demonstrated by searching for possible linkages between children with leukaemia and a selection of environmental coverages. Arc Info is used for the GIS parts and a Cray X-MP/48 supercomputer for the analysis. Ultimately, it is envisaged that GCEM will be able to run entirely within a GIS workstation environment.},
	number = {3},
	journal = {International Journal of Geographic Information Systems},
	author = {Openshaw, Stan and Cross, Anna and Charlton, Martin},
	month = jul,
	year = {1990},
	doi = {10.1080/02693799008941548},
	note = {MAG ID: 2094423578},
	pages = {297--311},
}

@article{fellner_automatic_2006,
	title = {Automatic visualisation of metro maps},
	volume = {17},
	doi = {10.1016/j.jvlc.2005.09.001},
	abstract = {We investigate the new problem of automatic metro map layout. In general, a metro map consists of a set of lines which have intersections or overlaps. We define a set of aesthetic criteria for good metro map layouts and present a method to produce such layouts automatically. Our method uses a variation of the spring algorithm with a suitable preprocessing step. The experimental results with real world data sets show that our method produces good metro map layouts quickly.},
	number = {3},
	journal = {Journal of Visual Languages and Computing},
	author = {Fellner, Dieter W. and Hong, Seok-Hee and Merrick, Damian and Nascimento, Hugo A. D. do},
	month = jun,
	year = {2006},
	doi = {10.1016/j.jvlc.2005.09.001},
	note = {MAG ID: 2078790659},
	pages = {203--224},
}

@article{stott_automatic_2011,
	title = {Automatic {Metro} {Map} {Layout} {Using} {Multicriteria} {Optimization}},
	volume = {17},
	doi = {10.1109/tvcg.2010.24},
	abstract = {This paper describes an automatic mechanism for drawing metro maps. We apply multicriteria optimization to find effective placement of stations with a good line layout and to label the map unambiguously. A number of metrics are defined, which are used in a weighted sum to find a fitness value for a layout of the map. A hill climbing optimizer is used to reduce the fitness value, and find improved map layouts. To avoid local minima, we apply clustering techniques to the map-the hill climber moves both stations and clusters when finding improved layouts. We show the method applied to a number of metro maps, and describe an empirical study that provides some quantitative evidence that automatically-drawn metro maps can help users to find routes more efficiently than either published maps or undistorted maps. Moreover, we have found that, in these cases, study subjects indicate a preference for automatically-drawn maps over the alternatives.},
	number = {1},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Stott, Jonathan and Rodgers, Peter and Martínez-Ovando, Juan Carlos and Walker, Stephen G.},
	month = jan,
	year = {2011},
	doi = {10.1109/tvcg.2010.24},
	pmid = {21071790},
	note = {MAG ID: 2091324757},
	pages = {101--114},
}

@article{ware_automated_2006,
	title = {Automated {Production} of {Schematic} {Maps} for {Mobile} {Applications}},
	volume = {10},
	doi = {10.1111/j.1467-9671.2006.00242.x},
	abstract = {The advent of high-end miniature technology, together with the increasing availability of large scale digital geographic data products, has created a demand for techniques and methodologies that assist in the automated generation of maps specifically tailored to mobile GIS applications. This paper concerns itself with the problem of automatic generation of schematic maps. Schematic maps are diagrammatic representations based on linear abstractions of networks. In the context of mobile mapping they are seen as being a particularly useful means of displaying transportation networks. This paper describes an algorithm that automates the production of schematic maps. The algorithm makes use of the simulated annealing optimisation technique. An implementation of the algorithm is also presented, together with experimental results.},
	number = {1},
	journal = {Transactions in Gis},
	author = {Ware, J. Mark and Taylor, George and Anand, Suchith and Thomas, Nathan},
	month = jan,
	year = {2006},
	doi = {10.1111/j.1467-9671.2006.00242.x},
	note = {MAG ID: 2020239183},
	pages = {25--42},
}

@article{alan_m_maceachren_evolution_1987,
	title = {The {Evolution}, {Application} and {Implications} of {Strip} {Format} {Travel} {Maps}},
	volume = {24},
	doi = {10.1179/caj.1987.24.2.147},
	abstract = {AbstractStrip format maps have been used as an aid in travel throughout recorded history. The evolution of this map format from Roman times to the present is examined with emphasis on the interaction of map form and function. Particular attention is given to the range in abstractness of strip maps when used with different travel modes or applied to different kinds of travel and the relationship of strip map popularity to restrictions on travel at various points in time. The use of strip maps as spatial process descriptions of the environment is also considered in relation to a variety of travel contexts.},
	number = {2},
	journal = {Cartographic Journal},
	author = {{Alan M. MacEachren} and MacEachren, Alan M. and {Gregory B. Johnson} and Johnson, Gregory B.},
	month = dec,
	year = {1987},
	doi = {10.1179/caj.1987.24.2.147},
	note = {MAG ID: 2020391621},
	pages = {147--158},
}

@article{robinson_geospatial_2017,
	title = {Geospatial big data and cartography: research challenges and opportunities for making maps that matter},
	volume = {3},
	doi = {10.1080/23729333.2016.1278151},
	abstract = {Geospatial big data present a new set of challenges and opportunities for cartographic researchers in technical, methodological and artistic realms. New computational and technical paradigms for ca ...},
	author = {Robinson, Anthony C. and Demšar, Urška and Moore, Antoni and Buckley, Aileen and Jiang, Bin and Field, Kenneth and Kraak, Menno-Jan and Camboim, Silvana Philippi and Sluter, Claudia Robbi},
	month = mar,
	year = {2017},
	doi = {10.1080/23729333.2016.1278151},
	note = {MAG ID: 2596574850},
	pages = {32--60},
}

@article{avelar_generating_2000,
	title = {Generating topologically correct schematic maps},
	volume = {336},
	doi = {10.3929/ethz-a-006653901},
	journal = {CTIT technical reports series},
	author = {Avelar, Silvania and Müller, Matthias},
	month = jan,
	year = {2000},
	doi = {10.3929/ethz-a-006653901},
	note = {MAG ID: 1572275542},
}

@article{avelar_modeling_2011,
	title = {Modeling a {Public} {Transport} {Network} for {Generation} of {Schematic} {Maps} and {Location} {Queries}},
	abstract = {Abstract Schematic maps, such as subway or transit maps, are produced by hand or purely graphics software at present. This is not only a timely process, but requires a skilled map designer. Automatic generation of schematic maps may improve the process but more importantly would extend the use of such maps to a larger audience. In order to produce such maps however, we need a database containing geographical information on network routes of the region under consideration. This database should also contain topological information in order to answer common location queries from the map user. One fundamental aspect in a GIS project is the data model, which describes how the geographic reality will be represented in the computer. This paper presents a data model to describe geographical and topological information of a public transport network. We have also built a user interface on top of the data model for supporting location queries. Keywords: data model, transportation network, location queries},
	author = {Avelar, Silvania},
	month = jan,
	year = {2011},
	note = {MAG ID: 185414188},
}

@book{krygier_making_2016,
	title = {Making {Maps}, {Third} {Edition}: {A} {Visual} {Guide} to {Map} {Design} for {GIS}},
	isbn = {978-1-4625-0998-0},
	shorttitle = {Making {Maps}, {Third} {Edition}},
	abstract = {Lauded for its accessibility and beautiful design, this text has given thousands of students and professionals the tools to create effective, compelling maps. Using a wealth of illustrations--with 74 in full color--to elucidate each concisely presented point, the revised and updated third edition continues to emphasize how design choices relate to the reasons for making a map and its intended purpose. All components of map making are covered: titles, labels, legends, visual hierarchy, font selection, how to turn phenomena into visual data, data organization, symbolization, and more. Innovative pedagogical features include a short graphic novella, good design/poor design map examples, end-of-chapter suggestions for further reading, and an annotated map examplar that runs throughout the book. New to This Edition *Expanded coverage of using mobile digital devices to collect data for maps, including discussions of location services and locational privacy. *New and revised topics: how to do sketch maps, how map categories and symbols have changed over time, designing maps on desktop computers and mobile devices, human perception and color, and more. *Separate, expanded chapter on map symbol abstraction. *Additional case studies of compelling phenomena such as children\&\#39;s traffic fatalities based on race, the spread of tropical diseases, and the 2012 presidential election. *Many additional color illustrations.},
	language = {en},
	publisher = {Guilford Publications},
	author = {Krygier, John and Wood, Denis},
	month = aug,
	year = {2016},
	note = {Google-Books-ID: pi6yCwAAQBAJ},
	keywords = {Science / Earth Sciences / Geography, Social Science / Human Geography, Technology \& Engineering / Cartography, Technology \& Engineering / Social Aspects},
}

@article{thomas_barkowsky_schematizing_2000,
	title = {Schematizing {Maps}: {Simplification} of {Geographic}},
	author = {{Thomas Barkowsky} and Barkowsky, T. and Barkowsky, Thomas and {Longin Jan Latecki} and Latecki, Longin Jan and {K. F. Richter} and Richter, Kai-Florian and Richter, K. F.},
	month = jan,
	year = {2000},
	note = {MAG ID: 2237486621},
}

@article{sergio_cabello_schematization_2001,
	title = {Schematization of road networks},
	doi = {10.1145/378583.378609},
	abstract = {We study the problem of computing schematized versions of network maps , like railroad maps. Every path of the schematized map has two or three links with restricted orientations, and topologically, the schematized map must be equivalent to the input map. Our approach applies to several types of schematizations, and certain additional constraints can be added. In the general case our algorithm takes \$O(n{\textbackslash}log{\textasciicircum}3n)\$ time, and when all paths in the input are monotone in some (not necessarily the same) direction, it runs in \$O(n{\textbackslash}log n)\$ time.},
	author = {{Sergio Cabello} and Cabello, Sergio and {Mark de Berg} and de Berg, Mark and {Steven van Dijk} and van Dijk, Steven and {Marc van Kreveld} and van Kreveld, Marc and {Tycho Strijk} and Strijk, Tycho},
	month = jun,
	year = {2001},
	doi = {10.1145/378583.378609},
	note = {MAG ID: 2091776054},
	pages = {33--39},
}

@article{annumaaria_nivala_need_2003,
	title = {Need for {Context}-{Aware} {Topographic} {Maps} in {Mobile} {Devices}.},
	author = {{Annu‐Maaria Nivala} and Nivala, Annu-Maaria and {L. Tiina Sarjakoski} and Sarjakoski, L. Tiina},
	month = jan,
	year = {2003},
	note = {MAG ID: 168004264},
	pages = {15--29},
}

@article{romedi_passini_wayfinding_1981,
	title = {Wayfinding: {A} conceptual framework},
	volume = {5},
	doi = {10.1016/0304-4009(81)90018-8},
	abstract = {Abstract   Wayfinding denotes man's ability to reach spatial destinations in novel as well as in familiar settings. This paper conceptualizes wayfinding in terms of spatial problem solving and identifies three distinct but not necessarily chronological phases: (1) the processing of environmental information from present and past experiences, (2) the making of decisions and the development of plans on the basis of this information with respect to a specific task and (3) the execution of plans and the transformation of decisions into behavioral actions. Outlined are some basic principles of spatial problem solving illustrated by observed wayfinding behavior. It is suggested in the paper that problem-specific wayfinding strategies and user-specific wayfinding styles are common.},
	number = {1},
	journal = {Urban Ecology},
	author = {{Romedi Passini} and Passini, R.},
	month = apr,
	year = {1981},
	doi = {10.1016/0304-4009(81)90018-8},
	note = {MAG ID: 2066392749},
	pages = {17--31},
}

@article{john_krygier_making_2005,
	title = {Making {Maps}: {A} {Visual} {Guide} to {Map} {Design} for {GIS}},
	abstract = {1. How to Make a Map 2. What's Your Map For? 3. Mappable Data 4. Map-Making Tools 5. The Geographic Framework 6. The Big Picture 7. Inner Workings 8. Map Generalization and Classification 9. Map Symbolization 10. Words on Maps 11. Color on Maps},
	author = {{John Krygier} and Krygier, John and {Denis Wood} and Wood, Denis},
	month = aug,
	year = {2005},
	note = {MAG ID: 1572644050},
}

@article{hernan_casakin_schematic_2000,
	title = {Schematic {Maps} as {Wayfinding} {Aids}},
	doi = {10.1007/3-540-45460-8_5},
	abstract = {Schematic maps are effective tools for representing information about the physical environment; they depict specific information in an abstract way. This study concentrates on spatial aspects of the physical environment such as branching points and connecting roads, which play a paramount role in the schematization of wayfinding maps. Representative classes of branchingpoints are identified and organized in a taxonomy. The use of prototypical branching points and connecting road types is empirically evaluated in the schematization of maps. The role played by the different functions according to which the map is classified is assessed, and main strategies applied during the schematization process are identified. Implications for navigational tasks are presented.},
	author = {{Hernán Casakin} and Casakin, Hernan and {Thomas Barkowsky} and Barkowsky, Thomas and Barkowsky, Thomas and {Alexander Klippel} and Klippel, Alexander and {Christian Freksa} and Freksa, Christian},
	month = jan,
	year = {2000},
	doi = {10.1007/3-540-45460-8_5},
	note = {MAG ID: 1518841988},
	pages = {54--71},
}

@article{steffen_bogen_visual_2009,
	title = {Visual {Navigation} with {Schematic} {Maps}},
	doi = {10.1007/978-1-4419-0312-9_4},
	abstract = {A prototypical example of the operational dimensions of visual information communication is the use of schematic maps for visual navigation. The implementation of maps on location-sensitive or handheld devices has changed the preliminaries of common mapping techniques. By an analysis of selected examples, both historic and current, we want to open up the space for innovative map design options. Our approach blends art history and computer science, and is based on a systematic, operational perspective. It may be unexpected, though, that it starts from the way that graphic design supports imaginative navigation on the map, rather than considering its utility for navigation in the physical space directly.},
	author = {{Steffen Bogen} and Bogen, Steffen and {Ulrik Brandes} and Brandes, Ulrik and {Hendrik S. Ziezold} and Ziezold, Hendrik},
	month = jan,
	year = {2009},
	doi = {10.1007/978-1-4419-0312-9_4},
	note = {MAG ID: 204221842},
	pages = {65--84},
}

@article{ariane_tom_referring_2003,
	title = {Referring to {Landmark} or {Street} {Information} in {Route} {Directions}: {What} {Difference} {Does} {It} {Make}?},
	doi = {10.1007/978-3-540-39923-0_24},
	abstract = {When describing routes in urban environments, speakers usually refer to both street names and visual landmarks. However, a navigational system can be designed which only refers to streets or, alternatively, only to landmarks. Does it make any difference which type of information users are provided with? The answer to this question is crucial for the design of navigational aids. We report two experiments. The first one showed that in a wayfinding task, route directions referring to streets were less effective than those referring to landmarks for guidance purposes. The second experiment showed that when people generate route directions, they tend to produce less street than landmark information. These studies provide a further illustration of the critical role of landmarks in route directions.},
	author = {{Ariane Tom} and Tom, Ariane and {Michel Denis} and Denis, Michel},
	month = sep,
	year = {2003},
	doi = {10.1007/978-3-540-39923-0_24},
	note = {MAG ID: 188848701},
	pages = {362--374},
}

@article{alastair_m_morrison_public_1996,
	title = {Public {Transport} {Maps} in {Western} {European} {Cities}},
	volume = {33},
	doi = {10.1179/caj.1996.33.2.93},
	abstract = {AbstractThe author toured 25 cities in 11 countries of western Europe during 1995, in order to study their public transport maps and to meet the people responsible for them. He found distinct French, Scandinavian, and Dutch styles in addition to the classic style of public transport map. Rules are proposed to govern the choice of mapping method, based on the number of transport modes, number of transport services, amount of overlap, and the use of colour coding in the streets. Schematic maps, resembling the London Underground map, are preferable for underground railways, but are definitely not suitable for buses. Factors in the design which need particular attention include: amount of base map detail, map size and purpose, emphasising the name of the terminus, the problem of insets, panels at interchange points, and background colours. Types of maps, previously rarely used, which should now be economically feasible since the introduction of computer mapping, include: maps of subsets, stop-specific route m...},
	number = {2},
	journal = {Cartographic Journal},
	author = {{Alastair M. Morrison} and Morrison, Alastair},
	month = dec,
	year = {1996},
	doi = {10.1179/caj.1996.33.2.93},
	note = {MAG ID: 1997950687},
	pages = {93--110},
}

@article{fellner_metro_2004,
	title = {The metro map layout problem},
	doi = {10.1007/978-3-540-31843-9_50},
	abstract = {We initiate a new problem of automatic metro map layout. In general, a metro map consists of a set of lines which have intersections or overlaps. We define a set of aesthetic criteria for good metro map layouts and present a method to produce such layouts automatically. Our method uses a variation of the spring algorithm with a suitable preprocessing step. The experimental results with real world data sets show that our method produces good metro map layouts quickly.},
	author = {Fellner, Dieter W. and {Seok‐Hee Hong} and Hong, Seok-Hee and {Damian Merrick} and Merrick, Damian and {Hugo Alexandre Dantas do Nascimento} and Nascimento, Hugo A. D. do},
	month = sep,
	year = {2004},
	doi = {10.1007/978-3-540-31843-9_50},
	note = {MAG ID: 1845022861},
	pages = {482--491},
}

@article{hochmair_influence_2009,
	title = {The {Influence} of {Map} {Design} on {Route} {Choice} from {Public} {Transportation} {Maps} in {Urban} {Areas}},
	volume = {46},
	issn = {0008-7041},
	url = {https://doi.org/10.1179/000870409X12472347560623},
	doi = {10.1179/000870409X12472347560623},
	abstract = {Based on a user study in the Internet, this research analyses how map design and annotated network information in public transportation maps affect utilized proxy criteria when planning the fastest route in an intra-urban transportation network. Further, it is examined whether annotated network information on schematic maps affects the map reader in successfully finding the fastest route within the trip planning process. For this second task, a schematic map and maps with annotated headways, departure times and current positions of transit vehicles are compared.},
	number = {3},
	urldate = {2024-02-27},
	journal = {The Cartographic Journal},
	author = {Hochmair, Hartwig},
	month = aug,
	year = {2009},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1179/000870409X12472347560623},
	keywords = {DISCRETE CHOICE MODEL, FASTEST ROUTE, MAP DESIGN, PROXY VARIABLES, PUBLIC TRANSPORTATION MAPS, ROUTE CHOICE},
	pages = {242--256},
}

@article{sadahiro_computer-aided_2016,
	title = {Computer-aided design of bus route maps},
	volume = {43},
	issn = {1523-0406},
	url = {https://doi.org/10.1080/15230406.2015.1077162},
	doi = {10.1080/15230406.2015.1077162},
	abstract = {The bus route map is a diagram that aims to convey necessary information for map readers to find an appropriate way of moving from an origin to a destination. Design of bus route map is a complicated and time-consuming task that requires careful consideration of readability and aesthetics. This paper proposes a new computational method for designing bus route maps. The method helps us to reduce six types of undesirable elements in bus route maps, i.e., gap, shift, crossing, overlap, misalignment, and acute bend. The method consists of two phases: line layout phase determines the relative order of bus routes on each road segment and map layout phase calculates the actual position of bus routes drawn on a map. This paper applies the method to the design of bus route maps of Chiba City, Japan. The result supports the effectiveness of the method as well as reveals open topics for future research.},
	number = {4},
	urldate = {2024-02-27},
	journal = {Cartography and Geographic Information Science},
	author = {Sadahiro, Yukio and Tanabe, Takahito and Pierre, Maxime and Fujii, Koichi},
	month = aug,
	year = {2016},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15230406.2015.1077162},
	keywords = {Bus route maps, cartographic design, computational procedure, mathematical optimization},
	pages = {361--376},
}

@mastersthesis{galvao_schematic_2016,
	title = {Schematic bus transit maps for the web using genetic algorithms},
	copyright = {openAccess},
	url = {https://run.unl.pt/handle/10362/18403},
	abstract = {The octilinear schematic map, layout recognized worldwide in metro maps, is an 
important transit informative tool. This research investigates how algorithms for the 
visualization of schematic maps can be availed in mobile web devices context in 
order to empower the efficiency in transmitting information of bus transit maps. A 
genetic algorithm for path octilinear schematization technique has been used and 
tested to create the schematic data. Location-based and interactivity functionalities 
were embedded to the resulting digital maps in order to create personalized maps to 
meet specific user needs. A prototype of a web application and real transit data of the 
city of Castellón in Spain was used to test the methodology. The results have shown 
that real time schematizations open possibilities concerning usability that add extra 
value to schematic transit maps. Additionally, suggested improvements have been 
made to the genetic algorithm and performance tests show that genetic algorithms are 
adequate, in terms of efficiency, to sketch bus transit maps automatically.},
	language = {eng},
	urldate = {2024-02-27},
	author = {Galvão, Marcelo de Lima},
	month = mar,
	year = {2016},
	note = {Accepted: 2016-07-05T14:29:12Z},
}

@article{avelar_design_2006,
	title = {On the {Design} of {Schematic} {Transport} {Maps}},
	volume = {41},
	issn = {0317-7173},
	url = {https://www.utpjournals.press/doi/abs/10.3138/A477-3202-7876-N514},
	doi = {10.3138/A477-3202-7876-N514},
	abstract = {Schematic drawings of route directions are one of the most common forms of graphic communication. People make sketches to communicate geographical ideas, and professionally designed schematic maps give orientation to thousands of users of a public transport system. Creating a schematic map for representing a transport network may be seen as a straightforward task; however, the underlying design of such maps can be quite complex. Map designers apply, consciously or subconsciously, various cartographic generalization techniques to emphasize important information and to improve the clarity of map content. At present, traditional mapping and GIS literature offers very little guidance to a map designer seeking cartographic rules or practical ideas for representing the elaborate route data of public transport systems schematically. This article aims to contribute to the design challenges of schematic, route-based mapping. Information about schematic maps and symbolization of route-based data is given. A case study of schematic map design for a public transport network is presented to show the need for support of cartographic science for the creation of schematic transport maps.},
	number = {3},
	urldate = {2024-02-27},
	journal = {Cartographica: The International Journal for Geographic Information and Geovisualization},
	author = {Avelar, Silvania and Hurni, Lorenz},
	month = sep,
	year = {2006},
	note = {Publisher: University of Toronto Press},
	keywords = {Keywords:, Mots clés:, cartes géographiques schématiques, cartographic design, conception cartographique, route data symbolization, réseau de transport, schematic maps, symbolisation des données sur les circuits, transport network},
	pages = {217--228},
}

@article{farr_wayfinding_2012,
	title = {Wayfinding: {A} simple concept, a complex process},
	volume = {32},
	issn = {0144-1647, 1464-5327},
	shorttitle = {Wayfinding},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01441647.2012.712555},
	doi = {10.1080/01441647.2012.712555},
	language = {en},
	number = {6},
	urldate = {2024-02-27},
	journal = {Transport Reviews},
	author = {Farr, Anna Charisse and Kleinschmidt, Tristan and Yarlagadda, Prasad and Mengersen, Kerrie},
	month = nov,
	year = {2012},
	pages = {715--743},
}

@article{galvao_evaluating_2021,
	title = {Evaluating schematic route maps in wayfinding tasks for in-car navigation},
	volume = {48},
	issn = {1523-0406},
	url = {https://doi.org/10.1080/15230406.2021.1943531},
	doi = {10.1080/15230406.2021.1943531},
	abstract = {Today’s navigation systems use topographic maps to communicate route information. Being general-purpose maps, topographic maps lack optimal support for the specific task of route reading and navigation. In the public transportation domain, research demonstrated that topographic maps do not support planning of routes as good as schematic maps. Our current paper applies this idea to the domain of in-car navigation. Schematic maps emphasize functional aspects of geography and direction information by highlighting information relevant to navigation actions and orientation. However, there is a lack of systematic studies researching the usability of schematic cartography in wayfinding tasks. This article evaluates schematic route maps, created with an algorithm developed in our previous work, regarding user interaction, navigation performance, and spatial memorability. We compare these schematic maps with correspondent non-schematic ones in two different tasks: prospective and situated (driving simulator) route reading. The schematic map and the corresponding non-schematic map are identical in terms of their elements and topology; they vary only in their geometric shape: on the schematic maps, features are highly generalized, following schematic simplification rules for clarity. The experimental data shows that participants using the schematic route maps require fewer map interactions to complete the tasks, orientation information is more visible and leads to more accurate spatial knowledge acquisition. This result contributes to a better understanding of schematic route visualizations’ benefits to support users in wayfinding and orientation tasks.},
	number = {5},
	urldate = {2024-02-27},
	journal = {Cartography and Geographic Information Science},
	author = {Galvão, Marcelo L. and Krukar, Jakub and Schwering, Angela},
	month = sep,
	year = {2021},
	pmid = {34531704},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15230406.2021.1943531},
	keywords = {Schematic map, digital cartography, human-computer interaction, navigation, route map, spatial cognition, spatial knowledge acquisition, spatial memorability, topological maps, usability, wayfinding},
	pages = {449--469},
}

@article{van_goethem_stenomaps_2014,
	title = {Stenomaps: {Shorthand} for shapes},
	volume = {20},
	issn = {1941-0506},
	shorttitle = {Stenomaps},
	url = {https://ieeexplore.ieee.org/document/6876003},
	doi = {10.1109/TVCG.2014.2346274},
	abstract = {We address some of the challenges in representing spatial data with a novel form of geometric abstraction-the stenomap. The stenomap comprises a series of smoothly curving linear glyphs that each represent both the boundary and the area of a polygon. We present an efficient algorithm to automatically generate these open, C1-continuous splines from a set of input polygons. Feature points of the input polygons are detected using the medial axis to maintain important shape properties. We use dynamic programming to compute a planar non-intersecting spline representing each polygon's base shape. The results are stylised glyphs whose appearance may be parameterised and that offer new possibilities in the 'cartographic design space'. We compare our glyphs with existing forms of geometric schematisation and discuss their relative merits and shortcomings. We describe several use cases including the depiction of uncertain model data in the form of hurricane track forecasting; minimal ink thematic mapping; and the depiction of continuous statistical data.},
	number = {12},
	urldate = {2024-02-27},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {van Goethem, Arthur and Reimer, Andreas and Speckmann, Bettina and Wood, Jo},
	month = dec,
	year = {2014},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	keywords = {Algorithm, Algorithm design and analysis, Complexity theory, Data visualization, Design, Dynamic programming, Feature extraction, Maps, Schematisation, Shape analysis, Splines (mathematics)},
	pages = {2053--2062},
}

@article{dykes_visualization_2022,
	title = {Visualization for epidemiological modelling: challenges, solutions, reflections and recommendations},
	volume = {380},
	issn = {1364-503X, 1471-2962},
	shorttitle = {Visualization for epidemiological modelling},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0299},
	doi = {10.1098/rsta.2021.0299},
	abstract = {We report on an ongoing collaboration between epidemiological modellers and visualization researchers by documenting and reflecting upon knowledge constructs—a series of ideas, approaches and methods taken from existing visualization research and practice—deployed and developed to support modelling of the COVID-19 pandemic. Structured independent commentary on these efforts is synthesized through iterative reflection to develop: evidence of the effectiveness and value of visualization in this context; open problems upon which the research communities may focus; guidance for future activity of this type and recommendations to safeguard the achievements and promote, advance, secure and prepare for future collaborations of this kind. In describing and comparing a series of related projects that were undertaken in unprecedented conditions, our hope is that this unique report, and its rich interactive supplementary materials, will guide the scientific community in embracing visualization in its observation, analysis and modelling of data as well as in disseminating findings. Equally we hope to encourage the visualization community to engage with impactful science in addressing its emerging data challenges. If we are successful, this showcase of activity may stimulate mutually beneficial engagement between communities with complementary expertise to address problems of significance in epidemiology and beyond. See
              https://ramp-vis.github.io/RAMPVIS-PhilTransA-Supplement/
              .
            
            This article is part of the theme issue ‘Technical challenges of modelling real-life epidemics and examples of overcoming these’.},
	language = {en},
	number = {2233},
	urldate = {2024-02-27},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Dykes, Jason and Abdul-Rahman, Alfie and Archambault, Daniel and Bach, Benjamin and Borgo, Rita and Chen, Min and Enright, Jessica and Fang, Hui and Firat, Elif E. and Freeman, Euan and Gönen, Tuna and Harris, Claire and Jianu, Radu and John, Nigel W. and Khan, Saiful and Lahiff, Andrew and Laramee, Robert S. and Matthews, Louise and Mohr, Sibylle and Nguyen, Phong H. and Rahat, Alma A. M. and Reeve, Richard and Ritsos, Panagiotis D. and Roberts, Jonathan C. and Slingsby, Aidan and Swallow, Ben and Torsney-Weir, Thomas and Turkay, Cagatay and Turner, Robert and Vidal, Franck P. and Wang, Qiru and Wood, Jo and Xu, Kai},
	month = oct,
	year = {2022},
	pages = {20210299},
}

@inproceedings{lewis_suitability_2017,
	title = {The {Suitability} of {Using} {Least} {Cost} {Path} {Analysis} in the {Prediction} of {Roman} {Roads} in the {Highland} and {Lowland} {Zones} of {Roman} {Britain}},
	url = {https://www.semanticscholar.org/paper/The-Suitability-of-Using-Least-Cost-Path-Analysis-Lewis/ca17fc14b50e81cb14cb7480cdc0f84c58287723},
	abstract = {This paper examines the suitability of using least cost path (LCP) analysis in the prediction of Roman roads in Roman Britain. Although LCP analysis is thought by some to represent a well established methodology, others reflect that more investigation into the parameters is needed. This paper describes the effect of using different cost functions on the computed LCP, including the newly developed Modified Hiking Function. Furthermore, this paper outlines the use of gdistance as a way to overcome limitations with off-the-shelf GIS software packages. Lastly, the use of flow maps as a new way to visualise LCPs is introduced.},
	urldate = {2024-02-26},
	author = {Lewis, Joseph},
	year = {2017},
}

@article{schindling_lidar_2014,
	title = {{LiDAR} as a tool for archaeological research: a case study},
	volume = {6},
	issn = {1866-9557, 1866-9565},
	shorttitle = {{LiDAR} as a tool for archaeological research},
	url = {http://link.springer.com/10.1007/s12520-014-0178-3},
	doi = {10.1007/s12520-014-0178-3},
	language = {en},
	number = {4},
	urldate = {2024-02-26},
	journal = {Archaeological and Anthropological Sciences},
	author = {Schindling, James and Gibbes, Cerian},
	month = dec,
	year = {2014},
	pages = {411--423},
}

@article{mark_gahegan_fourth_2020,
	title = {Fourth paradigm {GIScience}? {Prospects} for automated discovery and explanation from data},
	volume = {34},
	doi = {10.1080/13658816.2019.1652304},
	abstract = {ABSTRACTThis article discusses the prospects for automated discovery of explanatory models directly from geospatial data. Rather than taking an approach based on machine learning, which generally ...},
	number = {1},
	journal = {International Journal of Geographical Information Science},
	author = {{Mark Gahegan} and {Mark Gahegan} and {Mark Gahegan} and Gahegan, Mark},
	month = jan,
	year = {2020},
	doi = {10.1080/13658816.2019.1652304},
	note = {MAG ID: 2971980344},
	pages = {1--21},
}

@article{harvey_j_miller_data-driven_2015,
	title = {Data-driven geography},
	volume = {80},
	doi = {10.1007/s10708-014-9602-6},
	abstract = {The context for geographic research has shifted from a data-scarce to a data-rich environment, in which the most fundamental changes are not just the volume of data, but the variety and the velocity at which we can capture georeferenced data; trends often associated with the concept of Big Data. A data-driven geography may be emerging in response to the wealth of georeferenced data flowing from sensors and people in the environment. Although this may seem revolutionary, in fact it may be better described as evolutionary. Some of the issues raised by data-driven geography have in fact been longstanding issues in geographic research, namely, large data volumes, dealing with populations and messy data, and tensions between idiographic versus nomothetic knowledge. The belief that spatial context matters is a major theme in geographic thought and a major motivation behind approaches such as time geography, disaggregate spatial statistics and GIScience. There is potential to use Big Data to inform both geographic knowledge-discovery and spatial modeling. However, there are challenges, such as how to formalize geographic knowledge to clean data and to ignore spurious patterns, and how to build data-driven models that are both true and understandable.},
	number = {4},
	journal = {GeoJournal},
	author = {{Harvey J. Miller} and Miller, Harvey J. and {Michael F. Goodchild} and Goodchild, Michael F.},
	month = aug,
	year = {2015},
	doi = {10.1007/s10708-014-9602-6},
	note = {MAG ID: 2006047816},
	pages = {449--461},
}

@article{derya_birant_st-dbscan_2007,
	title = {{ST}-{DBSCAN}: {An} algorithm for clustering spatial-temporal data},
	volume = {60},
	doi = {10.1016/j.datak.2006.01.013},
	abstract = {This paper presents a new density-based clustering algorithm, ST-DBSCAN, which is based on DBSCAN. We propose three marginal extensions to DBSCAN related with the identification of (i) core objects, (ii) noise objects, and (iii) adjacent clusters. In contrast to the existing density-based clustering algorithms, our algorithm has the ability of discovering clusters according to non-spatial, spatial and temporal values of the objects. In this paper, we also present a spatial-temporal data warehouse system designed for storing and clustering a wide range of spatial-temporal data. We show an implementation of our algorithm by using this data warehouse and present the data mining results.},
	number = {1},
	author = {{Derya Birant} and Birant, Derya and {Alp Kut} and Kut, Alp},
	month = jan,
	year = {2007},
	doi = {10.1016/j.datak.2006.01.013},
	note = {MAG ID: 1971022913},
	pages = {208--221},
}

@article{xiao_xiang_zhu_deep_2017,
	title = {Deep {Learning} in {Remote} {Sensing}: {A} {Comprehensive} {Review} and {List} of {Resources}},
	volume = {5},
	doi = {10.1109/mgrs.2017.2762307},
	abstract = {Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization.},
	number = {4},
	journal = {IEEE Geoscience and Remote Sensing Magazine},
	author = {{Xiao Xiang Zhu} and Zhu, Xiao Xiang and {Devis Tuia} and Tuia, Devis and {Lichao Mou} and Mou, Lichao and {Gui-Song Xia} and Xia, Gui-Song and {Liangpei Zhang} and Zhang, Liangpei and {Feng Xu} and Xu, Feng and {Friedrich Fraundorfer} and Fraundorfer, Friedrich},
	month = dec,
	year = {2017},
	doi = {10.1109/mgrs.2017.2762307},
	note = {MAG ID: 2782522152},
	pages = {8--36},
}

@article{jia_deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	doi = {10.1109/cvpr.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	author = {{Jia Deng} and Deng, Jia and {Wei Dong} and Dong, Wei and {Richard Socher} and Socher, Richard and {Lijia Li} and {Li-Jia Li} and Li, Li-Jia and {Kai Li} and Li, Kai and {Feifei Li} and Fei-Fei, Li},
	month = jun,
	year = {2009},
	doi = {10.1109/cvpr.2009.5206848},
	note = {MAG ID: 2108598243},
	pages = {248--255},
}

@article{j_f_conley_using_2011,
	title = {Using moment invariants to analyze cluster shapes and hypothesize potential causes},
	volume = {25},
	doi = {10.1080/13658816.2010.496369},
	abstract = {Although there are many algorithms and statistical tests to detect clustering of geographical phenomena, such as disease cases, the follow-up task of analyzing the cluster to explain its existence and mitigate the cluster is generally left to the researcher. These cluster detection methods are useful only for part of the process of identifying, understanding, and mitigating disease clusters. This research develops and presents a computer program that uses the information about the shape of the cluster to evaluate the hypotheses about potential causes. To achieve this, the shapes of the clusters are represented by image moment invariant statistics developed in the field of computer vision, and these shape statistics are compared against a database of moment invariants of shapes representative of several geographical processes, such as diffusion along roads and wind diffusion. Experiments using simulated data of different types of disease transmission were carried out, and the ability of the program to accurately classify the different types of diffusion demonstrates the viability of this approach to automated cluster analysis.},
	number = {4},
	journal = {International Journal of Geographical Information Science},
	author = {{J. F. Conley} and Conley, J. F.},
	month = apr,
	year = {2011},
	doi = {10.1080/13658816.2010.496369},
	note = {MAG ID: 1996879117},
	pages = {657--680},
}

@article{anthony_c_robinson_presence_2018,
	title = {The {Presence} of {Absence} in {Geovisual} {Analytics} of {Big} {Data}},
	doi = {10.1109/bgdds.2018.8626811},
	abstract = {New spatial data sources are emerging that have attributes of big data. These sources challenge our ability to handle high degrees of velocity, volume, and variety, and also frequently include a wide range of attributes that stretch our ability to characterize their veracity. While it remains quite challenging to visually represent, interact with, and reason about these data, our focus in this work is to suggest what analysts may be able to learn from the gaps in what we measure or the absences suggested by certain attributes. Put simply, we need to know more about how to show the missingness and absence in spatial data and to understand how people reason with that type of information in geovisual analytics environments.},
	author = {{Anthony C. Robinson} and Robinson, Anthony C.},
	month = sep,
	year = {2018},
	doi = {10.1109/bgdds.2018.8626811},
	note = {MAG ID: 2911917590},
	pages = {8626811},
}

@article{alexander_savelyev_new_2012,
	title = {New {Methods} for {Representing} and {Interacting} with {Qualitative} {Geographic} {Information}},
	abstract = {Abstract : Our aim for this component of research was to evaluate the prototype SensePlace2 environment to gauge its support for key tasks related to spatio-temporal analysis of qualitative data derived from social media sources (with the focus on Twitter). Our preliminary findings reveal that participants view SensePlace2 as having the capability to integrate and analyze geospatial dimensions of social media, but that the execution of the interface has many limitations related to ease of use and support for efficient analysis. Participants proposed many additional features they felt would improve utility and a range of interface adjustments they felt would improve usability. Qualitative feedback from our tasks shows that users were able to generate good answers to our task prompts in most instances. However, users frequently mention that their answers were difficult to generate and that they were uncertain about the quality of those answers. This further supports the overall finding that they key mechanisms may exist to support solid analysis, but that the means for interacting with these mechanisms require significant further refinement. Results from this evaluation will be applied to specific interface improvements and the further development of refined analytical methods.},
	author = {{Alexander Savelyev} and Savelyev, Alexander and {Scott Pezanowski} and Pezanowski, Scott and {Anthony C. Robinson} and Robinson, Anthony C. and {Alan M. MacEachren} and MacEachren, Alan M.},
	month = oct,
	year = {2012},
	note = {MAG ID: 314743302},
}

@article{frank_hardisty_methods_2011,
	title = {Methods for ad-hoc delineation and analysis of categories of spatio-temporal events},
	doi = {10.1145/1999320.1999372},
	abstract = {Analysts are faced with increasing volume and complexity of spatially and spatio-temporally referenced events to analyze. One means of taming this volume and complexity is to develop methods and tools that can identify patterns, including spatio-temporal structure like clusters, in event data. To understand these methods and tools, we first present some of the motivation for the work, and then we detail the software architecture that we will use to support categorical analysis of spatio-temporal events.   Methods for analyzing spatial events, and spatio-temporal events, have experienced a recent renaissance. This upsurge in interest has occurred in part because of novel high-quality event sources which can provide complex data with geographic and temporal referents. Examples of such event sources include geographically located Twitter postings linked to documents, or photographs and video taken with GPS-enabled mobile phones. The products of these event sources are a vast stream of events that are linked with heterogeneous and voluminous data, including textual, imagery data as well as numerical data. One of the ways of making numerical sense of such heterogeneous data is to consider the text or media as a set of tagged categories. We can then apply methods for detecting structure in spatio-temporal events, including recently developed methods for disease outbreak detection, to these categories.   We are developing methods and associated software that will allow users to tag or label events, analyze them, and interact with visual representations of the event structure detected by the analysis. An integrated software system, called STempo, will provide the user with a fixed set of analysis tools and coordination topology to work from. The category tagging and structure detection tools will also be worked into the larger set of tools available in the GeoViz Toolkit, an interactive system for geographic visualization and analysis.},
	author = {{Frank Hardisty} and Hardisty, Frank and {Donna J. Peuquet} and Peuquet, Donna J. and {Sen Xu} and Xu, Sen and {Anthony C. Robinson} and Robinson, Anthony C.},
	month = may,
	year = {2011},
	doi = {10.1145/1999320.1999372},
	note = {MAG ID: 2051841364},
	pages = {51},
}

@article{jonathan_long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	doi = {10.1109/cvpr.2015.7298965},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	author = {{Jonathan Long} and Long, Jonathan and {Evan Shelhamer} and Shelhamer, Evan and {Trevor Darrell} and Darrell, Trevor},
	month = jun,
	year = {2015},
	doi = {10.1109/cvpr.2015.7298965},
	note = {MAG ID: 1903029394},
	pages = {3431--3440},
}

@article{stefan_van_der_walt_scikit-image_2014,
	title = {scikit-image: {Image} processing in {Python}},
	volume = {2},
	doi = {10.7717/peerj.453},
	abstract = {scikit-image is an image processing library that implements algorithms and utilities for use in research, education and industry applications. It is released under the liberal Modified BSD open source license, provides a well-documented API in the Python programming language, and is developed by an active, international team of collaborators. In this paper we highlight the advantages of open source to achieve the goals of the scikit-image library, and we showcase several real-world image processing applications that use scikit-image. More information can be found on the project homepage, http://scikit-image.org.},
	number = {1},
	journal = {PeerJ},
	author = {{Stéfan van der Walt} and van der Walt, Stefan and {Johannes L. Schönberger} and Schonberger, Johannes L. and Schönberger, Johannes L. and {Juan Nunez-Iglesias} and Nunez-Iglesias, Juan and {François Boulogne} and Boulogne, François and {Joshua D. Warner} and Warner, Joshua D. and {Neil Yager} and Yager, Neil and {Emmanuelle Gouillart} and Gouillart, Emmanuelle and {Tony Yu} and Yu, Tony S. and {Tony Yu}},
	month = jun,
	year = {2014},
	doi = {10.7717/peerj.453},
	pmcid = {4081273},
	pmid = {25024921},
	note = {MAG ID: 2015159529},
}

@article{evan_shelhamer_fully_2017,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	volume = {39},
	doi = {10.1109/tpami.2016.2572683},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Evan Shelhamer} and Shelhamer, Evan and {Jonathan Long} and Long, Jonathan and {Trevor Darrell} and Darrell, Trevor},
	month = apr,
	year = {2017},
	doi = {10.1109/tpami.2016.2572683},
	pmid = {27244717},
	note = {MAG ID: 2395611524},
	pages = {640--651},
}

@article{hasan_al-marzouqi_semantic_2023,
	title = {Semantic {Labeling} of {High}-{Resolution} {Images} {Using} {EfficientUNets} and {Transformers}},
	volume = {61},
	doi = {10.1109/tgrs.2023.3268159},
	abstract = {Semantic segmentation necessitates approaches that learn high-level characteristics while dealing with enormous quantities of data. Convolutional neural networks (CNNs) can learn unique and adaptive features to achieve this aim. However, due to the large size and high spatial resolution of remote sensing images, these networks cannot efficiently analyze an entire scene. Recently, deep transformers have proven their capability to record global interactions between different objects in the image. In this paper, we propose a new segmentation model that combines convolutional neural networks with transformers, and show that this mixture of local and global feature extraction techniques provides significant advantages in remote sensing segmentation. In addition, the proposed model includes two fusion layers that are designed to efficiently represent multimodal inputs and output of the network. The input fusion layer extracts feature maps summarizing the relationship between image content and elevation maps (DSM). The output fusion layer uses a novel multitask segmentation strategy where class labels are identified using class-specific feature extraction layers and loss functions. Finally, a fast-marching method is used to convert unidentified class labels to their closest known neighbors. Our results demonstrate that the proposed method improves segmentation accuracy compared to state-of-the-art techniques.},
	author = {{Hasan Al-Marzouqi} and {Lyes Saad Saoud}},
	month = jan,
	year = {2023},
	doi = {10.1109/tgrs.2023.3268159},
	note = {MAG ID: 4366310786},
	pages = {1--13},
}

@article{yingying_zhao_multi-scale_2022,
	title = {Multi-scale {Feature} {Weighted}-{Aggregating} and {Boundary} {Enhancement} {Network} for {Semantic} {Segmentation} of {High}-{Resolution} {Remote} {Sensing} {Images}},
	doi = {10.1109/jstars.2022.3205609},
	abstract = {High-resolution remote sensing images (HRRSIs) play an important role in large area and real-time earth observation tasks. However, HRRSIs typically comprise heterogeneous objects of various sizes and complex boundary lines, which poses challenges to HRRSI segmentation. Despite the fact that deep convolutional neural networks dramatically boosted the accuracy, several limitations exist in standard models. Existing methods, mainly concatenate multi-scale information to extract the various sizes of objects. However, these methods ignore differentiating information, making it difficult to take advantage of them and completely extract small objects. In addition, there have remained some difficulties in extracting boundary information with positions of uncertainty in previous works. In this paper, we propose a novel multi-scale feature weighted-aggregating and boundary enhancement network (MFBE-Net) for the segmentation of HRRSIs. ResNet-50, possessing a strong ability to extract features, is employed as the backbone. To fully utilize the information that was extracted, we propose a multi-scale feature weighted-aggregating module, which aims to weight-integrate deep features, shallow features, and global information. The boundary enhancement module is designed to solve the blurry boundary information problems and locate its positions. Coordinate attention is also applied in the framework to coherently label size-varied ground objects from different categories and reduce information redundancy. Meanwhile, a mixed loss function is used to supervise the network training process. Finally, MFBE-Net was verified on two public HRRSI datasets, and the experimental results show that the proposed framework outperformed other existing mainstream deep learning methods and could further improve the accuracy of HRRSI segmentation.},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {{Yingying Zhao} and {Yingying Zhao} and {Guizhou Zheng} and {Guo Zheng} and {Zhangyan Xu} and {Zhangyan Xu} and {Zhonghang Qiu} and {Zhonghang Qiu} and {Zhixing Chen} and {Zhuo Chen}},
	month = jan,
	year = {2022},
	doi = {10.1109/jstars.2022.3205609},
	note = {MAG ID: 4295308575},
	pages = {1--13},
}

@article{cheng_liu_mapping_2024,
	title = {Mapping property redevelopment via {GeoAI}: {Integrating} computer vision and socioenvironmental patterns and processes},
	volume = {144},
	doi = {10.1016/j.cities.2023.104644},
	abstract = {Domain knowledge of social and environmental sciences is generally derived from less structured small data and/or small models. The integration of deep learning with socioenvironmental and geographical patterns and processes is still in an early phase. This study proposes a flexible framework that synthesizes computer vision with patterns and processes of geographical phenomena (e.g., urban redevelopment) via deep convolutional neural networks and spatiotemporal neighborhoods, respectively. Meanwhile, undesirable visual cues (e.g., temporary objects such as cars) cause false alarms in urban change detection. Thus, a masked deep pyramid similarity model (i.e., the computer vision model) is proposed to minimize the negative impact of nonbuilding changes. This pipeline has robustness against obfuscation of undesirable street scene changes and elasticity of geographical knowledge representations in mapping property redevelopment. This model is reproducible and adaptable due to the wide availability of SVI and flexibility of the framework. The results suggest that the domain-driven rules and nonbuilding change masking mechanism can significantly increase the accuracy of a computer vision model. We also find that urban redevelopment should be understood as a locally tuned back-to-the-city process with weak replicability. Hybrid gentrification (i.e., the combination of “seesaw” gentrification and continuous gentrification) is observed globally with local variations. First, the population of those affected should be carefully identified and the benefits of gentrification need to be redistributed on a larger scale. For Auckland, decision-makers should not divert attention away from low-income tenants in outer suburbs that slipped below the radar of local government and scholars. Second, more attention should be given to less resourcing neighborhoods in constantly gentrifying neighborhoods when governments formulate and implement housing subsidy policies. Locally, incessant redevelopment in Auckland CBD, New Market and Remuera, etc., should be contained.},
	author = {{Cheng Liu} and {Weiye Song}},
	month = jan,
	year = {2024},
	doi = {10.1016/j.cities.2023.104644},
	note = {MAG ID: 4388602038},
	pages = {104644--104644},
}

@article{zhang_xin_deep_2021,
	title = {Deep learning for processing and analysis of remote sensing big data: a technical review},
	doi = {10.1080/20964471.2021.1964879},
	abstract = {In recent years, the rapid development of Earth observation technology has produced an increasing growth in remote sensing big data, posing serious challenges for effective and efficient processing...},
	author = {{Zhang Xin} and Zhang, Xin and Zhang, Xin and {Zhou Yinqing} and Zhou, Yanan and {Jiancheng Luo} and Luo, Jiancheng},
	month = aug,
	year = {2021},
	doi = {10.1080/20964471.2021.1964879},
	note = {MAG ID: 3196824925},
	pages = {1--34},
}

@article{karamitrou_towards_2022,
	title = {Towards the use of {Artificial} {Intelligence} {Deep} {Learning} {Networks} for detection of {Archaeological} {Sites}},
	volume = {10},
	doi = {10.1088/2051-672x/ac9492},
	abstract = {Abstract While remote sensing data have long been widely used in archaeological prospection over large areas, the task of examining such data is time consuming and requires experienced and specialist analysts. However, recent technological advances in the field of artificial intelligence (AI), and in particular deep learning methods, open possibilities for the automated analysis of large areas of remote sensing data. This paper examines the applicability and potential of supervised deep learning methods for the detection and mapping of different kinds of archaeological sites comprising features such as walls and linear or curvilinear structures of different dimensions, spectral and geometrical properties. Our work deliberately uses open-source imagery to demonstrate the accessibility of these tools. One of the main challenges facing AI approaches has been that they require large amounts of labeled data to achieve high levels of accuracy so that the training stage requires significant computational resources. Our results show, however, that even with relatively limited amounts of data, simple eight-layer, fully convolutional network can be trained efficiently using minimal computational resources, to identify and classify archaeological sites and successfully distinguish them from features with similar characteristics. By increasing the number of training sets and switching to the use of high-performance computing the accuracy of the identified areas increases. We conclude by discussing the future directions and potential of such methods in archaeological research.},
	number = {4},
	journal = {Surface topography},
	author = {Karamitrou, Alexandra A. and {Alexandra Karamitrou} and {Fraser Sturt} and {Fraser Sturt} and Bogiatzis, P. and {Petros Bogiatzis} and {David Beresford-Jones} and {David Beresford‐Jones}},
	month = sep,
	year = {2022},
	doi = {10.1088/2051-672x/ac9492},
	note = {MAG ID: 4297022811},
	pages = {044001--044001},
}

@article{kaili_yang_semi-automatic_2022,
	title = {Semi-{Automatic} {Method} of {Extracting} {Road} {Networks} from {High}-{Resolution} {Remote}-{Sensing} {Images}},
	volume = {12},
	doi = {10.3390/app12094705},
	abstract = {Road network extraction plays a critical role in data updating, urban development, and decision support. To improve the efficiency of labeling road datasets and addressing the problems of traditional methods of manually extracting road networks from high-resolution images, such as their slow speed and heavy workload, this paper proposes a semi-automatic method of road network extraction from high-resolution remote-sensing images. The proposed method needs only a few points to extract a single road in the image. After the roads are extracted one by one, the road network is generated according to the width of each road and the spatial relationships among the roads. For this purpose, we use regional growth, morphology, vector tracking, vector simplification, endpoint modification, road connections, and intersection connections to generate road networks. Experiments on four images with different terrains and different resolutions show that this method has high extraction accuracy under different image conditions. The comparisons with the semi-automatic GVF-snake method based on regional growth also showed its advantages and potentiality. The proposed method is a novel form of semi-automatic road network extraction, and it significantly increases the efficiency of road network extraction.},
	number = {9},
	journal = {Applied sciences},
	author = {{Kaili Yang} and Yang, Kaili and {Weihong Cui} and {Weihong Cui} and {Shu Shi} and {Shu Shi} and {Yu Liu} and {Yu Liu} and {Yuanjin Li} and {Yuanjin Li} and {Mengyu Ge} and {Mengyu Ge}},
	month = may,
	year = {2022},
	doi = {10.3390/app12094705},
	note = {MAG ID: 4229333188},
	pages = {4705--4705},
}

@article{benedikt_soja_machine_2023,
	title = {Machine {Learning}-{Based} {Exploitation} of {Crowdsourced} {GNSS} {Data} for {Atmospheric} {Studies}},
	doi = {10.1109/igarss52108.2023.10283441},
	abstract = {The Global Navigation Satellite System (GNSS) is a well-recognized tool to probe the Earth’s atmosphere. This contribution highlights how GNSS data collected from smartphones of voluntary contributors can be used to determine parameters of the troposphere and ionosphere. In this regard, the application of machine learning (ML) to characterize the quality of the crowd-sourced data and model atmospheric parameters is discussed. We demonstrate that in certain cases, GNSS data from smartphones can reach a precision that would allow such data to densify observations from existing geodetic infrastructures.},
	author = {{Benedikt Soja} and {Grzegorz Kłopotek} and {Yuanxin Pan} and {Laura Crocetti} and {Shuyin Mao} and {Mudathir Awadaljeed} and {M. Rothacher} and {Linda See} and {Tobias Sturn} and {Rudi Weinacker} and {Ian McCallum} and {Vicente Navarro}},
	month = jul,
	year = {2023},
	doi = {10.1109/igarss52108.2023.10283441},
	note = {MAG ID: 4387803626},
}

@article{g_blewitt_harnessing_2018,
	title = {Harnessing the {GPS} {Data} {Explosion} for {Interdisciplinary} {Science}},
	volume = {99},
	doi = {10.1029/2018eo104623},
	abstract = {More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.},
	journal = {Eos},
	author = {{G. Blewitt} and Blewitt, Geoffrey and {W. C. Hammond} and Hammond, William C. and {Corn Kreemer} and Kreemer, Corn},
	month = sep,
	year = {2018},
	doi = {10.1029/2018eo104623},
	note = {MAG ID: 2893726365},
}

@article{pan_determination_2024,
	title = {Determination of {High}-{Precision} {Tropospheric} {Delays} {Using} {Crowdsourced} {Smartphone} {GNSS} {Data}},
	url = {https://egusphere.copernicus.org/preprints/2024/egusphere-2024-66/},
	doi = {https://doi.org/10.5194/egusphere-2024-66},
	abstract = {Abstract. The Global Navigation Satellite System (GNSS) is a key asset for tropospheric monitoring. Currently, GNSS meteorology relies primarily on geodetic-grade stations. However, such stations are too costly to be densely deployed, which limits the contribution of GNSS to tropospheric monitoring. In 2016, Google released the raw GNSS measurement application programming interface for smartphones running on Android version 7.0 and higher. Since nowadays there are billions of Android smartphones worldwide, utilizing those devices for atmospheric monitoring represents a remarkable scientific opportunity. In this study, smartphone GNSS data collected in Germany as part of the Application of Machine Learning Technology for GNSS IoT Data Fusion (CAMALIOT) crowdsourcing campaign in 2022 were utilized to investigate this idea. Approximately twenty thousand raw GNSS observation files were collected there during the campaign. First, a dedicated data processing pipeline was established that consists of two major parts: machine learning (ML)-based data selection and ionosphere-free Precise Point Positioning (PPP)-based Zenith Total Delay (ZTD) estimation. The proposed method was validated with a dedicated smartphone data collection experiment conducted on the rooftop of the ETH campus. The results confirmed that ZTD estimates of mm-level precision could be achieved with smartphone data collected in an open-sky environment. The impacts of observation time span and utilization of multi-GNSS observations on ZTD estimation were also investigated. Subsequently, the crowdsourced data from Germany were processed by PPP with the ionospheric delays interpolated using observations from surrounding SAPOS (Satellite Positioning Service of the German State Survey) GNSS stations. The ZTDs derived from ERA5 and an ML-based ZTD product served as benchmarks. The results revealed that an accuracy of better than 10 mm can be achieved by utilizing selected high-quality crowdsourced smartphone data. This study marks the first successful demonstration of high-precision ZTD determination with crowdsourced smartphone GNSS data and reveals success factors and current limitations.},
	urldate = {2024-02-05},
	author = {Pan, Yuanxin and Kłopotek, Grzegorz and Crocetti, Laura and Weinacker, Rudi and Sturn, Tobias and See, Linda and Dick, Galina and Möller, Gregor and Rothacher, Markus and McCallum, Ian and Navarro, Vicente and Soja, Benedikt},
	month = jan,
	year = {2024},
	doi = {10.5194/egusphere-2024-66},
}

@article{mihai_niculita_geomorphometric_2020,
	title = {Geomorphometric {Methods} for {Burial} {Mound} {Recognition} and {Extraction} from {High}-{Resolution} {LiDAR} {DEMs}},
	volume = {20},
	doi = {10.3390/s20041192},
	abstract = {Archaeological topography identification from high-resolution DEMs (Digital Elevation Models) is a current method that is used with high success in archaeological prospecting of wide areas. I present a methodology through which burial mounds (tumuli) from LiDAR (Light Detection And Ranging) DEMS can be identified. This methodology uses geomorphometric and statistical methods to identify with high accuracy burial mound candidates. Peaks, defined as local elevation maxima are found as a first step. In the second step, local convexity watershed segments and their seeds are compared with positions of local peaks and the peaks that correspond or have in vicinity local convexity segments seeds are selected. The local convexity segments that correspond to these selected peaks are further fed to a Random Forest algorithm together with shape descriptors and descriptive statistics of geomorphometric variables in order to build a model for the classification. Multiple approaches to tune and select the proper training dataset, settings, and variables were tested. The validation of the model was performed on the full dataset where the training was performed and on an external dataset in order to test the usability of the method for other areas in a similar geomorphological and archaeological setting. The validation was performed against manually mapped, and field checked burial mounds from two neighbor study areas of 100 km2 each. The results show that by training the Random Forest on a dataset composed of between 75\% and 100\% of the segments corresponding to burial mounds and ten times more non-burial mounds segments selected using Latin hypercube sampling, 93\% of the burial mound segments from the external dataset are identified. There are 42 false positive cases that need to be checked, and there are two burial mound segments missed. The method shows great promise to be used for burial mound detection on wider areas by delineating a certain number of tumuli mounds for model training.},
	number = {4},
	journal = {Sensors},
	author = {{Mihai Niculiță} and {Mihai Niculiță} and Niculiță, Mihai and {Mihai Niculiță}},
	month = feb,
	year = {2020},
	doi = {10.3390/s20041192},
	note = {MAG ID: 3007462066},
	pages = {1192},
}

@article{karsten_lambers_integrating_2019,
	title = {Integrating {Remote} {Sensing}, {Machine} {Learning}, and {Citizen} {Science} in {Dutch} {Archaeological} {Prospection}},
	volume = {11},
	doi = {10.3390/rs11070794},
	abstract = {Although the history of automated archaeological object detection in 
remotely sensed data is short, progress and emerging trends are evident.
 Among them, the shift from rule-based approaches towards machine 
learning methods is, at the moment, the cause for high expectations, 
even though basic problems, such as the lack of suitable archaeological 
training data are only beginning to be addressed. In a case study in the
 central Netherlands, we are currently developing novel methods for 
multi-class archaeological object detection in LiDAR data based on 
convolutional neural networks (CNNs). This research is embedded in a 
long-term investigation of the prehistoric landscape of our study 
region. We here present an innovative integrated workflow that combines 
machine learning approaches to automated object detection in remotely 
sensed data with a two-tier citizen science project that allows us to 
generate and validate detections of hitherto unknown archaeological 
objects, thereby contributing to the creation of reliable, labeled 
archaeological training datasets. We motivate our methodological choices
 in the light of current trends in archaeological prospection, remote 
sensing, machine learning, and citizen science, and present the first 
results of the implementation of the workflow in our research area.},
	number = {7},
	journal = {Remote Sensing},
	author = {{Karsten Lambers} and Lambers, Karsten and {Wouter B. Verschoof‐van der Vaart} and der Vaart, Wouter B. Verschoof-van and {Quentin Bourgeois} and Bourgeois, Q.P.J.},
	month = apr,
	year = {2019},
	doi = {10.3390/rs11070794},
	note = {MAG ID: 2925480424},
	pages = {794},
}

@article{lei_luo_airborne_2019,
	title = {Airborne and spaceborne remote sensing for archaeological and cultural heritage applications: {A} review of the century (1907–2017)},
	volume = {232},
	doi = {10.1016/j.rse.2019.111280},
	abstract = {Abstract   Archaeological and cultural heritage (ACH), one of the core carriers of cultural diversity on our planet, has a direct bearing on the sustainable development of mankind. Documenting and protecting ACH is the common responsibility and duty of all humanity. It is governed by UNESCO along with the scientific communities that foster and encourage the use of advanced non-invasive techniques and methods for promoting scientific research into ACH and conservation of ACH sites. The use of remote sensing, a non-destructive tool, is increasingly popular by specialists around the world as it allows fast prospecting and mapping at multiple scales, rapid analysis of multisource datasets, and dynamic monitoring of ACH sites and their surrounding environments. The cost of using remote sensing is lower or even zero in practical applications. In this review, in order to discuss the advantages of airborne and spaceborne remote sensing (ASRS), the principles that make passive (photography, multispectral and hyperspectral) and active (synthetic aperture radar (SAR) and light detection and ranging radar (LiDAR)) imaging techniques suitable for ACH applications are first summarized and pointed out; a review of ASRS and the methodologies used over the past century is then presented together with relevant highlights from well-known research projects. Selected case studies from Mediterranean regions to East Asia illustrate how ASRS can be used effectively to investigate and understand archaeological features at multiple -scales and to monitor and assess the conservation status of cultural heritage sites in the context of sustainable development. An in-depth discussion on the limitations of ASRS and associated remaining challenges is presented along with conclusions and a look at future trends.},
	journal = {Remote Sensing of Environment},
	author = {{Lei Luo} and Luo, Lei and {Xinyuan Wang} and Wang, Xinyuan and {Huadong Guo} and Guo, Huadong and {Rosa Lasaponara} and Lasaponara, Rosa and {Xiangping Zong} and Zong, Xin and {Nicola Masini} and Masini, Nicola and Wang, Guizhou and {Guizhou Wang} and Wang, Guizhou and {Guizhou Wang} and {Pilong Shi} and {Pilong Shi} and Shi, Pilong and {Houcine Khatteli} and Khatteli, Houcine and {Fulong Chen} and Chen, Fulong and {Shahina Tariq} and Tariq, Shahina and {Jie Shao} and {Jie Shao} and {Jie Shao} and Shao, Jie and {Nabil Bachagha} and Bachagha, Nabil and {Ruixia Yang} and Yang, Ruixia and {Yulong Yao} and Yao, Ya},
	month = oct,
	year = {2019},
	doi = {10.1016/j.rse.2019.111280},
	note = {MAG ID: 2962045372},
	pages = {111280},
}

@article{ross_girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	doi = {10.1109/iccv.2015.169},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	author = {{Ross Girshick} and Girshick, Ross},
	month = dec,
	year = {2015},
	doi = {10.1109/iccv.2015.169},
	note = {MAG ID: 1536680647},
	pages = {1440--1448},
}

@article{dave_cowley_making_2020,
	title = {Making {LiGHT} {Work} of {Large} {Area} {Survey}? {Developing} {Approaches} to {Rapid} {Archaeological} {Mapping} and the {Creation} of {Systematic} {National}-scaled {Heritage} {Data}},
	volume = {3},
	doi = {10.5334/jcaa.49},
	abstract = {The characteristics and form of heritage data are fundamental to its utility in a range of applications, particularly so for heritage agencies who have a remit in management, policy, and the creation and curation of national databases of monuments, sites, and landscapes. Written from the perspective of an archaeological survey function in a national heritage agency, this paper draws on preliminary outcomes from a research and development project that aims to proof protocols for creating systematic data across large areas drawing heavily on remotely sensed data. This recognises that a systemic consideration of the implications of changing technology and data is sometimes desirable, rather than gradual assimilation of developments into existing practice. In particular, the issues being addressed relate to the challenges and opportunities of proliferating remote sensed data and digital workflows. These include the strategic assessment of threat, consideration of fitness for purpose of different datasets relative to landscape characteristics, the documentation of processes and sources of information, the suitability of data structures, and the mechanisms for automating site detection and data creation.},
	number = {1},
	author = {{Dave Cowley} and Cowley, Dave and {Dave C. Cowley} and {Łukasz Banaszek} and Banaszek, Łukasz and {George Geddes} and Geddes, George and {Angela Gannon} and Gannon, Angela and {Mike Middleton} and Middleton, Mike and {Kirsty Millican} and Millican, Kirsty and {Kirsty Millican}},
	month = apr,
	year = {2020},
	doi = {10.5334/jcaa.49},
	note = {MAG ID: 3017540848},
	pages = {109--121},
}

@article{wouter_b_verschoofvan_der_vaart_learning_2019,
	title = {Learning to {Look} at {LiDAR}: {The} {Use} of {R}-{CNN} in the {Automated} {Detection} of {Archaeological} {Objects} in {LiDAR} {Data} from the {Netherlands}},
	volume = {2},
	doi = {10.5334/jcaa.32},
	abstract = {Computer-aided methods for the automatic detection of archaeological objects are needed to cope with the ever-growing set of largely digital and easily available remotely sensed data. In this paper, a promising new technique for the automated detection of multiple classes of archaeological objects in LiDAR data is presented. This technique is based on R-CNNs (Regions-based Convolutional Neural Networks). Unlike normal CNNs, which classify the entire input image, R-CNNs address the problem of object detection, which requires correctly localising and classifying (multiple) objects within a larger image. We have incorporatedthis technique into a workflow, which enables the preprocessing of LiDAR data into the required data format and the conversion of the results of the object detection into geographical data, usable in a GIS environment. The proposed technique has been trained and tested on LiDAR data gathered from the central part of the Netherlands. This area contains a multitude of archaeological objects, including prehistoricbarrows and Celtic fields. The initial experiments show that we are able to automatically detect and categorise these two types of archaeological objects and thus proof the added value of this technique.},
	number = {1},
	author = {{Wouter B. Verschoof‐van der Vaart} and der Vaart, Wouter B. Verschoof-van and {Karsten Lambers} and Lambers, Karsten},
	month = mar,
	year = {2019},
	doi = {10.5334/jcaa.32},
	note = {MAG ID: 2922858103},
	pages = {31--40},
}

@article{rebecca_bennett_comparison_2012,
	title = {A {Comparison} of {Visualization} {Techniques} for {Models} {Created} from {Airborne} {Laser} {Scanned} {Data}},
	volume = {19},
	doi = {10.1002/arp.1414},
	abstract = {The uptake of airborne laser scanned (ALS) data (commonly known as airborne lidar) for heritage landscape assessment has grown rapidly in the past decade as data have become increasingly available. Likewise there has been a recent upsurge in published techniques for modelling the ground surface from ALS data to highlight archaeological features. However, many end-users of the data are not trained in remote sensing and visualization techniques and the lack of comparative assessment of techniques has increased the complexity of interpretation of the ALS-derived models. This study quantitatively compares five visualization techniques ranging from the commonly used shaded relief model to newer local relief and sky view factor modelling for a study area in the UK. Outputs are compared with the baseline data of the English Heritage National Mapping Programme aerial photographic archive transcription and assessed with respect to percentage visibility of feature length. Ancillary aspects of the outputs are discussed, such as geospatial shift of features, suitability for profile mapping, ease of interpretation and ability to combine with other data sources. It is concluded that although the overall performance of the models in terms of feature recognition is relatively even, consideration of all factors enables more transparent modelling choices to be made and facilitates critical interpretation of the features recorded. Copyright © 2012 John Wiley \& Sons, Ltd.},
	number = {1},
	journal = {Archaeological Prospection},
	author = {{Rebecca Bennett} and Bennett, Rebecca and {Kate Welham} and Welham, Kate and {Ross A. Hill} and Hill, Ross A. and {Andrew Ford} and Ford, Andrew},
	month = jan,
	year = {2012},
	doi = {10.1002/arp.1414},
	note = {MAG ID: 1932582440},
	pages = {41--48},
}

@article{alexandre_guyot_objective_2021,
	title = {Objective comparison of relief visualization techniques with deep {CNN} for archaeology},
	volume = {38},
	doi = {10.1016/j.jasrep.2021.103027},
	abstract = {Abstract   Archaeology has been profoundly transformed by the advent of airborne laser scanning (ALS) technology (a.k.a airborne LiDAR). High-resolution and high-precision synoptic views of earth’s topography are now available, even in densely forested environments, to identify and characterize landform patterns resulting from past human occupation. ALS-based archaeological prospection relies on digital terrain model (DTM) visualization techniques (VTs) that highlight subtle topographical changes perceived and interpreted by archaeologists. An increasing number of VTs have been developed, and they have been evaluated to date mainly based on subjective human perception. This study developed a new approach based on state-of-the-art computer-vision algorithms to benchmark VTs using objective metrics. Thirteen VTs were applied to a ALS-derived DTM, and a deep convolution neural network (deep CNN) was implemented and trained to automatically detect and segment archaeological structures from these images. Visual interpretation of the images showed that the most informative VT was e2MSTP, which combined a multiscale topographic analysis (MSTP) with a morphologically explicit image and a slope-invariant relief detrending technique. The deep CNN approach confirmed these results and provided objective performance metrics. This study indicates that the computer vision approach opens new perspectives in the objective selection of the most suitable VT for archaeological prospection.},
	journal = {Journal of Archaeological Science: Reports},
	author = {{Alexandre Guyot} and Guyot, Alexandre and {Marc Lennon} and Lennon, Marc and {Laurence Hubert‐Moy} and Hubert-Moy, Laurence},
	month = aug,
	year = {2021},
	doi = {10.1016/j.jasrep.2021.103027},
	note = {MAG ID: 3162589486},
	pages = {103027},
}

@article{benjamin_stular_airborne_2021,
	title = {Airborne {LiDAR} {Point} {Cloud} {Processing} for {Archaeology}. {Pipeline} and {QGIS} {Toolbox}},
	volume = {13},
	doi = {10.3390/rs13163225},
	abstract = {The use of topographic airborne LiDAR data has become an essential part of archaeological prospection. However, as a step towards theoretically aware, impactful, and reproducible research, a more rigorous and transparent method of data processing is required. To this end, we set out to create a processing pipeline for archaeology-specific point cloud processing and derivation of products that are optimized for general-purpose data. The proposed pipeline improves on ground and building point cloud classification. The main area of innovation in the proposed pipeline is raster grid interpolation. We have improved the state-of-the-art by introducing a hybrid interpolation technique that combines inverse distance weighting with a triangulated irregular network with linear interpolation. State-of-the-art solutions for enhanced visualizations are included and essential metadata and paradata are also generated. In addition, we have introduced a QGIS plug-in that implements the pipeline as a one-step process. It reduces the manual workload by 75 to 90 percent and requires no special skills other than a general familiarity with the QGIS environment. It is intended that the pipeline and tool will contribute to the white-boxing of archaeology-specific airborne LiDAR data processing. In discussion, the role of data processing in the knowledge production process is explored.},
	number = {16},
	journal = {Remote Sensing},
	author = {{Benjamin Štular} and Štular, Benjamin and {Stefan Eichert} and Eichert, Stefan and {Edisa Lozić} and Lozić, Edisa},
	month = aug,
	year = {2021},
	doi = {10.3390/rs13163225},
	note = {MAG ID: 3193461815},
	pages = {3225},
}

@article{alexandre_guyot_combined_2021,
	title = {Combined {Detection} and {Segmentation} of {Archeological} {Structures} from {LiDAR} {Data} {Using} a {Deep} {Learning} {Approach}},
	volume = {4},
	doi = {10.5334/jcaa.64},
	abstract = {Until recently, archeological prospection using LiDAR data was based mainly on expert-basedand time-consuming visual analyses. Currently, deep learning convolutional neural networks (deep CNN) are showing potential for automatic detection of objects inmany fields of application, including cultural heritage. However, these computer-visionbased algorithms remain strongly restricted by the large number of samples required totrain models and the need to define target classes before using the models. Moreover,the methods used to date for archaeological prospection are limited to detecting objects and cannot (semi-)automatically characterize the structures of interest. Inthis study, we assess the contribution of deep learning methods for detecting andcharacterizing archeological structures by performing object segmentation using a deep CNN approach with transfer learning. The approach was applied to a terrainvisualization image derived from airborne LiDAR data within a 200 km2 area in Brittany,France. Our study reveals that the approach can accurately (semi-)automatically detect, delineate, and characterize topographic anomalies, and thus provides aneffective tool to inventory many archaeological structures. These results provide newperspectives for large-scale archaeological mapping.},
	number = {1},
	author = {{Alexandre Guyot} and Guyot, Alexandre and {Marc Lennon} and Lennon, Marc and {Thierry Lorho} and Lorho, Thierry and {Laurence Hubert‐Moy} and Hubert-Moy, Laurence},
	year = {2021},
	doi = {10.5334/jcaa.64},
	note = {MAG ID: 3128185159},
	pages = {1},
}

@article{andres_menendez_blanco_following_2020,
	title = {Following the {Roman} {Army} between the {Southern} {Foothills} of the {Cantabrian} {Mountains} and the {Northern} {Plains} of {Castile} and {León} ({North} of {Spain}): {Archaeological} {Applications} of {Remote} {Sensing} and {Geospatial} {Tools}},
	volume = {10},
	doi = {10.3390/geosciences10120485},
	abstract = {Sixty-six new archaeological sites have been discovered thanks to the combined use of different remote sensing techniques and open access geospatial datasets (mainly aerial photography, satellite imagery, and airborne LiDAR). These sites enhance the footprint of the Roman military presence in the northern fringe of the River Duero basin (Leon, Palencia, Burgos and Cantabria provinces, Spain). This paper provides a detailed morphological description of 66 Roman military camps in northwestern Iberia that date to the late Republic or early Imperial eras. We discuss the different spatial datasets and GIS tools used for different geographic contexts of varied terrain and vegetation. Finally, it stresses out the relevance of these novel data to delve into the rationale behind the Roman army movements between the northern Duero valley and the southern foothills of the Cantabrian Mountains. We conclude that methodological approaches stimulated by open-access geospatial datasets and enriched by geoscientific techniques are fundamental to understand the expansion of the Roman state in northwestern Iberia during the 1st c. BC properly. This renewed context set up a challenging scenario to overcome traditional archaeological perspectives still influenced by the cultural-historical paradigm and the pre-eminence of classical written sources.},
	number = {12},
	author = {{Andrés Menéndez Blanco} and Blanco, Andrés Menéndez and {Jesús García Sánchez} and {Jesús García Sánchez} and {Jesús Nicasio García Sánchez} and {Jesús García Sánchez} and Sánchez, Jesús Nicasio García and García, José Manuel Costa and {José Manuel Costa García} and Costa-García, José Manuel and {João Fonte} and Fonte, João and {João Fonte} and Álvarez, David González and {David González Álvarez} and González-Álvarez, David and García, Víctor Vicente and {Víctor Vicente García} and García, Víctor Vicente},
	year = {2020},
	doi = {10.3390/geosciences10120485},
	note = {MAG ID: 3108754477},
	pages = {485},
}

@article{argyro_argyrou_review_2022,
	title = {A {Review} of {Artificial} {Intelligence} and {Remote} {Sensing} for {Archaeological} {Research}},
	volume = {14},
	doi = {10.3390/rs14236000},
	abstract = {The documentation and protection of archaeological and cultural heritage (ACH) using remote sensing, a non-destructive tool, is increasingly popular for experts around the world, as it allows rapid searching and mapping at multiple scales, rapid analysis of multi-source data sets, and dynamic monitoring of ACH sites and their environments. The exploitation of remote sensing data and their products have seen an increased use in recent years in the fields of archaeological science and cultural heritage. Different spatial and spectral analysis datasets have been applied to distinguish archaeological remains and detect changes in the landscape over time, and, in the last decade, archaeologists have adopted more thoroughly automated object detection approaches for potential sites. These approaches included, among others, object detection methods, such as those of machine learning (ML) and deep learning (DL) algorithms, as well as convolutional neural networks (CNN) and deep learning (DL) models using aerial and satellite images, airborne and spaceborne remote sensing (ASRS), multispectral, hyperspectral images, and active methods (synthetic aperture radar (SAR) and light detection and ranging radar (LiDAR)). Researchers also refer to the potential for archaeologists to explore such artificial intelligence (AI) approaches in various ways, such as identifying archaeological features and classifying them. Here, we present a review study related to the contributions of remote sensing (RS) and artificial intelligence in archaeology. However, a main question remains open in the field of research: the rate of positive contribution of remote sensing and artificial intelligence techniques in archaeological research. The scope of this study is to summarize the state of the art related to AI and RS for archaeological research and provide some further insights into the existing literature.},
	number = {23},
	journal = {Remote Sensing},
	author = {{Argyro Argyrou} and {Argyro Argyrou} and Agapiou, Athos and {Άθως Αγαπίου}},
	month = nov,
	year = {2022},
	doi = {10.3390/rs14236000},
	note = {MAG ID: 4310178578},
	pages = {6000--6000},
}

@article{alexander_bonhage_modified_2021,
	title = {A modified {Mask} region-based convolutional neural network approach for the automated detection of archaeological sites on high-resolution light detection and ranging-derived digital elevation models in the {North} {German} {Lowland}},
	volume = {28},
	doi = {10.1002/arp.1806},
	abstract = {Due to complicated backgrounds and unclear target orientation, automated object detection is difficult in the field of archaeology. Most of the current convolutional neural network (CNN) object-oriented detection techniques are based on a faster region-based CNN (R-CNN) and other one-stage detectors that often lack adequate processing speeds and detection accuracies. Recently, the two-stage detector Mask R-CNN technique achieved impressive results in object detection and instance segmentation problems and was successfully applied in the analysis of archaeological airborne laser scanning (ALS) data. In this study, we outline a modified Mask R-CNN technique that reliably and efficiently detects relict charcoal hearth (RCH) sites on light detection and ranging (LiDAR) data-based digital elevation models (DEMs). Using image augmentation and image preprocessing steps combined with the deep learning-based adaptive gradient method with a dynamic bound on the learning rate (AdaBound) optimization technique, we could improve the model's accuracy and significantly reduce its training time. We use DEMs based on high-resolution LiDAR data and the visualization for archaeological topography (VAT) technique that give images with a very strong contrast of the terrain and the outline of the sites of interest in the North German Lowland. Therefore, the model can identify RCH sites with an average recall of 83\% and an average precision of 87\%. Techniques such as the modified Mask R-CNN method outlined here will help to greatly improve our knowledge about archaeological site densities in the realm of historical charcoal production and past human-landscape interactions. This method provides an accurate, time-efficient and bias-free large-scale site mapping option not only for the North German Lowland but potentially for other landscapes as well.},
	number = {2},
	journal = {Archaeological Prospection},
	author = {{Alexander Bonhage} and Bonhage, Alexander and {Mahmoud Eltaher} and Eltaher, Mahmoud and {Thomas Raab} and Raab, Thomas and {Michael Breuß} and Breuß, Michael and {Alexandra Raab} and Raab, Alexandra and {Anna Schneider} and Schneider, Anna},
	year = {2021},
	doi = {10.1002/arp.1806},
	note = {MAG ID: 3127593607},
	pages = {177--186},
}

@article{meyer-hes_identification_2020,
	title = {Identification of {Archaeologically} {Relevant} {Areas} {Using} {Open} {Geodata}},
	volume = {70},
	doi = {10.1007/s42489-020-00049-w},
	abstract = {Light detection and ranging (LiDAR) and digital terrain models (DTM) revolutionized archeological prospection in the last two decades. Using the new technique, comprehensive areal detections of archeological relief structures (field monuments) hidden under dense vegetation became possible and archeologists found new sites even in well-known areas. In times of Open Geodata policies, archeologists have access to geospatial data sets such as DTM. Assessing its full potential requires automated workflows, which is a recent research topic in archeological research. However, all approaches, both manually and automated, are affected by misclassifications caused by confusions of archeological and modern structures. Digital landscape models (DLM) help differentiating structures by their location. Concerning these data, only 74\% of the total area of Westphalia and Lippe need archeological investigation, increasing precision of automated classification approaches.},
	number = {3},
	author = {Meyer-Heß, Fabian M. and {M. Fabian Meyer-Heß} and Meyer-Heß, M. Fabian},
	month = aug,
	year = {2020},
	doi = {10.1007/s42489-020-00049-w},
	note = {MAG ID: 3047653327},
	pages = {107--125},
}

@article{ramakant_nevatia_linear_1979,
	title = {Linear feature extraction and description},
	abstract = {A technique of edge detection and linking for linear feature extraction and its applications to detection of roads and runway like structures is described. Experimental results are included.},
	author = {{Ramakant Nevatia} and Nevatia, Ramakant and {K. Ramesh Babu} and Babu, K. Ramesh},
	month = aug,
	year = {1979},
	note = {MAG ID: 368793415},
	pages = {639--641},
}

@article{alexandre_guyot_detecting_2018,
	title = {Detecting {Neolithic} {Burial} {Mounds} from {LiDAR}-{Derived} {Elevation} {Data} {Using} a {Multi}-{Scale} {Approach} and {Machine} {Learning} {Techniques}},
	volume = {10},
	doi = {10.3390/rs10020225},
	abstract = {Airborne LiDAR technology is widely used in archaeology and over the past decade has emerged as an accurate tool to describe anthropomorphic landforms. Archaeological features are traditionally emphasised on a LiDAR-derived Digital Terrain Model (DTM) using multiple Visualisation Techniques (VTs), and occasionally aided by automated feature detection or classification techniques. Such an approach offers limited results when applied to heterogeneous structures (different sizes, morphologies), which is often the case for archaeological remains that have been altered throughout the ages. This study proposes to overcome these limitations by developing a multi-scale analysis of topographic position combined with supervised machine learning algorithms (Random Forest). Rather than highlighting individual topographic anomalies, the multi-scalar approach allows archaeological features to be examined not only as individual objects, but within their broader spatial context. This innovative and straightforward method provides two levels of results: a composite image of topographic surface structure and a probability map of the presence of archaeological structures. The method was developed to detect and characterise megalithic funeral structures in the region of Carnac, the Bay of Quiberon, and the Gulf of Morbihan (France), which is currently considered for inclusion on the UNESCO World Heritage List. As a result, known archaeological sites have successfully been geo-referenced with a greater accuracy than before (even when located under dense vegetation) and a ground-check confirmed the identification of a previously unknown Neolithic burial mound in the commune of Carnac.},
	number = {2},
	journal = {Remote Sensing},
	author = {{Alexandre Guyot} and Guyot, Alexandre and {Laurence Hubert‐Moy} and Hubert-Moy, Laurence and {Thierry Lorho} and Lorho, Thierry},
	month = feb,
	year = {2018},
	doi = {10.3390/rs10020225},
	note = {MAG ID: 2793222099},
	pages = {225},
}

@article{igor_zingman_detection_2016,
	title = {Detection of {Fragmented} {Rectangular} {Enclosures} in {Very} {High} {Resolution} {Remote} {Sensing} {Images}},
	volume = {54},
	doi = {10.1109/tgrs.2016.2545919},
	abstract = {We develop an approach for the detection of ruins of livestock enclosures (LEs) in alpine areas captured by high-resolution remotely sensed images. These structures are usually of approximately rectangular shape and appear in images as faint fragmented contours in complex background. We address this problem by introducing a rectangularity feature that quantifies the degree of alignment of an optimal subset of extracted linear segments with a contour of rectangular shape. The rectangularity feature has high values not only for perfectly regular enclosures but also for ruined ones with distorted angles, fragmented walls, or even a completely missing wall. Furthermore, it has a zero value for spurious structures with less than three sides of a perceivable rectangle. We show how the detection performance can be improved by learning a linear combination of the rectangularity and size features from just a few available representative examples and a large number of negatives. Our approach allowed detection of enclosures in the Silvretta Alps that were previously unknown. A comparative performance analysis is provided. Among other features, our comparison includes the state-of-the-art features that were generated by pretrained deep convolutional neural networks (CNNs). The deep CNN features, although learned from a very different type of images, provided the basic ability to capture the visual concept of the LEs. However, our handcrafted rectangularity-size features showed considerably higher performance.},
	number = {8},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {{Igor Zingman} and Zingman, Igor and {Dietmar Saupe} and Saupe, Dietmar and {Otávio Augusto Bizetto Penatti} and Penatti, Otávio A. B. and {Karsten Lambers} and Lambers, Karsten},
	month = apr,
	year = {2016},
	doi = {10.1109/tgrs.2016.2545919},
	note = {MAG ID: 2192434319},
	pages = {4580--4593},
}

@article{bernard_devereux_visualisation_2008,
	title = {Visualisation of {LiDAR} terrain models for archaeological feature detection},
	volume = {82},
	doi = {10.1017/s0003598x00096952},
	abstract = {LiDAR is developing into a formidable instrument of aerial survey. Here the author shows how the LiDAR picture can be enhanced so that features picked up by illumination from different directions can be combined in one comprehensive survey.},
	number = {316},
	journal = {Antiquity},
	author = {{Bernard Devereux} and Devereux, B.J. and {Gabriel S. Amable} and Amable, G.S. and {P. Crow} and Crow, P.},
	month = jun,
	year = {2008},
	doi = {10.1017/s0003598x00096952},
	note = {MAG ID: 1798489336},
	pages = {470--479},
}

@article{oivind_due_trier_using_2019,
	title = {Using deep neural networks on airborne laser scanning data: {Results} from a case study of semi-automatic mapping of archaeological topography on {Arran}, {Scotland}},
	volume = {26},
	doi = {10.1002/arp.1731},
	abstract = {Abstract This article presents results of a case study within a project that seeks to develop heavily automated analysis of digital topographic data to extract archaeological information and to expedite large area mapping. Drawing on developments in computer vision and machine learning, this has the potential to fundamentally recast the capacity of archaeological prospection to cover large areas and deal with mass data, breaking a dependency on human resource. Without such developments, the potential of the vast amount of archaeological information embedded in large topographic and image‐based datasets cannot be realized. The purpose of the case study reported on here is to assess existing developments in a Norwegian study against digital topographic data for the island of Arran, Scotland, examining the transferability of the approach and providing a proof of concept in a Scottish context. For Arran, three monument classes were assessed – prehistoric roundhouses, shieling huts of medieval or post‐medieval date, and small clearance cairns. These present different challenges to detection, with preliminary results ranging from a manageable mix of false positives and true identifications to the chaotic. The influence of variable morphology and the occurrence of other, largely natural, objects of confusion in the landscape is discussed, highlighting the potential improvements in automated detection routines offered by adding anthropogenic and natural false positives to additional confusion classes.},
	number = {2},
	journal = {Archaeological Prospection},
	author = {{Øivind Due Trier} and Trier, Øivind Due and Cowley, Dave and {David Cowley} and Cowley, David C. and {Anders U. Waldeland} and Waldeland, Anders U.},
	month = apr,
	year = {2019},
	doi = {10.1002/arp.1731},
	note = {MAG ID: 2903009718},
	pages = {165--175},
}

@article{chen_stemflow_2024,
	title = {stemflow: {A} {Python} {Package} for {Adaptive} {Spatio}-{Temporal} {Exploratory} {Model}},
	volume = {9},
	issn = {2475-9066},
	shorttitle = {stemflow},
	url = {https://joss.theoj.org/papers/10.21105/joss.06158},
	doi = {10.21105/joss.06158},
	abstract = {Chen et al., (2024). stemflow: A Python Package for Adaptive Spatio-Temporal Exploratory Model. Journal of Open Source Software, 9(94), 6158, https://doi.org/10.21105/joss.06158},
	language = {en},
	number = {94},
	urldate = {2024-02-24},
	journal = {Journal of Open Source Software},
	author = {Chen, Yangkang and Gu, Zhongru and Zhan, Xiangjiang},
	month = feb,
	year = {2024},
	pages = {6158},
}

@article{harrie_machine_2024,
	title = {Machine learning in cartography},
	volume = {51},
	issn = {1523-0406},
	url = {https://doi.org/10.1080/15230406.2023.2295948},
	doi = {10.1080/15230406.2023.2295948},
	abstract = {Machine learning is increasingly used as a computing paradigm in cartographic research. In this extended editorial, we provide some background of the papers in the CaGIS special issue Machine Learning in Cartography with a special focus on pattern recognition in maps, cartographic generalization, style transfer, and map labeling. In addition, the paper includes a discussion about map encodings for machine learning applications and the possible need for explicit cartographic knowledge and procedural modeling in cartographic machine learning models.},
	number = {1},
	urldate = {2024-02-22},
	journal = {Cartography and Geographic Information Science},
	author = {Harrie, Lars and Touya, Guillaume and Oucheikh, Rachid and Ai, Tinghua and Courtial, Azelle and Richter, Kai-Florian},
	month = jan,
	year = {2024},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15230406.2023.2295948},
	keywords = {Cartography, deep learning, machine learning, map generalization, map labeling, pattern recognition, style transfer},
	pages = {1--19},
}

@article{hu_enriching_2022,
	title = {Enriching the metadata of map images: a deep learning approach with {GIS}-based data augmentation},
	volume = {36},
	issn = {1365-8816},
	shorttitle = {Enriching the metadata of map images},
	url = {https://doi.org/10.1080/13658816.2021.1968407},
	doi = {10.1080/13658816.2021.1968407},
	abstract = {Maps in the form of digital images are widely available in geoportals, Web pages, and other data sources. The metadata of map images, such as spatial extents and place names, are critical for their indexing and searching. However, many map images have either mismatched metadata or no metadata at all. Recent developments in deep learning offer new possibilities for enriching the metadata of map images via image-based information extraction. One major challenge of using deep learning models is that they often require large amounts of training data that have to be manually labeled. To address this challenge, this paper presents a deep learning approach with GIS-based data augmentation that can automatically generate labeled training map images from shapefiles using GIS operations. We utilize such an approach to enrich the metadata of map images by adding spatial extents and place names extracted from map images. We evaluate this GIS-based data augmentation approach by using it to train multiple deep learning models and testing them on two different datasets: a Web Map Service image dataset at the continental scale and an online map image dataset at the state scale. We then discuss the advantages and limitations of the proposed approach.},
	number = {4},
	urldate = {2024-02-20},
	journal = {International Journal of Geographical Information Science},
	author = {Hu, Yingjie and Gui, Zhipeng and Wang, Jimin and Li, Muxian},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/13658816.2021.1968407},
	keywords = {GeoAI, Map image, data augmentation, geospatial metadata, map information extraction},
	pages = {799--821},
}

@article{manucharyan_deep_2021,
	title = {A {Deep} {Learning} {Approach} to {Spatiotemporal} {Sea} {Surface} {Height} {Interpolation} and {Estimation} of {Deep} {Currents} in {Geostrophic} {Ocean} {Turbulence}},
	volume = {13},
	copyright = {© 2020. The Authors.},
	issn = {1942-2466},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001965},
	doi = {10.1029/2019MS001965},
	abstract = {Satellite altimeters provide global observations of sea surface height (SSH) and present a unique data set for advancing our theoretical understanding of upper-ocean dynamics and monitoring its variability. Considering that mesoscale SSH patterns can evolve on timescales comparable to or shorter than satellite return periods, it is challenging to accurately reconstruct the continuous SSH evolution as currently available altimetry observations are still spatially and temporally sparse. Here we explore the possibility of SSH interpolation via Deep Learning by using synthetic observations from an idealized quasigeostrophic model of baroclinic ocean turbulence. We demonstrate that Convolutional Neural Networks with Residual Learning are superior in SSH reconstruction to linear and recently developed dynamical interpolation techniques. Also, the deep neural networks can provide a skillful state estimate of unobserved deep ocean currents at mesoscales. These conspicuous results suggest that SSH patterns of eddies might contain substantial information about the underlying deep ocean currents that are necessary for SSH prediction. Our training data are focused on highly idealized physics and diversification of processes needs to be considered to more accurately represent the real ocean. In addition, methodological improvements such as transfer learning and implementation of dynamically aware loss functions might be necessary to consider before its ultimate use with real satellite observations. Nonetheless, by providing a proof of concept based on synthetic data, our results point to deep learning as a viable alternative to existing interpolation and, more generally, state estimation methods for satellite observations of eddying currents.},
	language = {en},
	number = {1},
	urldate = {2024-02-20},
	journal = {Journal of Advances in Modeling Earth Systems},
	author = {Manucharyan, Georgy E. and Siegelman, Lia and Klein, Patrice},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019MS001965},
	keywords = {Deep Learning, baroclinic instability, deep ocean flows, mesoscale eddies, sea surface height interpolation, state estimation},
	pages = {e2019MS001965},
}

@misc{abdishakur_deep_2020,
	title = {Deep learning for {Geospatial} data applications — {Multi}-label {Classification}},
	url = {https://medium.com/spatial-data-science/deep-learning-for-geospatial-data-applications-multi-label-classification-2b0a1838fcf3},
	abstract = {A beginner's guide and tutorial for classifying satellite images with Fastai},
	language = {en},
	urldate = {2024-02-20},
	journal = {Spatial Data Science},
	author = {Abdishakur},
	month = oct,
	year = {2020},
}

@incollection{cao_deep_2022,
	address = {Singapore},
	title = {Deep {Learning} of {Big} {Geospatial} {Data}: {Challenges} and {Opportunities}},
	isbn = {978-981-19381-6-0},
	shorttitle = {Deep {Learning} of {Big} {Geospatial} {Data}},
	url = {https://doi.org/10.1007/978-981-19-3816-0_18},
	abstract = {With rapid advances of geospatial data acquisition technologies, spatiotemporal data have become increasingly available. As the geography and spatial science community is shifting rapidly to embrace the data-rich era, the long-standing challenges facing the spatiotemporal analysis remain not only unsolved but of increasing prominence in producing geographic knowledge out of the rich data. This chapter reviews these challenges posed by the big spatiotemporal data and discusses the recent progresses in addressing them with a particular focus on the promises of deep learning and GeoAI methods. The chapter is then concluded with a discussion on possible future directions.},
	language = {en},
	urldate = {2024-02-20},
	booktitle = {New {Thinking} in {GIScience}},
	publisher = {Springer Nature},
	author = {Cao, Guofeng},
	editor = {Li, Bin and Shi, Xun and Zhu, A-Xing and Wang, Cuizhen and Lin, Hui},
	year = {2022},
	doi = {10.1007/978-981-19-3816-0_18},
	keywords = {Big data, GIScience, GeoAI, Machine learning, Spatial statistics},
	pages = {159--169},
}

@misc{noauthor_geospatial_nodate,
	title = {Geospatial {Artificial} {Intelligence} ({GeoAI})},
	url = {https://www.oxfordbibliographies.com/display/document/obo-9780199874002/obo-9780199874002-0228.xml},
	abstract = {"Geospatial Artificial Intelligence (GeoAI)" published on  by null.},
	language = {en},
	urldate = {2024-02-20},
	journal = {obo},
}

@article{yang_diagnostic_2020,
	title = {Diagnostic accuracy of deep learning in orthopaedic fractures: a systematic review and meta-analysis},
	volume = {75},
	issn = {0009-9260},
	shorttitle = {Diagnostic accuracy of deep learning in orthopaedic fractures},
	url = {https://www.sciencedirect.com/science/article/pii/S0009926020302075},
	doi = {10.1016/j.crad.2020.05.021},
	abstract = {Aim
To gather and compare related clinical studies, and to investigate the accuracy and reliability of deep learning in detecting orthopaedic fractures.
Materials and methods
This study is a retrospective combination and interpretation of prospectively acquired data. Articles from PubMed, EMBASE, the Cochrane library databases, and reference lists of the qualified articles were retrieved. Heterogeneity between studies was assessed using a random effective model. Pooled sensitivity, specificity, diagnostic odds ratio, and area under the receiver operating characteristic curve (AUC) were obtained by a random model. This work was managed from October 2018 to March 2020.
Results
Fourteen studies were included in this systematic review and nine were synthesized in the meta-analysis. The pooled sensitivity and specificity for the whole group (17 trials, 5,434 images) were 0.87 and 0.91, respectively. The AUC was 0.95. Eight trials (1,574 images) were included in the long-bone group, which contained seven studies. The pooled sensitivity was 0.96 and specificity was 0.94. The AUC was 0.99. Heterogeneity existed in the four pooled results of the whole group and the pooled specificity of the long-bone group.
Conclusions
Deep learning is reliable in fracture diagnosis and has high diagnostic accuracy, which is similar to that of general physicians and is unlikely to produce a large number of false diagnoses; however, the ability of deep learning to localize the fracture needs more attention and testing. Deep learning can be extremely helpful with pre-classification of clinical diagnoses.},
	number = {9},
	urldate = {2024-02-20},
	journal = {Clinical Radiology},
	author = {Yang, S. and Yin, B. and Cao, W. and Feng, C. and Fan, G. and He, S.},
	month = sep,
	year = {2020},
	pages = {713.e17--713.e28},
}

@article{grekousis_artificial_2019,
	title = {Artificial neural networks and deep learning in urban geography: {A} systematic review and meta-analysis},
	volume = {74},
	issn = {0198-9715},
	shorttitle = {Artificial neural networks and deep learning in urban geography},
	url = {https://www.sciencedirect.com/science/article/pii/S0198971518302928},
	doi = {10.1016/j.compenvurbsys.2018.10.008},
	abstract = {Artificial neural networks (ANNs) and their latest advancement in deep learning are blooming in computer science. Geography has integrated these artificial intelligence techniques, but not with the same enthusiasm. The main reason for hesitation is that ANNs are still confronted as complex and black boxes. However, ANNs might be more solid methods than conventional approaches when dealing with complex geographical problems. This study considers the great potential of ANNs for research in urban geography. First, using the PRISMA protocol, it provides a statistical review of 140 papers on studies that employed ANNs in urban geography between 1997 and 2016. Second, it performs a quantitative meta-analysis using non-parametric bootstrapping. 45 (of the 140) papers were assessed regarding ANNs' overall accuracy (OA) achieved when used for urban growth prediction or urban land-use classification. Third, a new guideline for reporting ANNs is proposed. Statistical review indicated that ANNs performed better in 75.7\% of case studies compared to conventional methods. Meta-analysis found that on bootstrapped averages, the median OA achieved when using, ANNs was higher than the median OA achieved by other techniques by 2.3\% (p {\textless} .001). ANNs also performed better when used for classification compared to prediction. Analysis also identified inadequate presentation of ANNs and related results when used in urban studies. For this reason, a new guideline for reporting ANNs is suggested in this work to ensure consistency and easier dissemination of individual lessons learned. These findings aim to motivate further studies on ANNs and deep learning in urban geography.},
	urldate = {2024-02-20},
	journal = {Computers, Environment and Urban Systems},
	author = {Grekousis, George},
	month = mar,
	year = {2019},
	keywords = {Artificial Neural Networks, Deep Learning, Guidelines on Reporting results, Meta-analysis, Trends, Urban Geography},
	pages = {244--256},
}

@article{liu_review_2022,
	title = {A review of spatially-explicit {GeoAI} applications in {Urban} {Geography}},
	volume = {112},
	issn = {1569-8432},
	url = {https://www.sciencedirect.com/science/article/pii/S1569843222001339},
	doi = {10.1016/j.jag.2022.102936},
	abstract = {Urban Geography studies forms, social fabrics, and economic structures of cities from a geographic perspective. Catalysed by the increasingly abundant spatial big data, Urban Geography seeks new models and research paradigms to explain urban phenomena and address urban issues. Recent years have witnessed significant advances in spatially-explicit geospatial artificial intelligence (GeoAI), which integrates spatial studies and AI, primarily focusing on incorporating spatial thinking and concept into deep learning models for urban studies. This paper provides an overview of techniques and applications of spatially-explicit GeoAI in Urban Geography based on 581 papers identified using a systematic review approach. We examined and screened papers in three scopes of Urban Geography (Urban Dynamics, Social Differentiation of Urban Areas, and Social Sensing) and found that although GeoAI is a trending topic in geography and the applications of deep neural network-based methods are proliferating, the development of spatially-explicit GeoAI models is still at their early phase. We identified three challenges of existing models and advised future research direction towards developing multi-scale explainable spatially-explicit GeoAI. This review paper acquaints beginners with the basics of GeoAI and state-of-the-art and serve as an inspiration to attract more research in exploring the potential of spatially-explicit GeoAI in studying the socio-economic dimension of the city and urban life.},
	urldate = {2024-02-20},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Liu, Pengyuan and Biljecki, Filip},
	month = aug,
	year = {2022},
	keywords = {Deep learning, Graph neural network, Location encoder, Socio-economics, Urban studies},
	pages = {102936},
}

@article{maceachren_research_2001,
	title = {Research {Challenges} in {Geovisualization}},
	volume = {28},
	issn = {1523-0406},
	url = {https://doi.org/10.1559/152304001782173970},
	doi = {10.1559/152304001782173970},
	number = {1},
	urldate = {2024-02-20},
	journal = {Cartography and Geographic Information Science},
	author = {MacEachren, Alan M. and Kraak, Menno-Jan},
	month = jan,
	year = {2001},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1559/152304001782173970},
	pages = {3--12},
}

@article{maceachren_exploratory_1997,
	series = {Exploratory {Cartograpic} {Visualisation}},
	title = {Exploratory cartographic visualization: {Advancing} the agenda},
	volume = {23},
	issn = {0098-3004},
	shorttitle = {Exploratory cartographic visualization},
	url = {https://www.sciencedirect.com/science/article/pii/S0098300497000186},
	doi = {10.1016/S0098-3004(97)00018-6},
	abstract = {An approach to the visualization of georeferenced data is presented. This approach is rooted in cartography and emphasizes the use of visual methods in research and decision making. Several definitions proposed within cartography are considered and the links between “cartographic” visualization and scientific visualization more generally are discussed. From this base, a perspective on visualization is articulated in which attention is directed to the goals for use of maps and related georeferenced displays. We argue that a use-based approach is needed in order to develop information processing environments appropriate to distinct stages of scientific research and decision making. The paper concludes by proposing a set of research problems that are prompted by taking a use-based approach to visualization, and then outlining the selection and context of the papers in this special issue.},
	number = {4},
	urldate = {2024-02-20},
	journal = {Computers \& Geosciences},
	author = {Maceachren, Alan M. and Kraak, Menno-Jan},
	month = may,
	year = {1997},
	keywords = {Cartography, Dynamic maps, Exploratory data analysis, Visualization},
	pages = {335--343},
}

@article{kent_form_2018,
	title = {Form {Follows} {Feedback}: {Rethinking} {Cartographic} {Communication}},
	volume = {13},
	issn = {1744-6716},
	shorttitle = {Form {Follows} {Feedback}},
	url = {http://www.westminsterpapers.org/articles/10.16997/wpcc.296/},
	doi = {10.16997/wpcc.296},
	language = {en},
	number = {2},
	urldate = {2024-02-20},
	journal = {Westminster Papers in Communication and Culture},
	author = {Kent, Alexander J.},
	month = oct,
	year = {2018},
	pages = {96--112},
}

@misc{noauthor_geospatial_nodate-1,
	title = {Geospatial deep learning with {TorchGeo}},
	url = {https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/},
	abstract = {TorchGeo is a PyTorch domain library providing datasets, samplers, transforms, and pre-trained models specific to geospatial data.},
	language = {en},
	urldate = {2024-02-12},
	journal = {PyTorch},
}

@misc{noauthor_benefits_nodate,
	title = {The benefits and value of open data {\textbar} data.europa.eu},
	url = {https://data.europa.eu/en/publications/datastories/benefits-and-value-open-data},
	urldate = {2024-02-08},
}

@misc{gillies_ancient_2020,
	type = {a gazetteer of past places},
	title = {Ancient {Places} in {Pleiades}},
	copyright = {Copyright © The Contributors. Sharing and remixing permitted under terms of the Creative Commons Attribution 3.0 License (cc-by).},
	url = {https://pleiades.stoa.org/places/index_html},
	abstract = {Pleiades is a gazetteer of ancient places. At present, it has extensive coverage for the Greek and Roman world, and is expanding into Ancient Near Eastern, Byzantine, Celtic, Early Islamic, and Early Medieval geography.},
	language = {en},
	urldate = {2024-02-05},
	journal = {Pleiades: a gazetteer of past places},
	author = {Gillies, Sean},
	collaborator = {Elliott, Tom},
	month = may,
	year = {2020},
	note = {Last Modified: 2020-05-28T13:37:13-04:00
Publisher: Institute for the Study of the Ancient World (NYU); Ancient World Mapping Center (UNC-CH)
Section: a gazetteer of past places},
}

@article{see_collecting_2023,
	title = {Collecting volunteered geographic information from the {Global} {Navigation} {Satellite} {System} ({GNSS}): experiences from the {CAMALIOT} project},
	volume = {16},
	issn = {1753-8947},
	shorttitle = {Collecting volunteered geographic information from the {Global} {Navigation} {Satellite} {System} ({GNSS})},
	url = {https://doi.org/10.1080/17538947.2023.2239761},
	doi = {10.1080/17538947.2023.2239761},
	abstract = {Raw observations (carrier-phase and code observations) from the Global Navigation Satellite System (GNSS) can now be accessed from Android mobile phones (Version 7.0 onwards). This paves the way for GNSS data to be utilized for low-cost precise positioning or in ionospheric or tropospheric applications. This paper presents results from data collection campaigns using the CAMALIOT mobile app. In the first campaign, 116.3 billion measurements from 11,828 mobile devices were collected from all continents. Although participation decreased during the second campaign, data are still being collected globally. In this contribution, we demonstrate the potential of volunteered geographic information (VGI) from mobile phones to fill data gaps in geodetic station networks that collect GNSS data, e.g. in Brazil, but also how the data can provide a denser set of observations than current networks in countries across Europe. We also show that mobile phones capable of dual-frequency reception, which is an emerging technology that can provide a richer source of GNSS data, are contributing in a substantial way. Finally, we present the results from a survey of participants to indicate that participation is diverse in terms of backgrounds and geography, where the dominant motivation for participation is to contribute to scientific research.},
	number = {1},
	urldate = {2024-02-05},
	journal = {International Journal of Digital Earth},
	author = {See, Linda and Soja, Benedikt and Kłopotek, Grzegorz and Sturn, Tobias and Weinacker, Rudi and Karanam, Santosh and Georgieva, Ivelina and Pan, Yuanxin and Crocetti, Laura and Rothacher, Markus and Navarro, Vicente and Fritz, Steffen and McCallum, Ian},
	month = oct,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17538947.2023.2239761},
	keywords = {Citizen science, Earth observation, crowdsourcing, global navigation satellite system (GNSS), mobile apps, volunteered geographic information (VGI)},
	pages = {2818--2841},
}

@misc{noauthor_roman_2022,
	title = {Roman marching camps in {Scotland} {\textbar} {ScottishHistory}.org},
	url = {https://www.scottishhistory.org/blog/roman-marching-camps-scotland/},
	abstract = {Scotland is truly fortunate in the number and range of Roman marching camps already identified here, and without a shadow of doubt, many more remain to be},
	language = {en-GB},
	urldate = {2024-02-05},
	month = jan,
	year = {2022},
	note = {Section: Scottish History Blog},
}

@article{abriha_strategies_2023,
	title = {Strategies in training deep learning models to extract building from multisource images with small training sample sizes},
	volume = {16},
	issn = {1753-8947},
	url = {https://doi.org/10.1080/17538947.2023.2210312},
	doi = {10.1080/17538947.2023.2210312},
	abstract = {Building extraction from remote sensing data is an important topic in urban studies and the deep learning methods have an increasing role due to their minimal requirements in training data to reach outstanding performance. We aimed to investigate the original U-Net architecture’s efficiency in building segmentation with different number of training images and the role of data augmentation based on multisource remote sensing data with varying spatial and spectral resolutions (WorldView-2 [WV2], WorldView-3 [WV3] images and an aerial orthophoto [ORTHO]). When the trainings and predictions were conducted on the same image, U-Net provided good results with very few training images (validation accuracies: 94-97\%; 192 images). Combining the ORTHO’s and WV2’s training data for prediction on WV3 provided poor results with low F1-score (0.184). However, the inclusion of only 48 WV3 training images significantly improved the F1-score (0.693), thus, most buildings were correctly identified. Accordingly, using only independent reference data (other than the target image) is not enough to train an accurate model. In our case, the reference from WW2 and ORTHO images did not provide an acceptable basis to train a good model, but a minimal number of training images from the targeted WV3 improved the accuracy (F1-score: 69\%).},
	number = {1},
	urldate = {2023-10-25},
	journal = {International Journal of Digital Earth},
	author = {Abriha, Dávid and Szabó, Szilárd},
	month = dec,
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17538947.2023.2210312},
	keywords = {Building segmentation, U-Net, deep learning, remote sensing, urban analysis},
	pages = {1707--1724},
}

@article{jamil_review_2022,
	title = {A {Review} on {Deep} {Learning} {Application} for {Detection} of {Archaeological} {Structures}},
	volume = {26},
	copyright = {Copyright (c) 2022 Journal of Advanced Research in Applied Sciences and Engineering Technology},
	issn = {2462-1943},
	url = {https://www.akademiabaru.com/submit/index.php/araset/article/view/4431},
	doi = {10.37934/araset.26.1.714},
	abstract = {Over the last few years, archaeologist have started to look at automated object detection for searching of potential historical sites, using object identification methods that includes neural network-based and non-neural network-based approaches. However, there is a scarcity of reviews on Convolutional Neural Networks (CNN) based Deep Learning (DL) models for object detection in the archaeological field. The purpose of this review is to examine existing research that has been implemented in the area of ancient structures object detection using Convolutional Neural Networks. Notably, CNN based object detection has the difficulty to draw a boundary box around the object and was implemented mainly for object classification. Various algorithms such as, the Region-based Convolutional Neural Network (R-CNN) and Mask Region-based Convolutional Neural Network (MR-CNN) was developed to solve this problem, yielding a more accurate, time-efficient, and bias-free deep learning model. This paper intends to provide a technical reference highlighting articles from Scopus, Web of Science, and IEEE Xplore databases pertaining to the usage of Convolutional Neural Network based techniques to detect structures and objects in the archaeological field.},
	language = {en},
	number = {1},
	urldate = {2024-01-18},
	journal = {Journal of Advanced Research in Applied Sciences and Engineering Technology},
	author = {Jamil, Amirah Hanani and Yakub, Fitri and Azizan, Azizul and Roslan, Shairatul Akma and Zaki, Sheikh Ahmad and Ahmad, Syafiq Asyraff},
	month = jan,
	year = {2022},
	note = {Number: 1},
	keywords = {Archaeological, deep learning, structure detection aids},
	pages = {7--14},
}

@book{ketkar_deep_2021,
	address = {Berkeley, CA},
	edition = {Second},
	title = {Deep learning with {Python}: learn best practices of deep learning models with {PyTorch}},
	isbn = {1484253647;9781484253649;},
	url = {https://go.exlibris.link/sydMNz9V},
	abstract = {Master the practical aspects of implementing deep learning solutions with PyTorch, using a hands-on approach to understanding both theory and practice. This updated edition will prepare you for applying deep learning to real world problems with a sound theoretical foundation and practical know-how with PyTorch, a platform developed by Facebook's Artificial Intelligence Research Group. You'll start with a perspective on how and why deep learning with PyTorch has emerged as an path-breaking framework with a set of tools and techniques to solve real-world problems. Next, the book will ground you with the mathematical fundamentals of linear algebra, vector calculus, probability and optimization. Having established this foundation, you'll move on to key components and functionality of PyTorch including layers, loss functions and optimization algorithms. You'll also gain an understanding of Graphical Processing Unit (GPU) based computation, which is essential for training deep learning models. All the key architectures in deep learning are covered, including feedforward networks, convolution neural networks, recurrent neural networks, long short-term memory networks, autoencoders and generative adversarial networks. Backed by a number of tricks of the trade for training and optimizing deep learning models, this edition of Deep Learning with Python explains the best practices in taking these models to production with PyTorch. You will: Review machine learning fundamentals such as overfitting, underfitting, and regularization. Understand deep learning fundamentals such as feed-forward networks, convolution neural networks, recurrent neural networks, automatic differentiation, and stochastic gradient descent. Apply in-depth linear algebra with PyTorch Explore PyTorch fundamentals and its building blocks Work with tuning and optimizing models .},
	language = {English},
	number = {Book, Whole},
	publisher = {Apress},
	author = {Ketkar, Nikhil and Moolayil, Jojo},
	year = {2021},
	doi = {10.1007/978-1-4842-5364-9},
	keywords = {Data mining, Machine learning, Python},
}

@article{soroush_deep_2020,
	title = {Deep {Learning} in {Archaeological} {Remote} {Sensing}: {Automated} {Qanat} {Detection} in the {Kurdistan} {Region} of {Iraq}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Deep {Learning} in {Archaeological} {Remote} {Sensing}},
	url = {https://www.mdpi.com/2072-4292/12/3/500},
	doi = {10.3390/rs12030500},
	abstract = {In this paper, we report the results of our work on automated detection of qanat shafts on the Cold War-era CORONA Satellite Imagery. The increasing quantity of air and space-borne imagery available to archaeologists and the advances in computational science have created an emerging interest in automated archaeological detection. Traditional pattern recognition methods proved to have limited applicability for archaeological prospection, for a variety of reasons, including a high rate of false positives. Since 2012, however, a breakthrough has been made in the field of image recognition through deep learning. We have tested the application of deep convolutional neural networks (CNNs) for automated remote sensing detection of archaeological features. Our case study is the qanat systems of the Erbil Plain in the Kurdistan Region of Iraq. The signature of the underground qanat systems on the remote sensing data are the semi-circular openings of their vertical shafts. We choose to focus on qanat shafts because they are promising targets for pattern recognition and because the richness and the extent of the qanat landscapes cannot be properly captured across vast territories without automated techniques. Our project is the first effort to use automated techniques on historic satellite imagery that takes advantage of neither the spectral imagery resolution nor very high (sub-meter) spatial resolution.},
	language = {en},
	number = {3},
	urldate = {2024-01-22},
	journal = {Remote Sensing},
	author = {Soroush, Mehrnoush and Mehrtash, Alireza and Khazraee, Emad and Ur, Jason A.},
	month = jan,
	year = {2020},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {CORONA, Kurdistan Region of Iraq (KRG), archaeology, convolutional neural networks (CNNs), deep learning, image segmentation, qanat, remote sensing},
	pages = {500},
}

@inproceedings{mareboyana_parallel_2006,
	title = {Parallel algorithm for linear feature detection from airborne {LiDAR} data},
	volume = {6209},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/6209/62090I/Parallel-algorithm-for-linear-feature-detection-from-airborne-LiDAR-data/10.1117/12.668782.full},
	doi = {10.1117/12.668782},
	abstract = {Linear features from airport images correspond to runways, taxiways and roads. Detecting runways helps pilots to focus on runway incursions in poor visibility conditions. In this work, we attempt to detect linear features from LiDAR swath in near real time using parallel implementation on G5-based apple cluster called Xseed. Data from LiDAR swath is converted into a uniform grid with nearest neighbor interpolation. The edges and gradient directions are computed using standard edge detection algorithms such as Canny's detector. Edge linking and detecting straight-line features are described. Preliminary results on Reno, Nevada airport data are included.},
	urldate = {2023-10-25},
	booktitle = {Airborne {Intelligence}, {Surveillance}, {Reconnaissance} ({ISR}) {Systems} and {Applications} {III}},
	publisher = {SPIE},
	author = {Mareboyana, Manohar and Chi, Paul},
	month = may,
	year = {2006},
	keywords = {LiDAR, feature extraction, line extraction, parallel algorithm},
	pages = {150--155},
}

@incollection{uhl_automating_2021,
	address = {Cham},
	title = {Automating {Information} {Extraction} from {Large} {Historical} {Topographic} {Map} {Archives}: {New} {Opportunities} and {Challenges}},
	isbn = {978-3-030-55462-0},
	shorttitle = {Automating {Information} {Extraction} from {Large} {Historical} {Topographic} {Map} {Archives}},
	url = {https://doi.org/10.1007/978-3-030-55462-0_20},
	abstract = {Historical maps constitute unique sources of retrospective geographic information. Recently, several archives containing historical map series covering large spatial and temporal extents have been systematically scanned and made available to the public. The spatial-temporal information contained in such archives represents valuable information for a myriad of scientific applications. However, this geographic information needs to be unlocked and provided in analysis-ready geospatial data formats using adequate extraction and recognition techniques that can handle the typically very large volumes of complex data and thus, requiring high degrees of automation. Whereas traditional approaches for information extraction from map documents typically involve a certain degree of user interaction, recently, a number of methods has been proposed aiming to overcome such shortcomings and to fully automate these information extraction tasks based on machine learning methods and the automated generation of training data, among others. In this chapter, we provide an overview of these recent trends, on existing, publicly available map archives, and the opportunities and challenges associated with these developments.},
	language = {en},
	urldate = {2024-01-04},
	booktitle = {Handbook of {Big} {Geospatial} {Data}},
	publisher = {Springer International Publishing},
	author = {Uhl, Johannes H. and Duan, Weiwei},
	editor = {Werner, Martin and Chiang, Yao-Yi},
	year = {2021},
	doi = {10.1007/978-3-030-55462-0_20},
	keywords = {feature extraction, historical maps, information extraction},
	pages = {509--522},
}

@inproceedings{riba_kornia_2020,
	address = {Snowmass Village, CO, USA},
	title = {Kornia: an {Open} {Source} {Differentiable} {Computer} {Vision} {Library} for {PyTorch}},
	isbn = {978-1-72816-553-0},
	shorttitle = {Kornia},
	url = {https://ieeexplore.ieee.org/document/9093363/},
	doi = {10.1109/WACV45572.2020.9093363},
	urldate = {2024-01-30},
	booktitle = {2020 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Riba, Edgar and Mishkin, Dmytro and Ponsa, Daniel and Rublee, Ethan and Bradski, Gary},
	month = mar,
	year = {2020},
	pages = {3663--3672},
}

@misc{riba_kornia_nodate,
	title = {Kornia},
	url = {https://kornia.github.io/},
	language = {en},
	urldate = {2024-01-30},
	author = {Riba, Edgar},
}

@book{campsvalls_deep_2021,
	edition = {1},
	title = {Deep {Learning} for the {Earth} {Sciences}: {A} {Comprehensive} {Approach} to {Remote} {Sensing}, {Climate} {Science}, and {Geosciences}},
	isbn = {978-1-119-64614-3 978-1-119-64618-1},
	shorttitle = {Deep {Learning} for the {Earth} {Sciences}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119646181},
	language = {en},
	urldate = {2024-01-30},
	publisher = {Wiley},
	editor = {Camps‐Valls, Gustau and Tuia, Devis and Zhu, Xiao Xiang and Reichstein, Markus},
	month = sep,
	year = {2021},
	doi = {10.1002/9781119646181},
}

@inproceedings{stewart_torchgeo_2022,
	address = {Seattle Washington},
	title = {{TorchGeo}: deep learning with geospatial data},
	isbn = {978-1-4503-9529-8},
	shorttitle = {{TorchGeo}},
	url = {https://dl.acm.org/doi/10.1145/3557915.3560953},
	doi = {10.1145/3557915.3560953},
	language = {en},
	urldate = {2024-01-29},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Advances} in {Geographic} {Information} {Systems}},
	publisher = {ACM},
	author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Ferres, Juan M. Lavista and Banerjee, Arindam},
	month = nov,
	year = {2022},
	keywords = {deep learning, geospatial, neural networks, osgeo, pytorch, remote sensing, torchgeo},
	pages = {1--12},
}

@article{naylor_extreme_2022,
	title = {Extreme weather, school logbooks and social vulnerability: {The} {Outer} {Hebrides}, {Scotland}, in the late nineteenth and early twentieth centuries},
	volume = {78},
	issn = {03057488},
	shorttitle = {Extreme weather, school logbooks and social vulnerability},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0305748822000524},
	doi = {10.1016/j.jhg.2022.07.006},
	language = {en},
	urldate = {2024-01-24},
	journal = {Journal of Historical Geography},
	author = {Naylor, Simon and Macdonald, Neil and Bowen, James P. and Endfield, Georgina},
	month = oct,
	year = {2022},
	pages = {84--94},
}

@article{macdonald_understanding_2023,
	title = {Understanding weather futures based on the past: a case of {Stornoway}, {Outer} {Hebrides}},
	volume = {139},
	issn = {1470-2541, 1751-665X},
	shorttitle = {Understanding weather futures based on the past},
	url = {https://www.tandfonline.com/doi/full/10.1080/14702541.2022.2158366},
	doi = {10.1080/14702541.2022.2158366},
	language = {en},
	number = {1-2},
	urldate = {2024-01-24},
	journal = {Scottish Geographical Journal},
	author = {Macdonald, N. and Naylor, S. and Bowen, J. P. and Harvey-Fishenden, A. and Graham, E.},
	month = apr,
	year = {2023},
	pages = {115--132},
}

@article{veale_dealing_2017,
	title = {Dealing with the deluge of historical weather data: the example of the {\textless}span style="font-variant:small-caps;"{\textgreater}{TEMPEST}{\textless}/span{\textgreater} database},
	volume = {4},
	issn = {2054-4049, 2054-4049},
	shorttitle = {Dealing with the deluge of historical weather data},
	url = {https://rgs-ibg.onlinelibrary.wiley.com/doi/10.1002/geo2.39},
	doi = {10.1002/geo2.39},
	abstract = {People have long been interested in the history of weather, particularly extremes, and chronologies of past events drawing on information from written records have been compiled and published throughout history. In recent years, concern over current and future weather and climate has triggered a new level of interest in past weather events and their impacts. This interest, alongside the development of digital humanities research methods, has resulted in a rapid growth in the number of online databases relating to historic weather and climate around the world. This paper documents the design, creation and content of one such database,
              TEMPEST
              , an online repository for extreme weather history in the
              UK
              .
              TEMPEST
              has been created as the major output of the
              AHRC
              funded project ‘Spaces of Experience and Horizons of Expectation: The Implications of Extreme Weather in the
              UK
              , Past, Present and Future’ (2013‐2017). Unlike the majority of existing databases that rely on published materials,
              TEMPEST
              's records are drawn from primary research into original documentary sources held in archives around the
              UK
              . The c. 18,000 records that
              TEMPEST
              currently contains offer personalised and geo‐referenced insights into the relationship between society and extreme weather in the
              UK
              spanning a period of over 400 years. In this paper we outline potential applications for
              TEMPEST
              and suggest directions for future research and resources in historical weather. We also consider broader issues for the digital humanities relating to the storage, archiving, ownership, and usage of data and the need to ensure connectivity between complementary datasets.},
	language = {en},
	number = {2},
	urldate = {2024-01-24},
	journal = {Geo: Geography and Environment},
	author = {Veale, Lucy and Endfield, Georgina and Davies, Sarah and Macdonald, Neil and Naylor, Simon and Royer, Marie‐Jeanne and Bowen, James and Tyler‐Jones, Richard and Jones, Cerys},
	month = jul,
	year = {2017},
	pages = {e00039},
}

@article{willyard_can_2024,
	title = {Can autoimmune diseases be cured? {Scientists} see hope at last},
	volume = {625},
	copyright = {2024 Springer Nature Limited},
	shorttitle = {Can autoimmune diseases be cured?},
	url = {https://www.nature.com/articles/d41586-024-00169-7},
	doi = {10.1038/d41586-024-00169-7},
	abstract = {After decades of frustration and failed attempts, scientists might finally be on the cusp of developing therapies to restore immune ‘tolerance’ in conditions such as diabetes, lupus and multiple sclerosis.},
	language = {en},
	number = {7996},
	urldate = {2024-01-24},
	journal = {Nature},
	author = {Willyard, Cassandra},
	month = jan,
	year = {2024},
	note = {Bandiera\_abtest: a
Cg\_type: News Feature
Number: 7996
Publisher: Nature Publishing Group
Subject\_term: Immunology, Medical research},
	keywords = {Immunology, Medical research},
	pages = {646--648},
}

@article{gonzalez_ndvi_2019,
	title = {{NDVI} {Identification} and {Survey} of a {Roman} {Road} in the {Northern} {Spanish} {Province} of Álava},
	volume = {11},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/6/725},
	doi = {10.3390/rs11060725},
	abstract = {The Iter 34 (Antonine Itinerary XXXIV) is the name of the Roman road that crosses the province of Álava from west to east. Since no specific path was...},
	language = {en},
	number = {6},
	urldate = {2024-01-22},
	journal = {Remote Sensing},
	author = {González, Juan José Fuldain and Hernández, Félix Rafael Varón},
	month = mar,
	year = {2019},
	note = {Publisher: MDPI AG},
	pages = {725},
}

@article{stott_searching_2019,
	title = {Searching for {Viking} {Age} {Fortresses} with {Automatic} {Landscape} {Classification} and {Feature} {Detection}},
	volume = {11},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/16/1881},
	doi = {10.3390/rs11161881},
	abstract = {Across the world, cultural heritage is eradicated at an unprecedented rate by development, agriculture, and natural erosion. Remote sensing using airborne...},
	language = {en},
	number = {16},
	urldate = {2024-01-22},
	journal = {Remote Sensing},
	author = {Stott, David and Kristiansen, Søren Munch and Sindbæk, Søren Michael},
	month = aug,
	year = {2019},
	note = {Publisher: MDPI AG},
	pages = {1881},
}

@article{bachagha_use_2023,
	title = {The {Use} of {Machine} {Learning} and {Satellite} {Imagery} to {Detect} {Roman} {Fortified} {Sites}: {The} {Case} {Study} of {Blad} {Talh} ({Tunisia} {Section})},
	volume = {13},
	issn = {2076-3417},
	shorttitle = {The {Use} of {Machine} {Learning} and {Satellite} {Imagery} to {Detect} {Roman} {Fortified} {Sites}},
	url = {https://www.mdpi.com/2076-3417/13/4/2613},
	doi = {10.3390/app13042613},
	abstract = {This study focuses on an ad hoc machine-learning method for locating archaeological sites in arid environments. Pleiades (P1B) were uploaded to the cloud...},
	language = {en},
	number = {4},
	urldate = {2024-01-22},
	journal = {Applied Sciences},
	author = {Bachagha, Nabil and Elnashar, Abdelrazek and Tababi, Moussa and Souei, Fatma and Xu, Wenbin},
	month = feb,
	year = {2023},
	note = {Publisher: MDPI AG},
	pages = {2613},
}

@article{kucukdemirci_deep_2020,
	title = {Deep learning based automated analysis of archaeo-geophysical images},
	volume = {27},
	copyright = {© 2020 John Wiley \& Sons, Ltd.},
	issn = {1099-0763},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/arp.1763},
	doi = {10.1002/arp.1763},
	abstract = {Thanks to recent advances in deep learning (DL) and the increasing availability of large labeled/annotated datasets and trained network models, there has been impressive progress in the automated analysis of images from different scientific domains such as medicine, microbiology, astronomy and remote sensing. The automated analysis of archaeo-geophysical data is also considered important due to the large spatial extent of areas covered by landscape surveys using multi-sensor arrays driven by motorized carts and subsequently the large volume of collected data. In this work, a convolutional neural network (CNN) is built by Python 3.6 programming language using the Deep Learning Library of Keras with Tensorflow backends, a library that implements the building blocks for CNN. The network is trained from scratch adopting U-Net architecture to accomplish an automatic analysis of the archaeo-geophysical features with emphasis on ground-penetrating radar (GPR) anomalies.},
	language = {en},
	number = {2},
	urldate = {2024-01-22},
	journal = {Archaeological Prospection},
	author = {Küçükdemirci, Melda and Sarris, Apostolos},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/arp.1763},
	keywords = {GPR (ground-penetrating radar), U-Net, archaeo-geophysics, convolutional neural networks (CNNs), deep learning, feature extraction},
	pages = {107--118},
}

@article{cao_deep_2020,
	title = {Deep learning-based remote and social sensing data fusion for urban region function recognition},
	volume = {163},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S092427162030054X},
	doi = {10.1016/j.isprsjprs.2020.02.014},
	abstract = {Urban region function recognition is key to rational urban planning and management. Due to the complex socioeconomic nature of functional land use, recognizing urban region function in high-density cities using remote sensing images alone is difficult. The inclusion of social sensing has the potential to improve the function classification performance. However, effectively integrating the multi-source and multi-modal remote and social sensing data remains technically challenging. In this paper, we have proposed a novel end-to-end deep learning-based remote and social sensing data fusion model to address this issue. Two neural network based methods, one based on a 1-dimensional convolutional neural network (CNN) and the other based on a long short-term memory (LSTM) network, have been developed to automatically extract discriminative time-dependent social sensing signature features, which are fused with remote sensing image features extracted via a residual neural network. One of the major difficulties in exploiting social and remote sensing data is that the two data sources are asynchronous. We have developed a deep learning-based strategy to address this missing modality problem by enforcing cross-modal feature consistency (CMFC) and cross-modal triplet (CMT) constraints. We train the model in an end-to-end manner by simultaneously optimizing three costs, including the classification cost, the CMFC cost and the CMT cost. Extensive experiments have been conducted on publicly available datasets to demonstrate the effectiveness of the proposed method in fusing remote and social sensing data for urban region function recognition. The results show that the seemingly unrelated physically sensed image data and social activities sensed signatures can indeed complement each other to help enhance the accuracy of urban region function recognition.},
	urldate = {2024-01-22},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Cao, Rui and Tu, Wei and Yang, Cuixin and Li, Qing and Liu, Jun and Zhu, Jiasong and Zhang, Qian and Li, Qingquan and Qiu, Guoping},
	month = may,
	year = {2020},
	keywords = {Deep learning, Multi-modal data fusion, Remote sensing, Social sensing, Urban function recognition},
	pages = {82--97},
}

@article{character_archaeologic_2021,
	title = {Archaeologic {Machine} {Learning} for {Shipwreck} {Detection} {Using} {Lidar} and {Sonar}},
	volume = {13},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/9/1759},
	doi = {10.3390/rs13091759},
	abstract = {The objective of this project is to create a new implementation of a deep learning model that uses digital elevation data to detect shipwrecks automatically...},
	language = {en},
	number = {9},
	urldate = {2024-01-22},
	journal = {Remote Sensing},
	author = {Character, Leila and Jr, Agustin Ortiz and Beach, Tim and Luzzadder-Beach, Sheryl},
	month = apr,
	year = {2021},
	note = {Publisher: MDPI AG},
	pages = {1759},
}

@article{gao_road_2019,
	title = {Road {Extraction} from {High}-{Resolution} {Remote} {Sensing} {Imagery} {Using} {Refined} {Deep} {Residual} {Convolutional} {Neural} {Network}},
	volume = {11},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/11/5/552},
	doi = {10.3390/rs11050552},
	abstract = {Road extraction is one of the most significant tasks for modern transportation systems. This task is normally difficult due to complex backgrounds such as...},
	language = {en},
	number = {5},
	urldate = {2024-01-22},
	journal = {Remote Sensing},
	author = {Gao, Lin and Song, Weidong and Dai, Jiguang and Chen, Yang},
	month = mar,
	year = {2019},
	note = {Publisher: MDPI AG},
	pages = {552},
}

@article{kadhim_critical_2023,
	title = {A {Critical} {Review} of {Remote} {Sensing} {Approaches} and {Deep} {Learning} {Techniques} in {Archaeology}},
	volume = {23},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10054340/},
	doi = {10.3390/s23062918},
	abstract = {To date, comprehensive reviews and discussions of the strengths and limitations of Remote Sensing (RS) standalone and combination approaches, and Deep Learning (DL)-based RS datasets in archaeology have been limited. The objective of this paper is, therefore, to review and critically discuss existing studies that have applied these advanced approaches in archaeology, with a specific focus on digital preservation and object detection. RS standalone approaches including range-based and image-based modelling (e.g., laser scanning and SfM photogrammetry) have several disadvantages in terms of spatial resolution, penetrations, textures, colours, and accuracy. These limitations have led some archaeological studies to fuse/integrate multiple RS datasets to overcome limitations and produce comparatively detailed outcomes. However, there are still knowledge gaps in examining the effectiveness of these RS approaches in enhancing the detection of archaeological remains/areas. Thus, this review paper is likely to deliver valuable comprehension for archaeological studies to fill knowledge gaps and further advance exploration of archaeological areas/features using RS along with DL approaches.},
	number = {6},
	urldate = {2024-01-22},
	journal = {Sensors (Basel, Switzerland)},
	author = {Kadhim, Israa and Abed, Fanar M.},
	month = mar,
	year = {2023},
	pmid = {36991628},
	pmcid = {PMC10054340},
	pages = {2918},
}

@article{olivier_implementing_2021,
	title = {Implementing {State}-of-the-{Art} {Deep} {Learning} {Approaches} for {Archaeological} {Object} {Detection} in {Remotely}-{Sensed} {Data}: {The} {Results} of {Cross}-{Domain} {Collaboration}.},
	volume = {4},
	issn = {25148362},
	shorttitle = {Implementing {State}-of-the-{Art} {Deep} {Learning} {Approaches} for {Archaeological} {Object} {Detection} in {Remotely}-{Sensed} {Data}},
	url = {https://go.gale.com/ps/i.do?p=AONE&sw=w&issn=25148362&v=2.1&it=r&id=GALE%7CA688529570&sid=googleScholar&linkaccess=abs},
	doi = {10.5334/jcaa.78},
	abstract = {{\textless}em{\textgreater}Gale{\textless}/em{\textgreater} Academic OneFile includes Implementing State-of-the-Art Deep Learning Approaches by Martin Olivier and Wouter Verschoof-van. Click to explore.},
	language = {English},
	number = {1},
	urldate = {2024-01-22},
	journal = {Journal of Computer Applications in Archaeology},
	author = {Olivier, Martin and Vaart, Wouter Verschoof-van der},
	month = dec,
	year = {2021},
	note = {Publisher: Ubiquity Press Ltd.},
	pages = {274--290},
}

@article{marmanis_classification_2018,
	title = {Classification with an edge: {Improving} semantic image segmentation with boundary detection},
	volume = {135},
	issn = {0924-2716},
	shorttitle = {Classification with an edge},
	url = {https://www.sciencedirect.com/science/article/pii/S092427161630572X},
	doi = {10.1016/j.isprsjprs.2017.11.009},
	abstract = {We present an end-to-end trainable deep convolutional neural network (DCNN) for semantic segmentation with built-in awareness of semantically meaningful boundaries. Semantic segmentation is a fundamental remote sensing task, and most state-of-the-art methods rely on DCNNs as their workhorse. A major reason for their success is that deep networks learn to accumulate contextual information over very large receptive fields. However, this success comes at a cost, since the associated loss of effective spatial resolution washes out high-frequency details and leads to blurry object boundaries. Here, we propose to counter this effect by combining semantic segmentation with semantically informed edge detection, thus making class boundaries explicit in the model. First, we construct a comparatively simple, memory-efficient model by adding boundary detection to the segnet encoder-decoder architecture. Second, we also include boundary detection in fcn-type models and set up a high-end classifier ensemble. We show that boundary detection significantly improves semantic segmentation with CNNs in an end-to-end training scheme. Our best model achieves {\textgreater}90\% overall accuracy on the ISPRS Vaihingen benchmark.},
	urldate = {2024-01-22},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Marmanis, D. and Schindler, K. and Wegner, J. D. and Galliani, S. and Datcu, M. and Stilla, U.},
	month = jan,
	year = {2018},
	pages = {158--172},
}

@article{yuan_review_2021,
	title = {A review of deep learning methods for semantic segmentation of remote sensing imagery},
	volume = {169},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417420310836},
	doi = {10.1016/j.eswa.2020.114417},
	abstract = {Semantic segmentation of remote sensing imagery has been employed in many applications and is a key research topic for decades. With the success of deep learning methods in the field of computer vision, researchers have made a great effort to transfer their superior performance to the field of remote sensing image analysis. This paper starts with a summary of the fundamental deep neural network architectures and reviews the most recent developments of deep learning methods for semantic segmentation of remote sensing imagery including non-conventional data such as hyperspectral images and point clouds. In our review of the literature, we identified three major challenges faced by researchers and summarize the innovative development to address them. As tremendous efforts have been devoted to advancing pixel-level accuracy, the emerged deep learning methods demonstrated much-improved performance on several public data sets. As to handling the non-conventional, unstructured point cloud and rich spectral imagery, the performance of the state-of-the-art methods is, on average, inferior to that of the satellite imagery. Such a performance gap also exists in learning from small data sets. In particular, the limited non-conventional remote sensing data sets with labels is an obstacle to developing and evaluating new deep learning methods.},
	urldate = {2024-01-22},
	journal = {Expert Systems with Applications},
	author = {Yuan, Xiaohui and Shi, Jianfang and Gu, Lichuan},
	month = may,
	year = {2021},
	keywords = {Deep neural networks, Remote sensing imagery, Semantic image segmentation},
	pages = {114417},
}

@article{altaweel_automated_2022,
	title = {Automated {Archaeological} {Feature} {Detection} {Using} {Deep} {Learning} on {Optical} {UAV} {Imagery}: {Preliminary} {Results}},
	volume = {14},
	issn = {2072-4292},
	shorttitle = {Automated {Archaeological} {Feature} {Detection} {Using} {Deep} {Learning} on {Optical} {UAV} {Imagery}},
	url = {https://www.mdpi.com/2072-4292/14/3/553},
	doi = {10.3390/rs14030553},
	abstract = {This communication article provides a call for unmanned aerial vehicle (UAV) users in archaeology to make imagery data more publicly available while...},
	language = {en},
	number = {3},
	urldate = {2024-01-22},
	journal = {Remote Sensing},
	author = {Altaweel, Mark and Khelifi, Adel and Li, Zehao and Squitieri, Andrea and Basmaji, Tasnim and Ghazal, Mohammed},
	month = jan,
	year = {2022},
	note = {Publisher: MDPI AG},
	pages = {553},
}

@article{verschoof-van_der_vaart_combining_2020,
	title = {Combining {Deep} {Learning} and {Location}-{Based} {Ranking} for {Large}-{Scale} {Archaeological} {Prospection} of {LiDAR} {Data} from {The} {Netherlands}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2220-9964},
	url = {https://www.mdpi.com/2220-9964/9/5/293},
	doi = {10.3390/ijgi9050293},
	abstract = {This paper presents WODAN2.0, a workflow using Deep Learning for the automated detection of multiple archaeological object classes in LiDAR data from the Netherlands. WODAN2.0 is developed to rapidly and systematically map archaeology in large and complex datasets. To investigate its practical value, a large, random test dataset—next to a small, non-random dataset—was developed, which better represents the real-world situation of scarce archaeological objects in different types of complex terrain. To reduce the number of false positives caused by specific regions in the research area, a novel approach has been developed and implemented called Location-Based Ranking. Experiments show that WODAN2.0 has a performance of circa 70\% for barrows and Celtic fields on the small, non-random testing dataset, while the performance on the large, random testing dataset is lower: circa 50\% for barrows, circa 46\% for Celtic fields, and circa 18\% for charcoal kilns. The results show that the introduction of Location-Based Ranking and bagging leads to an improvement in performance varying between 17\% and 35\%. However, WODAN2.0 does not reach or exceed general human performance, when compared to the results of a citizen science project conducted in the same research area.},
	language = {en},
	number = {5},
	urldate = {2024-01-22},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Verschoof-van der Vaart, Wouter B. and Lambers, Karsten and Kowalczyk, Wojtek and Bourgeois, Quentin P. J.},
	month = may,
	year = {2020},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Deep Learning, Faster R-CNN, LiDAR, The Netherlands, citizen science},
	pages = {293},
}

@article{verschoof-van_der_vaart_applying_2022,
	title = {Applying automated object detection in archaeological practice: {A} case study from the southern {Netherlands}},
	volume = {29},
	copyright = {© 2021 The Authors. Archaeological Prospection published by John Wiley \& Sons Ltd.},
	issn = {1099-0763},
	shorttitle = {Applying automated object detection in archaeological practice},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/arp.1833},
	doi = {10.1002/arp.1833},
	abstract = {Within archaeological prospection, Deep Learning algorithms are developed to detect objects within large remotely sensed datasets. These approaches are generally tested in an (ideal) experimental setting but have not been applied in different contexts or ‘in the wild’, that is, incorporated in archaeological prospection. This research explores the applicability, knowledge discovery—on both a quantitative and qualitative level—and efficiency gain resulting from employing an automated detection tool called WODAN within (Dutch) archaeological practice. WODAN has been used to detect barrows and Celtic fields in LiDAR data from the Dutch Midden-Limburg area, which differs in archaeology, geo-(morpho)logy and land-use from the Veluwe in which it was developed. The results show that WODAN was able to detect potential barrows and Celtic fields, including previously unknown examples, and provided information about the structuring of the landscape in the past. Based on the results, combined human-computer strategies are argued, in which automated detection has a complementary, rather than a substitute role, to manual analysis. This can offset the inherent biases in manual analysis and deal with the problem that current automated detection methods only detect objects similar to the pre-defined target class(es). The incorporation of automated detection into archaeological prospection, in which the results of automated detection are used to highlight areas of interest and to enhance and add detail to existing archaeological predictive maps, seems logical and feasible.},
	language = {en},
	number = {1},
	urldate = {2024-01-22},
	journal = {Archaeological Prospection},
	author = {Verschoof-van der Vaart, Wouter B. and Lambers, Karsten},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/arp.1833},
	keywords = {Lidar, Netherlands, archaeological prospection, deep learning, landscape archaeology, object detection},
	pages = {15--31},
}

@incollection{todd_scotland_2004,
	address = {Malden, MA, USA},
	title = {Scotland and the {Northern} {Frontier}: {Second} to {Fourth} {Centuries} {AD}},
	isbn = {978-0-470-99886-1 978-0-631-21823-4},
	shorttitle = {Scotland and the {Northern} {Frontier}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470998861.ch9},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {A {Companion} to {Roman} {Britain}},
	publisher = {Blackwell Publishing Ltd},
	author = {Hanson, W. S.},
	editor = {Todd, Malcolm},
	month = jan,
	year = {2004},
	doi = {10.1002/9780470998861.ch9},
	pages = {136--161},
}

@article{aqdus_potential_2012,
	title = {The potential of hyperspectral and multi-spectral imagery to enhance archaeological cropmark detection: a comparative study},
	volume = {39},
	issn = {0305-4403},
	shorttitle = {The potential of hyperspectral and multi-spectral imagery to enhance archaeological cropmark detection},
	url = {https://www.sciencedirect.com/science/article/pii/S0305440312000465},
	doi = {10.1016/j.jas.2012.01.034},
	abstract = {Aerial photography has made the single most important contribution to our improved appreciation of the density, diversity and distribution of archaeological sites in Britain since World War Two. This is particularly the case for areas of intensive lowland agriculture where ploughed-out sites are known mainly from marks in the crops growing above them. However, reconnaissance for such cropmarks is not equally effective throughout the lowlands, because of the particular conditions of drier weather, well-drained soils and arable agriculture required before they become visible, and is highly unpredictable. Given that the appearance of cropmarks is linked to moisture stress in growing plants, they are potentially detectable at bandwidths outside the visible spectrum and before they become apparent therein. This paper focuses on the application of two spectral enhancement techniques: Principal component analysis and Tasselled cap transformation. Comparing a range of imagery (CASI-2, ATM and digital vertical photographic data) from two case study areas in Lowland Scotland, each with very different environmental, agricultural and archaeological backgrounds to facilitate further comparisons, the paper demonstrates that multi-spectral/hyperspectral imagery can enhance the identification of otherwise invisible archaeological sites, particularly in the near-infrared part of the spectrum. However, the lower spatial resolution of such imagery, compared to photography, can make the often diffuse and incomplete cropmark traces more difficult to determine with confidence.},
	number = {7},
	urldate = {2024-01-18},
	journal = {Journal of Archaeological Science},
	author = {Aqdus, Syed Ali and Hanson, William S. and Drummond, Jane},
	month = jul,
	year = {2012},
	keywords = {ATM, Aerial archaeology, CASI, Clyde valley, Cropmarks, Hyperspectral, Inveresk, Multi-spectral, Principal component analysis, Scotland, Tasselled cap transformation},
	pages = {1915--1924},
}

@incollection{hanson_spy_2013,
	address = {New York, NY},
	title = {A {Spy} in the {Sky}: {The} {Potential} of {Historical} {Aerial} and {Satellite} {Photography} for {Archaeological} {Research}},
	isbn = {978-1-4614-4505-0},
	shorttitle = {A {Spy} in the {Sky}},
	url = {https://doi.org/10.1007/978-1-4614-4505-0_1},
	abstract = {Aerial photography has facilitated recognition of the density, diversity and complexity of human settlement activity across the fertile lowlands of Europe over millennia, but application of the standard technique of observer-directed archaeological aerial reconnaissance is not universal for a variety of reasons. This introductory chapter highlights the considerable and largely untapped potential of historical aerial and satellite photography for archaeological area survey and landscape analysis, contextualising the examples contained in the volume, which range widely both geographically and chronologically. It draws attention to the range of archival sources available and to the additional benefits of using them, including visualisation of the landscape as it was half a century or more ago before the destructive impact of late twentieth-century development; time-change analysis of the condition of known archaeological monuments; and the discovery of archaeological sites now destroyed.},
	language = {en},
	urldate = {2024-01-18},
	booktitle = {Archaeology from {Historical} {Aerial} and {Satellite} {Archives}},
	publisher = {Springer},
	author = {Hanson, William S. and Oltean, Ioana A.},
	editor = {Hanson, William S. and Oltean, Ioana A.},
	year = {2013},
	doi = {10.1007/978-1-4614-4505-0_1},
	keywords = {Aerial Photography, Archaeological Research, Archaeological Site, Khmer Rouge, Satellite Photograph},
	pages = {3--10},
}

@article{giger_machine_2018,
	series = {Data {Science}: {Big} {Data} {Machine} {Learning} and {Artificial} {Intelligence}},
	title = {Machine {Learning} in {Medical} {Imaging}},
	volume = {15},
	issn = {1546-1440},
	url = {https://www.sciencedirect.com/science/article/pii/S1546144017316733},
	doi = {10.1016/j.jacr.2017.12.028},
	abstract = {Advances in both imaging and computers have synergistically led to a rapid rise in the potential use of artificial intelligence in various radiological imaging tasks, such as risk assessment, detection, diagnosis, prognosis, and therapy response, as well as in multi-omics disease discovery. A brief overview of the field is given here, allowing the reader to recognize the terminology, the various subfields, and components of machine learning, as well as the clinical potential. Radiomics, an expansion of computer-aided diagnosis, has been defined as the conversion of images to minable data. The ultimate benefit of quantitative radiomics is to (1) yield predictive image-based phenotypes of disease for precision medicine or (2) yield quantitative image-based phenotypes for data mining with other -omics for discovery (ie, imaging genomics). For deep learning in radiology to succeed, note that well-annotated large data sets are needed since deep networks are complex, computer software and hardware are evolving constantly, and subtle differences in disease states are more difficult to perceive than differences in everyday objects. In the future, machine learning in radiology is expected to have a substantial clinical impact with imaging examinations being routinely obtained in clinical practice, providing an opportunity to improve decision support in medical image interpretation. The term of note is decision support, indicating that computers will augment human decision making, making it more effective and efficient. The clinical impact of having computers in the routine clinical practice may allow radiologists to further integrate their knowledge with their clinical colleagues in other medical specialties and allow for precision medicine.},
	number = {3, Part B},
	urldate = {2024-01-18},
	journal = {Journal of the American College of Radiology},
	author = {Giger, Maryellen L.},
	month = mar,
	year = {2018},
	keywords = {Machine learning, computer-aided diagnosis, computer-assisted decision support, deep learning, radiomics},
	pages = {512--520},
}

@article{litjens_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	volume = {42},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841517301135},
	doi = {10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
	urldate = {2024-01-18},
	journal = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sánchez, Clara I.},
	month = dec,
	year = {2017},
	keywords = {Convolutional neural networks, Deep learning, Medical imaging, Survey},
	pages = {60--88},
}

@article{bundzel_semantic_2020,
	title = {Semantic {Segmentation} of {Airborne} {LiDAR} {Data} in {Maya} {Archaeology}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/22/3685},
	doi = {10.3390/rs12223685},
	abstract = {Airborne LiDAR produced large amounts of data for archaeological research over the past decade. Labeling this type of archaeological data is a tedious process. We used a data set from Pacunam LiDAR Initiative survey of lowland Maya region in Guatemala. The data set contains ancient Maya structures that were manually labeled, and ground verified to a large extent. We have built and compared two deep learning-based models, U-Net and Mask R-CNN, for semantic segmentation. The segmentation models were used in two tasks: identification of areas of ancient construction activity, and identification of the remnants of ancient Maya buildings. The U-Net based model performed better in both tasks and was capable of correctly identifying 60–66\% of all objects, and 74–81\% of medium sized objects. The quality of the resulting prediction was evaluated using a variety of quantifiers. Furthermore, we discuss the problems of re-purposing the archaeological style labeling for production of valid machine learning training sets. Ultimately, we outline the value of these models for archaeological research and present the road map to produce a useful decision support system for recognition of ancient objects in LiDAR data.},
	language = {en},
	number = {22},
	urldate = {2024-01-18},
	journal = {Remote Sensing},
	author = {Bundzel, Marek and Jaščur, Miroslav and Kováč, Milan and Lieskovský, Tibor and Sinčák, Peter and Tkáčik, Tomáš},
	month = jan,
	year = {2020},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LiDAR, Mask R-CNN, Maya archaeology, U-Net, convolutional neural network, decision support, semantic segmentation},
	pages = {3685},
}

@inproceedings{phelan_detection_2020,
	title = {Detection of ringforts from aerial photography using machine learning},
	url = {https://ieeexplore.ieee.org/document/9180159},
	doi = {10.1109/ISSC49989.2020.9180159},
	abstract = {Ringforts are one of the most populous field monuments in Ireland with approximately 45000 examples surviving to date. Their distribution and dispersal patterns are key to our understanding of the habitation patterns of our ancestors. Due to the nature of these structures and the construction materials used, centuries of abandonment means that they often go unnoticed at ground level, while being easily identified from an aerial perspective. The increased requirements of land use for the development of urban areas, infrastructure and increased industrialised farming practices means that these monuments are under threat. Recent developments in the field of machine learning coupled with access to hi-resolution multi-spectral satellite imagery from Open Data sources, presents the opportunity to investigate the development of a system for the automated detection of these features. If successful, such a system could provide an automated, efficient and cost effective tool for the detection of interference or destruction of known sites as well as the discovery of new ones.},
	urldate = {2024-01-18},
	booktitle = {2020 31st {Irish} {Signals} and {Systems} {Conference} ({ISSC})},
	author = {Phelan, Keith and Riordan, Daniel},
	month = jun,
	year = {2020},
	note = {ISSN: 2688-1454},
	pages = {1--6},
}

@article{wei_fine-grained_2022,
	title = {Fine-{Grained} {Image} {Analysis} {With} {Deep} {Learning}: {A} {Survey}},
	volume = {44},
	issn = {1939-3539},
	shorttitle = {Fine-{Grained} {Image} {Analysis} {With} {Deep} {Learning}},
	url = {https://ieeexplore.ieee.org/abstract/document/9609630},
	doi = {10.1109/TPAMI.2021.3126648},
	abstract = {Fine-grained image analysis (FGIA) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. The task of FGIA targets analyzing visual objects from subordinate categories, e.g., species of birds or models of cars. The small inter-class and large intra-class variation inherent to fine-grained image analysis makes it a challenging problem. Capitalizing on advances in deep learning, in recent years we have witnessed remarkable progress in deep learning powered FGIA. In this paper we present a systematic survey of these advances, where we attempt to re-define and broaden the field of FGIA by consolidating two fundamental fine-grained research areas – fine-grained image recognition and fine-grained image retrieval. In addition, we also review other key issues of FGIA, such as publicly available benchmark datasets and related domain-specific applications. We conclude by highlighting several research directions and open problems which need further exploration from the community.},
	number = {12},
	urldate = {2024-01-18},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wei, Xiu-Shen and Song, Yi-Zhe and Aodha, Oisin Mac and Wu, Jianxin and Peng, Yuxin and Tang, Jinhui and Yang, Jian and Belongie, Serge},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages = {8927--8948},
}

@article{mas_application_2008,
	title = {The application of artificial neural networks to the analysis of remotely sensed data},
	volume = {29},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160701352154},
	doi = {10.1080/01431160701352154},
	abstract = {Artificial neural networks (ANNs) have become a popular tool in the analysis of remotely sensed data. Although significant progress has been made in image classification based upon neural networks, a number of issues remain to be resolved. This paper reviews remotely sensed data analysis with neural networks. First, we present an overview of the main concepts underlying ANNs, including the main architectures and learning algorithms. Then, the main tasks that involve ANNs in remote sensing are described. The limitations and crucial issues relating to the application of the neural network approach are discussed. A brief review of the implementation of ANNs in some of the most popular image processing software packages is presented. Finally, we discuss the application perspectives of neural networks in remote sensing image analysis.},
	number = {3},
	urldate = {2024-01-18},
	journal = {International Journal of Remote Sensing},
	author = {Mas, J. F. and Flores, J. J.},
	month = feb,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431160701352154},
	pages = {617--663},
}

@article{canny_computational_1986,
	title = {A computational approach to edge detection},
	volume = {8},
	issn = {0162-8828},
	abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
	language = {eng},
	number = {6},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Canny, J.},
	month = jun,
	year = {1986},
	pmid = {21869365},
	pages = {679--698},
}

@article{small_lost_2016,
	title = {The lost {Roman} road from {Chichester} to {Arundel}},
	volume = {4},
	url = {https://historicengland.org.uk/whats-new/research/back-issues/lost-roman-road/},
	journal = {Historic England Research. Uncovering our urban wetlands},
	author = {Small, Fiona},
	year = {2016},
	pages = {2--8},
}

@misc{university_of_portsmouth_vision_2023,
	title = {Vision of {Britain}},
	url = {https://www.visionofbritain.org.uk/data/},
	urldate = {2024-01-03},
	author = {University of Portsmouth},
	year = {2023},
}

@inproceedings{li_automatic_2020,
	address = {Virtual Event CA USA},
	title = {An {Automatic} {Approach} for {Generating} {Rich}, {Linked} {Geo}-{Metadata} from {Historical} {Map} {Images}},
	isbn = {978-1-4503-7998-4},
	url = {https://dl.acm.org/doi/10.1145/3394486.3403381},
	doi = {10.1145/3394486.3403381},
	language = {en},
	urldate = {2024-01-14},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Li, Zekun and Chiang, Yao-Yi and Tavakkol, Sasan and Shbita, Basel and Uhl, Johannes H. and Leyk, Stefan and Knoblock, Craig A.},
	month = aug,
	year = {2020},
	pages = {3290--3298},
}

@article{apan_mapping_2002,
	title = {Mapping and analysis of changes in the riparian landscape structure of the {Lockyer} {Valley} catchment, {Queensland}, {Australia}},
	volume = {59},
	issn = {01692046},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0169204601002468},
	doi = {10.1016/S0169-2046(01)00246-8},
	language = {en},
	number = {1},
	urldate = {2024-01-14},
	journal = {Landscape and Urban Planning},
	author = {Apan, Armando A. and Raine, Steven R. and Paterson, Mark S.},
	month = mar,
	year = {2002},
	keywords = {Relief presentation, relief analysis},
	pages = {43--57},
}

@article{roscam_abbing_shifting_2024,
	title = {Shifting your research from {X} to {Mastodon}? {Here}’s what you need to know},
	volume = {5},
	issn = {2666-3899},
	shorttitle = {Shifting your research from {X} to {Mastodon}?},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389923003239},
	doi = {10.1016/j.patter.2023.100914},
	abstract = {Since Elon Musk’s purchase of Twitter/X and subsequent changes to that platform, computational social science researchers may be considering shifting their research programs to Mastodon and the fediverse. This article sounds several notes of caution about such a shift. We explain key differences between the fediverse and X, ultimately arguing that research must be with the fediverse, not on it.},
	number = {1},
	urldate = {2024-01-14},
	journal = {Patterns},
	author = {Roscam Abbing, Roel and Gehl, Robert W.},
	month = jan,
	year = {2024},
	keywords = {mastodon, social media, twitter},
	pages = {100914},
}

@article{rostain_two_2024,
	title = {Two thousand years of garden urbanism in the {Upper} {Amazon}},
	volume = {383},
	url = {https://www.science.org/doi/10.1126/science.adi6317},
	doi = {10.1126/science.adi6317},
	abstract = {A dense system of pre-Hispanic urban centers has been found in the Upano Valley of Amazonian Ecuador, in the eastern foothills of the Andes. Fieldwork and light detection and ranging (LIDAR) analysis have revealed an anthropized landscape with clusters of monumental platforms, plazas, and streets following a specific pattern intertwined with extensive agricultural drainages and terraces as well as wide straight roads running over great distances. Archaeological excavations date the occupation from around 500 BCE to between 300 and 600 CE. The most notable landscape feature is the complex road system extending over tens of kilometers, connecting the different urban centers, thus creating a regional-scale network. Such extensive early development in the Upper Amazon is comparable to similar Maya urban systems recently highlighted in Mexico and Guatemala.},
	number = {6679},
	urldate = {2024-01-13},
	journal = {Science},
	author = {Rostain, Stéphen and Dorison, Antoine and de Saulieu, Geoffroy and Prümers, Heiko and Le Pennec, Jean-Luc and Mejía Mejía, Fernando and Freire, Ana Maritza and Pagán-Jiménez, Jaime R. and Descola, Philippe},
	month = jan,
	year = {2024},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {183--189},
}

@article{challis_generic_2011,
	title = {A {Generic} {Toolkit} for the {Visualization} of {Archaeological} {Features} on {Airborne} {LiDAR} {Elevation} {Data}},
	volume = {18},
	copyright = {Copyright © 2011 John Wiley \& Sons, Ltd.},
	issn = {1099-0763},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/arp.421},
	doi = {10.1002/arp.421},
	abstract = {A range of techniques have become established for the visualization and analysis of airborne LiDAR elevation data within the field of archaeology. In this paper we discuss the visualization of test data representing archaeological features in a variety of terrains using a suite of techniques, all available through generic geographical information system or image processing software. These comprise elevation shading using constrained colour ramps, slope analysis, hill-shading, principal component analysis of multi-azimuth hill-shading, local relief models and solar insolation modelling. The strengths and weaknesses of each technique are discussed and a generic toolkit, suited to the visualization of airborne LiDAR data for archaeological purposes, is presented. Copyright © 2011 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {4},
	urldate = {2024-01-09},
	journal = {Archaeological Prospection},
	author = {Challis, Keith and Forlin, Paolo and Kincey, Mark},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/arp.421},
	keywords = {LiDAR, analytical techniques, archaeology, toolkit, visualization},
	pages = {279--289},
}

@article{hesse_lidar-derived_2010,
	title = {{LiDAR}-derived {Local} {Relief} {Models} – a new tool for archaeological prospection},
	volume = {17},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1099-0763},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/arp.374},
	doi = {10.1002/arp.374},
	abstract = {Local relief models (LRM) are proposed as a new tool for archaeological prospection. A data processing approach is presented which produces LRM from LiDAR-derived high-resolution digital elevation models (DEMs). The LRM represents local, small-scale elevation differences after removing the large-scale landscape forms from the data. The LRM greatly enhances the visibility of small-scale, shallow topographic features irrespective of the illumination angle and allows their relative elevations as well as their volumes to be measured directly. This makes the LRM an improved basis for spatially extensive archaeological prospection over a wide range of landscapes. The LRM raster map of local positive and negative relief variations can be used for the mapping and prospection of archaeological features such as burial mounds, linear and circular earthworks, sunken roads, agricultural terraces, ridge and furrow fields, kiln podia and mining/quarrying sites. This approach is currently being used in a project aimed at the complete archaeological mapping and prospection of the state Baden-Württemberg (Germany), covering an area of 35 751 km2. The goal of the project is the verification and extension of the existing archaeological data base. An object-based local relief vector layer is produced as a by-product; however, due to the common amalgamation of natural and anthropogenic features this cannot be used efficiently for archaeological prospection at present. Copyright © 2010 John Wiley \& Sons, Ltd.},
	language = {de},
	number = {2},
	urldate = {2024-01-09},
	journal = {Archaeological Prospection},
	author = {Hesse, Ralf},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/arp.374},
	keywords = {Baden-Württemberg, LiDAR, airborne laser scanning, archaeological prospection, data processing, local relief model},
	pages = {67--72},
}

@article{zaksek_sky-view_2011,
	title = {Sky-{View} {Factor} as a {Relief} {Visualization} {Technique}},
	volume = {3},
	issn = {2072-4292},
	url = {http://www.mdpi.com/2072-4292/3/2/398},
	doi = {10.3390/rs3020398},
	language = {en},
	number = {2},
	urldate = {2024-01-09},
	journal = {Remote Sensing},
	author = {Zakšek, Klemen and Oštir, Kristof and Kokalj, Žiga},
	month = feb,
	year = {2011},
	keywords = {Archaeological, Relief presentation, Visualisation},
	pages = {398--415},
}

@article{costa-garcia_reassessment_2019,
	title = {{THE} {REASSESSMENT} {OF} {THE} {ROMAN} {MILITARY} {PRESENCE} {IN} {GALICIA} {AND} {NORTHERN} {PORTUGAL} {THROUGH} {DIGITAL} {TOOLS}: {ARCHAEOLOGICAL} {DIVERSITY} {AND} {HISTORICAL} {PROBLEMS}},
	shorttitle = {{THE} {REASSESSMENT} {OF} {THE} {ROMAN} {MILITARY} {PRESENCE} {IN} {GALICIA} {AND} {NORTHERN} {PORTUGAL} {THROUGH} {DIGITAL} {TOOLS}},
	url = {https://zenodo.org/records/3457524},
	doi = {10.5281/zenodo.3457524},
	abstract = {Traditionally, the study of the Roman military presence in Galicia (Spain) and Northern Portugal has been based on the fragmentary documentation offered by Greek and Latin authors or epigraphy, with archaeology occupying a very secondary place in these historical narratives. In particular, the information is very scarce for the period between the 2nd century BCE and 1st century CE, when these territories were conquered and integrated into the Roman world. This work presents new Roman military sites discovered through an integrated methodology involving an intensive application of remote sensing techniques in order to provide information to foster a paradigm shift in this field of study. Distributed over a wide geographical area and displaying a wide morpho-typological and locational diversity, this new archaeological evidence not only reflects the ability of the Roman army to adapt to local natural and cultural environments, but also reveals a major operational and logistical assortment that may relate to the diachronic nature of the military presence in the region.},
	urldate = {2024-01-09},
	author = {Costa-García, J-M. and Fonte, J. and Gago, M.},
	month = sep,
	year = {2019},
	keywords = {Archaeological prospection, Historic narratives, NW Iberia, Remote sensing, Roman military sites},
}

@article{guimil-farina_dotting_2015,
	title = {“{Dotting} the joins”: a non-reconstructive use of {Least} {Cost} {Paths} to approach ancient roads. {The} case of the {Roman} roads in the {NW} {Iberian} {Peninsula}},
	volume = {54},
	issn = {0305-4403},
	shorttitle = {“{Dotting} the joins”},
	url = {https://www.sciencedirect.com/science/article/pii/S0305440314004439},
	doi = {10.1016/j.jas.2014.11.030},
	abstract = {The use of GIS tools to explore questions related to movement in archaeological contexts has been common in the last years. Least Cost Paths (LCP) have been especially successful among them, most often with the objective of predicting or reconstructing the layout of ancient routes. In this paper we propose an alternate use of those tools, aimed at trying to identify the main locations taken into account when defining the routes, rather than at predicting or reconstructing them. Through a rather simple and straightforward methodological sequence, based on the successive testing of very explicit hypotheses, we show how this approach can produce significant new knowledge while dodging some typical issues of LCP analysis. We illustrate the approach with the case study of the Roman roads in the north-west Iberian Peninsula.},
	urldate = {2024-01-09},
	journal = {Journal of Archaeological Science},
	author = {Güimil-Fariña, Alejandro and Parcero-Oubiña, César},
	month = feb,
	year = {2015},
	keywords = {Ancient roads, GIS, Least Cost Paths, NW Iberian Peninsula, Roman roads, Route layout factors},
	pages = {31--44},
}

@article{gethin_roman_2014,
	title = {The {Roman} {Marching} {Camp} and {Road} at {Loups} {Fell}, {Tebay}},
	volume = {45},
	issn = {0068-113X, 1753-5352},
	url = {https://www.cambridge.org/core/journals/britannia/article/abs/roman-marching-camp-and-road-at-loups-fell-tebay/26D79C8329B6DDEC0131D5BD1C5EBF0A},
	doi = {10.1017/S0068113X13000548},
	abstract = {A 15.2-ha Roman marching camp has been identified at the northern end of the Lune gorge where the Roman road northwards from Ribchester emerges into the Eden valley and the Cumbrian plain. The road runs here on an alignment that has not been determined before. The camp lies astride the road and faced east. The remains of the camp and the road are described and discussed.},
	language = {en},
	urldate = {2024-01-09},
	journal = {Britannia},
	author = {Gethin, Bryn and Toller, Hugh},
	month = nov,
	year = {2014},
	note = {Publisher: Cambridge University Press},
	keywords = {Loups Fell, Roman army, Roman road, Tebay, marching camp},
	pages = {1--10},
}

@inproceedings{yousaf_comparative_2018,
	title = {A {Comparative} {Study} of {Various} {Edge} {Detection} {Methods}},
	url = {https://ieeexplore.ieee.org/document/8564267},
	doi = {10.1109/CIS2018.2018.00029},
	abstract = {Edge detection is the core research area among different fields such as; image processing, computer vision, machine learning and pattern recognition. In object detection, the first obligatory step is to determine the edges of an object (feature vector) in better way that is further used for processing. The feature vector comprises of nothing but key point description, which provides the information of edges. Edge detection is the key to success and is somehow or the other dependent on it. This paper unfolds the different state-of-the-art edge detection methods and a mild comparison between different edge detection methods.},
	urldate = {2024-01-05},
	booktitle = {2018 14th {International} {Conference} on {Computational} {Intelligence} and {Security} ({CIS})},
	author = {Yousaf, Rehan Mehmood and Habib, Hafiz Adnan and Dawood, Hussain and Shafiq, Sidra},
	month = nov,
	year = {2018},
	keywords = {CANNY algorithm, log algorithm, review},
	pages = {96--99},
}

@inproceedings{song_new_2002,
	title = {A new approach for line recognition in large-size images using {Hough} transform},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/1044582},
	doi = {10.1109/ICPR.2002.1044582},
	abstract = {Applications of the Hough Transform (HT) have been limited to small-size images for a long time. For large-size images, peak detection and line verification become much more time-consuming. Many HT-based line detection methods are not able to detect line width. This paper proposes a new approach for detecting line segments using HT, with applicability to large size images, especially for those situations where line width is critical. Our approach applies a boundary recorder to eliminate redundant analyses, and employs an image-analysis-based line-verification method to overcome the difficulty of using a threshold to distinguish short lines from noise. It avoids overlapping lines by removing the pixels of detected line segments, a method which is more robust than only clearing the N/spl times/N neighborhood. This approach could be easily extended to improved HT methods that perform global accumulation. Experimental results show that this approach is very time efficient for large-size images.},
	urldate = {2024-01-05},
	booktitle = {2002 {International} {Conference} on {Pattern} {Recognition}},
	author = {Song, Jiqiang and Cai, Min and Lyu, M.R. and Cai, Shijie},
	month = aug,
	year = {2002},
	note = {ISSN: 1051-4651},
	keywords = {Hough transform, hough, line extraction},
	pages = {33--36 vol.1},
}

@article{klemmer_satellite_2024,
	title = {Satellite images reveal untracked human activity on the oceans},
	volume = {625},
	copyright = {2024 Springer Nature Limited},
	url = {https://www.nature.com/articles/d41586-023-03983-7},
	doi = {10.1038/d41586-023-03983-7},
	abstract = {Machine learning enables unprecedented mapping of industry at sea.},
	language = {en},
	number = {7993},
	urldate = {2024-01-06},
	journal = {Nature},
	author = {Klemmer, Konstantin and Rolf, Esther},
	month = jan,
	year = {2024},
	note = {Bandiera\_abtest: a
Cg\_type: News And Views
Number: 7993
Publisher: Nature Publishing Group
Subject\_term: Environmental sciences},
	keywords = {Environmental sciences},
	pages = {31--32},
}

@inproceedings{zheng_contour_2012,
	address = {Dordrecht},
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {Contour {Lines} {Extraction} from {Color} {Scanned} {Topographical} {Maps} with {Improved} {Snake} {Algorithm}},
	isbn = {978-94-007-1839-5},
	doi = {10.1007/978-94-007-1839-5_125},
	abstract = {Contour lines are the most important features to characterize three dimensional terrain on color scanned topographical map sheet. Based on the analysis of characteristics of contour lines, a novel snake model for automated tracking is proposed. With the variable force control and gradient vector flow, contour lines can be vectorized automatically. The tracking algorithm is significant for its flexible form and combination of global field information with local tracking. Furthermore, the algorithm has general reference to other kinds of graphical vectorization.},
	language = {en},
	booktitle = {Computer, {Informatics}, {Cybernetics} and {Applications}},
	publisher = {Springer Netherlands},
	author = {Zheng, Huali and Guo, ZhouWei},
	editor = {He, Xingui and Hua, Ertian and Lin, Yun and Liu, Xiaozhu},
	year = {2012},
	keywords = {Contour lines, Snake algorithm, Topographical map},
	pages = {1153--1160},
}

@article{mohammadpour_automatic_2020,
	title = {Automatic {Lineament} {Extraction} {Method} in {Mineral} {Exploration} {Using} {CANNY} {Algorithm} and {Hough} {Transform}},
	volume = {54},
	issn = {1556-1976},
	url = {https://doi.org/10.1134/S0016852120030085},
	doi = {10.1134/S0016852120030085},
	abstract = {Copper mineralization in Kahang porphyry mining district as a tectonically active region in the center of Iran, at the middle of Urmia–Dokhtar magmatic assemblage (UDMA), has been drastically controlled by structural lineaments. Determination of these footprint features is a good guide to identify the location of ore occurrences. One way to recognize lineaments is to process satellite imagery along with geophysical data. Although the visual extraction of lineaments from such data set is the most common method, automatic methods for detecting lineaments can highly reduce user errors and runtime. The most efficient automated methods in this regard are those that simultaneously take edge detection filters with lineament extraction algorithms into consideration. In this work, the CANNY algorithm was employed as an edge-detector filter at first and later Hough transform was used to extract linear features from satellite imagery and geophysical magnetometry data. The proposed methods were implemented on the high-resolution imagery data collected by QuickBird satellite along with ground-based magnetometry data to extract the shallow and deep-seated lineaments. After investigating and plotting this structural controller in a map, the dominant orientation was in the NE–SW direction perpendicular to the UDMA. Generated lineament density map also indicated that the eastern, southeast, and western portions of the area had high potential for porphyry copper mineralization.},
	language = {en},
	number = {3},
	urldate = {2024-01-05},
	journal = {Geotectonics},
	author = {Mohammadpour, M. and Bahroudi, A. and Abedi, M.},
	month = may,
	year = {2020},
	keywords = {CANNY algorithm, Hough transform, automatic lineament extraction, edge detection, porphyry copper},
	pages = {366--382},
}

@article{zhang_fast_1984,
	title = {A fast parallel algorithm for thinning digital patterns},
	volume = {27},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/357994.358023},
	doi = {10.1145/357994.358023},
	number = {3},
	urldate = {2024-01-05},
	journal = {Communications of the ACM},
	author = {Zhang, T. Y. and Suen, C. Y.},
	month = mar,
	year = {1984},
	keywords = {parallel algorithm, skeletonization, thinning of digital patterns},
	pages = {236--239},
}

@article{princen_hierarchical_1990,
	title = {A hierarchical approach to line extraction based on the {Hough} transform},
	volume = {52},
	issn = {0734-189X},
	url = {https://www.sciencedirect.com/science/article/pii/0734189X9090123D},
	doi = {10.1016/0734-189X(90)90123-D},
	abstract = {An efficient method for finding straight lines in edge maps is described. The algorithm is based on a pyramid structure with each layer in the pyramid splitting the complete image into a number of subimages. At the bottom level of the pyramid short line segments are detected by applying a Hough transform to small subimages. The algorithm proceeds, bottom up, from this low level description by grouping line segments within local neighborhoods into longer lines. Line segments which have local support propagate up the hierarchy and take part in grouping at higher levels. The length of a line determines approximately the level in the pyramid to which it propagates. Hence we obtain a hierarchical description of the line segments in a scene which can be useful in matching. The algorithm has a number of advantages over previously proposed hierarchical methods for the detection of straight lines. It is quite efficient and has a particularly attractive architecture which is suitable for parallel implementation.},
	number = {1},
	urldate = {2024-01-05},
	journal = {Computer Vision, Graphics, and Image Processing},
	author = {Princen, John and Illingworth, John and Kittler, Josef},
	month = oct,
	year = {1990},
	pages = {57--77},
}

@patent{hough_method_1962,
	title = {Method and means for recognizing complex patterns},
	url = {https://patents.google.com/patent/US3069654/en},
	nationality = {US},
	assignee = {Individual},
	number = {US3069654A},
	urldate = {2024-01-05},
	author = {Hough, Paul V. C.},
	month = dec,
	year = {1962},
	keywords = {framelet, line, microsecond, pulse, segment},
}

@article{wang_novel_2018,
	title = {A {Novel} {Method} for {Reconstructing} {Broken} {Contour} {Lines} {Extracted} from {Scanned} {Topographic} {Maps}},
	volume = {1},
	url = {https://ica-proc.copernicus.org/articles/1/121/2018/},
	doi = {10.5194/ica-proc-1-121-2018},
	abstract = {It is known that after segmentation and morphological operations on scanned topographic maps, gaps occur in contour lines. It is also well known that filling these gaps and reconstruction of contour lines with high accuracy and completeness is not an easy problem. In this paper, a novel method is proposed dedicated in automatic or semiautomatic filling up caps and reconstructing broken contour lines in binary images. The key part of end points’ auto-matching and reconnecting is deeply discussed after introducing the procedure of reconstruction, in which some key algorithms and mechanisms are presented and realized, including multiple incremental backing trace to get weighted average direction angle of end points, the max constraint angle control mechanism based on the multiple gradient ranks, combination of weighted Euclidean distance and deviation angle to determine the optimum matching end point, bidirectional parabola control, etc. Lastly, experimental comparisons based on typically samples are complemented between proposed method and the other representative method, the results indicate that the former holds higher accuracy and completeness, better stability and applicability.},
	language = {English},
	urldate = {2024-01-04},
	journal = {Proceedings of the ICA},
	author = {Wang, Feng and Liu, Pingzhi and Yang, Yun and Wei, Haiping and An, Xiaoya},
	month = may,
	year = {2018},
	note = {Publisher: Copernicus GmbH},
	keywords = {broken contour lines, line extraction, points’ auto-matching, reconstruction, scanned topographic maps},
	pages = {1--7},
}

@inproceedings{jinyang_automatic_2004,
	title = {Automatic extraction of contour lines from scanned topographic map},
	volume = {5},
	url = {https://ieeexplore.ieee.org/abstract/document/1370296},
	doi = {10.1109/IGARSS.2004.1370296},
	abstract = {The automatic extraction of contour lines from scanned topographic map is one of the challenging subjects in GIS. Most of the methods available rely heavily on image processing on pixel scale and pay little attention to spatial relationships, which are inherent characteristics of contour lines. This work brings forward a new scheme aiming at the effective realization of automatic extraction of contour lines. Based on mathematical morphology, this scheme tries to acquire spatial relationships of contour lines and make use of them for matching and connecting broken contour lines. The whole process of automatic extraction of contour lines includes the following aspects: acquisition of contour line image by color separation, erasion of image noise, image skeletonizing process, line tracking process, acquisition of spatial relationships by dilation of contour lines, matching and connecting broken contour lines based on spatial relationships, compression and output of vector data. Software was designed according to the scheme and experiments on scanned maps further proved the effectiveness of the method.},
	urldate = {2024-01-04},
	booktitle = {{IGARSS} 2004. 2004 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Jinyang, Du and Yumei, Zhang},
	month = sep,
	year = {2004},
	pages = {2886--2888 vol.5},
}

@article{samet_new_2012,
	title = {A new approach to the reconstruction of contour lines extracted from topographic maps},
	volume = {23},
	issn = {1047-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S1047320312000363},
	doi = {10.1016/j.jvcir.2012.02.005},
	abstract = {It is known that after segmentation and morphological operations on topographic maps, gaps occur in contour lines. It is also well known that filling these gaps and reconstruction of contour lines with high accuracy is not an easy problem. In this paper, a nontrivial semi-automatic approach to solve this problem is proposed. The main idea of the proposed approach is based on local and geometric properties such as (1) parabolic and opposite directions, (2) the differences of y-ordinate of end points, (3) changing the directions of x-axis and y-ordinate to the nearest clockwise direction and (4) avoiding the use of the second end point of a small piece of any contour line in the same mask if its other end point is used. The proposed approach was implemented on the base of many topographic maps with different resolutions and complexity. The obtained results show that the proposed approach increases accuracy and performance.},
	number = {4},
	urldate = {2024-01-04},
	journal = {Journal of Visual Communication and Image Representation},
	author = {Samet, Refik and Hancer, Emrah},
	month = may,
	year = {2012},
	keywords = {Cartography, Contour lines, Feature extraction, Gaps and end points, Geometric properties, Geophysical image processing, Reconstruction, Topographic maps},
	pages = {642--647},
}

@article{wu_automatic_2021,
	title = {Automatic {Road} {Extraction} from {High}-{Resolution} {Remote} {Sensing} {Images} {Using} a {Method} {Based} on {Densely} {Connected} {Spatial} {Feature}-{Enhanced} {Pyramid}},
	volume = {14},
	issn = {2151-1535},
	url = {https://ieeexplore.ieee.org/document/9285177},
	doi = {10.1109/JSTARS.2020.3042816},
	abstract = {Road extraction is an important task in remote sensing image information extraction. Recently, deep learning semantic segmentation has become an important method of road extraction. Due to the impact of the loss of multiscale spatial features, the results of road extraction still contain incomplete or fractured results. In this article, we proposed a deep learning model, which is called the dense-global-residual network that reduces the loss of spatial information and enhances context awareness. In the dense-global-residual network, the residual network is used to extract the features at different levels. To obtain more abundant multiscale features, a dense and global spatial pyramid pooling module based on Atrous Spatial Pyramid Pooling is built to perceive and aggregate the contextual information. The proposed method obtains better results on the GF-2 road dataset and public Massachusetts road dataset of aerial imagery. In order to prove the effectiveness of our method, we compared with four methods, such as DeepLabV3+, U-net, D-LinkNet, and coord-dense-global model, and found that the accuracy of our method is considerably better. Moreover, the dense-global-residual network can also effectively extract roads, especially trees and building shadows that occlude the road. In addition, our method can successfully extract roads in regions of different development levels in universality experiments. This indicates that the proposed method can effectively maintain the completeness and continuity of roads and improve the accuracy of road segmentation from high-resolution remote sensing images.},
	urldate = {2024-01-04},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Wu, Qiangqiang and Luo, Feng and Wu, Penghai and Wang, Biao and Yang, Hui and Wu, Yanlan},
	year = {2021},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	pages = {3--17},
}

@article{fitton_optimising_1998,
	title = {Optimising the application of the {Hough} transform for automatic feature extraction from geoscientific images},
	volume = {24},
	issn = {0098-3004},
	url = {https://www.sciencedirect.com/science/article/pii/S0098300498000703},
	doi = {10.1016/S0098-3004(98)00070-3},
	abstract = {We have adapted the Hough transform to extract linear features successfully from geoscientific datasets. The Hough transform is used in an automatic technique, which makes use of a parameter space to describe features of interest in images. This method has been widely applied in machine vision for recognition of features in highly structured images. Geoscientific data is more demanding. Features of interest within scenes of natural environments exist on all scales, are often partially obscured and the images are usually noisy. Pre-processing of images before the HT is essential. Adaptations of the HT to cope with particular properties of geoscientific data include: optimising the dimensions of the discrete transform domain; using feature-modelling to cancel lines found; transforming multi-scale tiles of the original image and correcting amplitudes in the transformed domain to account for the position of features. These specific adaptations produce a method for automatic feature detection which requires the user to select only two parameters. Output of the procedure is rich in feature content and accurate, leaving a clean result for statistical analysis. This optimised HT is robust for natural scenes, coping in particular with short line-segments.},
	number = {10},
	urldate = {2024-01-04},
	journal = {Computers \& Geosciences},
	author = {Fitton, N. C. and Cox, S. J. D.},
	month = dec,
	year = {1998},
	keywords = {Algorithms, Faults, Feature recognition, Image processing, Joints, Lines, Parameter space methods, Software, Statistics, Transforms},
	pages = {933--951},
}

@incollection{fischler_detection_1987,
	address = {San Francisco (CA)},
	title = {Detection of {Roads} and {Linear} {Structures} in {Low}-{Resolution} {Aerial} {Imagery} {Using} a {Multisource} {Knowledge} {Integration} {Technique}**{The} work reported herein was supported by the {Defense} {Advanced} {Research} {Projects} {Agency} under {Contracts} {DAAG29}–76-{C}-0057 and {MDA903}–79-{C}-0588; and by the {U}.{S}. {Army} {Engineer} {Topographic} {Laboratory} under {Contract} {DAAK70}–78-{C}-0114.},
	isbn = {978-0-08-051581-6},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080515816500714},
	abstract = {This paper describes a computer-based approach to the problem of detecting and precisely delineating roads, and similar “line-like” structures, appearing in low-resolution aerial imagery. The approach is based on a new paradigm for combining local information from multiple, and possibly incommensurate, sources, including various line and edge detection operators, map knowledge about the likely path of roads through an image, and generic knowledge about roads (e.g., connectivity, curvature, and width constraints). The final interpretation of the scene is achieved by using either a graph search or dynamic programming technique to optimize a global figure of merit. Implementation details and experimental results are included.},
	urldate = {2024-01-04},
	booktitle = {Readings in {Computer} {Vision}},
	publisher = {Morgan Kaufmann},
	author = {Fischler, M. A. and Tenenbaum, J. M. and Wolf, H. C.},
	editor = {Fischler, Martin A. and Firschein, Oscar},
	month = jan,
	year = {1987},
	doi = {10.1016/B978-0-08-051581-6.50071-4},
	pages = {741--752},
}

@inproceedings{groom_historical_2020,
	title = {Historical {Maps} – {Machine} {Learning} {Helps} {Us} over the {Map} {Vectorisation} {Crux}},
	url = {http://lazarus.elte.hu/avhm/proc/AVHM_proceedings_11_Groom_etal.pdf},
	doi = {10.21862/avhm2020.11},
	urldate = {2024-01-04},
	booktitle = {Automatic {Vectorisation} of {Historical} {Maps}: {International} workshop organized by the {ICA} {Commission} on {Cartographic} {Heritage} into the {Digital}. {Budapest} – 13 {March}, 2020},
	publisher = {Department of Cartography and Geoinformatics ELTE},
	author = {Groom, Geoff and Levin, Gregor and Svenningsen, Stig and Linnet Perner, Mads},
	year = {2020},
	keywords = {historic, historical maps, line extraction, vectorisation},
	pages = {91--100},
}

@article{quackenbush_review_2004,
	title = {A {Review} of {Techniques} for {Extracting} {Linear} {Features} from {Imagery}},
	volume = {70},
	issn = {00991112},
	url = {http://openurl.ingenta.com/content/xref?genre=article&issn=0099-1112&volume=70&issue=12&spage=1383},
	doi = {10.14358/PERS.70.12.1383},
	language = {en},
	number = {12},
	urldate = {2023-10-25},
	journal = {Photogrammetric Engineering \& Remote Sensing},
	author = {Quackenbush, Lindi J.},
	month = dec,
	year = {2004},
	keywords = {extraction, imagery, line extraction},
	pages = {1383--1392},
}

@article{ekim_automatic_2021,
	title = {Automatic {Road} {Extraction} from {Historical} {Maps} {Using} {Deep} {Learning} {Techniques}: {A} {Regional} {Case} {Study} of {Turkey} in a {German} {World} {War} {II} {Map}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2220-9964},
	shorttitle = {Automatic {Road} {Extraction} from {Historical} {Maps} {Using} {Deep} {Learning} {Techniques}},
	url = {https://www.mdpi.com/2220-9964/10/8/492},
	doi = {10.3390/ijgi10080492},
	abstract = {Scanned historical maps are available from different sources in various scales and contents. Automatic geographical feature extraction from these historical maps is an essential task to derive valuable spatial information on the characteristics and distribution of transportation infrastructures and settlements and to conduct quantitative and geometrical analysis. In this research, we used the Deutsche Heereskarte 1:200,000 Türkei (DHK 200 Turkey) maps as the base geoinformation source to construct the past transportation networks using the deep learning approach. Five different road types were digitized and labeled to be used as inputs for the proposed deep learning-based segmentation approach. We adapted U-Net++ and ResneXt50\_32×4d architectures to produce multi-class segmentation masks and perform feature extraction to determine various road types accurately. We achieved remarkable results, with 98.73\% overall accuracy, 41.99\% intersection of union, and 46.61\% F1 score values. The proposed method can be implemented in DHK maps of different countries to automatically extract different road types and used for transfer learning of different historical maps.},
	language = {en},
	number = {8},
	urldate = {2024-01-04},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Ekim, Burak and Sertel, Elif and Kabadayı, M. Erdem},
	month = aug,
	year = {2021},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural networks, deep learning, fully convolutional networks, historical maps, road classification, segmentation},
	pages = {492},
}

@article{hu_enriching_2022-1,
	title = {Enriching the metadata of map images: a deep learning approach with {GIS}-based data augmentation},
	volume = {36},
	issn = {1365-8816},
	shorttitle = {Enriching the metadata of map images},
	url = {https://doi.org/10.1080/13658816.2021.1968407},
	doi = {10.1080/13658816.2021.1968407},
	abstract = {Maps in the form of digital images are widely available in geoportals, Web pages, and other data sources. The metadata of map images, such as spatial extents and place names, are critical for their indexing and searching. However, many map images have either mismatched metadata or no metadata at all. Recent developments in deep learning offer new possibilities for enriching the metadata of map images via image-based information extraction. One major challenge of using deep learning models is that they often require large amounts of training data that have to be manually labeled. To address this challenge, this paper presents a deep learning approach with GIS-based data augmentation that can automatically generate labeled training map images from shapefiles using GIS operations. We utilize such an approach to enrich the metadata of map images by adding spatial extents and place names extracted from map images. We evaluate this GIS-based data augmentation approach by using it to train multiple deep learning models and testing them on two different datasets: a Web Map Service image dataset at the continental scale and an online map image dataset at the state scale. We then discuss the advantages and limitations of the proposed approach.},
	number = {4},
	urldate = {2024-01-04},
	journal = {International Journal of Geographical Information Science},
	author = {Hu, Yingjie and Gui, Zhipeng and Wang, Jimin and Li, Muxian},
	month = apr,
	year = {2022},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/13658816.2021.1968407},
	keywords = {GeoAI, Map image, data augmentation, geospatial metadata, map information extraction},
	pages = {799--821},
}

@misc{altaweel_extracting_2022,
	title = {Extracting {Geospatial} {Data} from {Historical} {Maps}},
	url = {https://www.geographyrealm.com/extract-geospatial-data-maps/},
	abstract = {New methods can detect location names from historical maps, make them searchable, and allow for the automated extraction of geographic data from those maps.},
	language = {en-US},
	urldate = {2024-01-04},
	journal = {Geography Realm},
	author = {Altaweel, Mark},
	month = mar,
	year = {2022},
}

@misc{dempsey_automating_2013,
	title = {Automating {Extracting} {GIS} {Data} from {Scanned} {Maps}},
	url = {https://www.geographyrealm.com/automating-extracting-gis-data-scanned-maps/},
	abstract = {The New York Public Library Labs (NYPL Labs) has posted on Github the code to its open source map-vectorizer project.  NYPL Lab’s map-vectorizer project is seeking to automate (“like OCR for maps”) the process of extracting polygon and attribute information from old scanned maps.   Extracting Building Information from Historical Maps The code was developed with ... Read more},
	language = {en-US},
	urldate = {2024-01-04},
	journal = {Geography Realm},
	author = {Dempsey, Caitlin},
	month = nov,
	year = {2013},
}

@article{howland_exploring_2023,
	title = {Exploring geomagnetic variations in ancient mesopotamia: {Archaeomagnetic} study of inscribed bricks from the 3rd–1st millennia {BCE}},
	volume = {120},
	copyright = {Copyright © 2023 the Author(s). Published by PNAS.},
	shorttitle = {Exploring geomagnetic variations in ancient mesopotamia},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2313361120},
	doi = {10.1073/pnas.2313361120},
	abstract = {This study presents 32 high-resolution geomagnetic intensity data points from Mesopotamia,
spanning the 3rd to the 1st millennia BCE. These data co...},
	language = {EN},
	number = {52},
	urldate = {2023-12-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Howland, Matthew D. and Tauxe, Lisa and Gordin, Shai and Altaweel, Mark and Cych, Brendan and Ben-Yosef, Erez},
	month = dec,
	year = {2023},
	note = {Company: National Academy of Sciences
Distributor: National Academy of Sciences
Institution: National Academy of Sciences
Label: National Academy of Sciences
Publisher: Proceedings of the National Academy of Sciences},
	keywords = {Archaeological, archeomagnitism, paleomagnetism},
	pages = {e2313361120},
}

@inproceedings{wang_extraction_2024,
	address = {Singapore},
	series = {Smart {Innovation}, {Systems} and {Technologies}},
	title = {Extraction and {Fusion} of {Geographic} {Information} from {Multi}-source {Remote} {Sensing} {Images} {Based} on {Artificial} {Intelligence}},
	isbn = {978-981-9966-41-7},
	doi = {10.1007/978-981-99-6641-7_2},
	abstract = {Multi-source RS image fusion technology is mainly a data processing technology to organize and correlate the image data of the same scene under different imaging modes through specific calculation rules, and then obtain more accurate, perfect and rich information of comprehensive images. The information contained in remote sensing (RS) images of different sensors is imprecise, uncertain and fuzzy to varying degrees, so the fusion method used in fusing these information must solve these problems. The way to solve the problem of image registration is to solve the problem of relative correction of images. On the basis of analyzing and discussing the principle, hierarchy, structure and characteristics of multi-source RS image data fusion, this article puts forward the extraction and fusion technology of multi-source RS image geographic information combined with artificial intelligence (AI) algorithm. Compared with K-means classification method, this classification fusion method can effectively reduce the uncertain information in the classification process and improve the classification accuracy. The results verify the feasibility of the extraction and fusion method of geographic information from multi-source RS images proposed in this article in practical application.},
	language = {en},
	booktitle = {Proceedings of {International} {Conference} on {Artificial} {Intelligence} and {Communication} {Technologies} ({ICAICT} 2023)},
	publisher = {Springer Nature},
	author = {Wang, Zirui},
	editor = {Kountchev, Roumen and Patnaik, Srikanta and Nakamatsu, Kazumi and Kountcheva, Roumiana},
	year = {2024},
	keywords = {Artificial intelligence, Geographic information, K-means classification method, Multi-source remote sensing images},
	pages = {17--27},
}

@article{li_remainnet_2023,
	title = {{RemainNet}: {Explore} {Road} {Extraction} from {Remote} {Sensing} {Image} {Using} {Mask} {Image} {Modeling}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {{RemainNet}},
	url = {https://www.mdpi.com/2072-4292/15/17/4215},
	doi = {10.3390/rs15174215},
	abstract = {Road extraction from a remote sensing image is a research hotspot due to its broad range of applications. Despite recent advancements, achieving precise road extraction remains challenging. Since a road is thin and long, roadside objects and shadows cause occlusions, thus influencing the distinguishment of the road. Masked image modeling reconstructs masked areas from unmasked areas, which is similar to the process of inferring occluded roads from nonoccluded areas. Therefore, we believe that mask image modeling is beneficial for indicating occluded areas from other areas, thus alleviating the occlusion issue in remote sensing image road extraction. In this paper, we propose a remote sensing image road extraction network named RemainNet, which is based on mask image modeling. RemainNet consists of a backbone, image prediction module, and semantic prediction module. An image prediction module reconstructs a masked area RGB value from unmasked areas. Apart from reconstructing original remote sensing images, a semantic prediction module of RemainNet also extracts roads from masked images. Extensive experiments are carried out on the Massachusetts Roads dataset and DeepGlobe Road Extraction dataset; the proposed RemainNet improves 0.82–1.70\% IoU compared with other state-of-the-art road extraction methods.},
	language = {en},
	number = {17},
	urldate = {2024-01-04},
	journal = {Remote Sensing},
	author = {Li, Zhenghong and Chen, Hao and Jing, Ning and Li, Jun},
	month = jan,
	year = {2023},
	note = {Number: 17
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {masked image modeling, remote sensing, road extraction, semantic segmentation},
	pages = {4215},
}

@article{duque_system_2023,
	title = {{SYSTEM} {ARCHITECTURE} {FOR} {GEOSPATIAL} {VIRTUAL} {DATA} {INTEGRATION} {IN} {WEB}-{BASED} {APPLICATIONS}},
	volume = {XLVIII-1-W2-2023},
	issn = {1682-1750},
	url = {https://isprs-archives.copernicus.org/articles/XLVIII-1-W2-2023/939/2023/},
	doi = {10.5194/isprs-archives-XLVIII-1-W2-2023-939-2023},
	abstract = {The wide availability of geospatial data from different sources makes it necessary to create systems that are able to use and integrate the data to generate added value. We propose a system architecture following FAIR principles (Findable, Accessible, Interoperable, Reusable) and state-of-the-art methodologies for a server-side web-based application that performs virtual data integration over data sources that implement geospatial information standards. The architecture extends the mediator-wrapper design pattern with additional components that provide the system with additional flexibility and modularity, much needed for modern web applications. The architecture is composed of the mask, which acts as the interface of the system towards external users; a mediator that handles processing and data integration logic; a set of wrappers that communicate with the external data sources; persistent storage to provide flexible configuration and metadata capabilities to the system; and messaging queue for enabling asynchronous processing. At the same time, the architecture\&rsquo;s components are divided into four layers, each one with a specific role: presentation, configuration, processing, and communication.},
	language = {English},
	urldate = {2024-01-04},
	journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
	author = {Duque, J. P. and Brovelli, M. A.},
	month = dec,
	year = {2023},
	note = {Conference Name: ISPRS Geospatial Week 2023 - 2–7 September 2023, Cairo, Egypt
Publisher: Copernicus GmbH},
	keywords = {FAIR principles, data integration, geospatial data, geospatial standards, system architecture, web-gis},
	pages = {939--944},
}

@article{matas_robust_2000,
	title = {Robust {Detection} of {Lines} {Using} the {Progressive} {Probabilistic} {Hough} {Transform}},
	volume = {78},
	issn = {1077-3142},
	url = {https://doi.org/10.1006/cviu.1999.0831},
	doi = {10.1006/cviu.1999.0831},
	abstract = {In the paper we present the progressive probabilistic Hough transform (PPHT). Unlike the probabilistic HT, where the standard HT is performed on a preselected fraction of input points, the PPHT minimizes the amount of computation needed to detect lines by exploiting the difference in the fraction of votes needed to reliably detect lines with different numbers of supporting points. The fraction of points used for voting need not be specified ad hoc or using a priori knowledge, as in the probabilistic HT; it is a function of the inherent complexity of data. The algorithm is ideally suited for real-time applications with a fixed amount of available processing time, since voting and line detection are interleaved. The most salient features are likely to be detected first. While retaining its robustness, experiments show that the PPHT has, in many circumstances, advantages over the standard HT.},
	number = {1},
	urldate = {2024-01-03},
	journal = {Computer Vision and Image Understanding},
	author = {Matas, J. and Galambos, C. and Kittler, J.},
	month = apr,
	year = {2000},
	keywords = {canny, hough, image, line extraction},
	pages = {119--137},
}

@misc{english_heritage_how_2023,
	title = {How to spot: {A} {Roman} {Road}},
	shorttitle = {How to spot},
	url = {https://www.english-heritage.org.uk/visit/inspire-me/blog/blog-posts/how-to-spot-a-roman-road/},
	abstract = {Most Roman roads are straight. Well, straight-ish. Mary-Ann Ochota has put together a handy guide to help you spot a Roman road in the English landscape.},
	urldate = {2023-11-03},
	journal = {English Heritage},
	author = {English Heritage},
	year = {2023},
}

@article{lijun_geo-information_2023,
	title = {Geo-information mapping improves {Canny} edge detection method},
	volume = {17},
	issn = {1751-9667},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12764},
	doi = {10.1049/ipr2.12764},
	abstract = {Aiming at the shortcomings of the current Canny edge detection method in terms of noise removal, threshold setting, and edge recognition, this paper proposes a method for improving Canny edge detection by geo-information mapping. The shortcomings of the traditional Canny edge detection method are analyzed by using the Canny optimal criterion and Tobler's First Law, which points out the direction of edge detection optimization by using the difference between edge properties and noise properties. The property characteristics and spatial distribution rules of edge points and edge lines are inspected using the geographic information mapping theory and technical methods, and edge identification criteria are defined at two levels of edge points and edge lines. Finally, the method model of improved Canny edge detection is constructed by combining guided filtering. The experimental results show that the improved edge detection method has the advantages of enriched edge details, accurate edge recognition, and strong self-adaptive capability. This is a new attempt of geo-information mapping theory and technical method in image edge detection, which has certain theoretical significance and strong practical guidance.},
	language = {en},
	number = {6},
	urldate = {2024-01-03},
	journal = {IET Image Processing},
	author = {Lijun, Yang and Mengbo, Li and Tongxin, Wu and Youfeng, Bao and Junhui, Li and Yi, Jiang},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/ipr2.12764},
	keywords = {Canny, Tobler's First Law, correlation, edge detection, geo-information mapping, object-oriented},
	pages = {1893--1904},
}

@book{frere_roman_1983,
	address = {Cambridge},
	series = {Cambridge air surveys},
	title = {Roman {Britain} from the air},
	isbn = {978-0-521-25088-7},
	number = {4},
	publisher = {Cambridge University Press},
	author = {Frere, Sheppard Sunderland and St Joseph, John Kenneth Sinclair},
	year = {1983},
	keywords = {Aerial photography in archaeology, Antiquities, Roman, Great Britain},
}

@book{ferrar_roman_2003,
	address = {Oxford},
	series = {{BAR} {British} series},
	title = {The {Roman} survey of {Britain}},
	isbn = {978-1-84171-348-9},
	number = {359},
	publisher = {John and Erica Hedges Ltd},
	author = {Ferrar, Michael J. and Richardson, Alan},
	year = {2003},
	keywords = {Great Britain, History, Roads, Roman, Roman period, 55 B.C.-449 A.D, Romans, Surveys},
}

@book{davies_roads_2002,
	address = {Stroud},
	title = {Roads in {Roman} {Britain}},
	publisher = {Tempus},
	author = {Davies, Hugh},
	year = {2002},
	keywords = {Antiquities, Roman, Great Britain, Roads, Roman},
}

@book{bagshawe_roman_1979,
	series = {Shire {Archaeology}},
	title = {Roman {Roads}},
	isbn = {0-85263-458-7},
	publisher = {Shire  Publications},
	author = {Bagshawe, Richard},
	year = {1979},
}

@misc{nowosad_jakub_2023,
	title = {Jakub {Nowosad}’s website - {Simulating} spatial patterns with the spatial kinetic {Ising} model},
	url = {https://jakubnowosad.com/posts/2023-12-17-spatialising-bp1/},
	language = {en},
	urldate = {2023-12-20},
	author = {Nowosad, Jakub},
	month = dec,
	year = {2023},
}

@misc{khellaf_spot_2023,
	title = {Spot: {A} {Natural} {Language} {Interface} for {Geospatial} {Searches} in {OSM}},
	shorttitle = {Spot},
	url = {http://arxiv.org/abs/2311.08093},
	doi = {10.48550/arXiv.2311.08093},
	abstract = {Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to be an invaluable resource for their work due to its extensive coverage and intricate details of various locations, which play a crucial role in investigating news scenes. Despite its value, OSM's complexity presents considerable accessibility and usability challenges, especially for those without a technical background. To address this, we introduce 'Spot', a user-friendly natural language interface for querying OSM data. Spot utilizes a semantic mapping from natural language to OSM tags, leveraging artificially generated sentence queries and a T5 transformer. This approach enables Spot to extract relevant information from user-input sentences and display candidate locations matching the descriptions on a map. To foster collaboration and future advancement, all code and generated data is available as an open-source repository.},
	urldate = {2023-12-12},
	publisher = {arXiv},
	author = {Khellaf, Lynn and Schlicht, Ipek Baris and Bayer, Julia and Bouwmeester, Ruben and Miraß, Tilman and Wagner, Tilman},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08093 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{rees_accuracy_2023,
	title = {Accuracy of {Local} {Authority} {Population} {Forecasts} {Produced} by a {New} {Minimal} {Data} {Model}: {A} {Case} {Study} of {England}},
	volume = {42},
	issn = {1573-7829},
	shorttitle = {Accuracy of {Local} {Authority} {Population} {Forecasts} {Produced} by a {New} {Minimal} {Data} {Model}},
	url = {https://doi.org/10.1007/s11113-023-09839-2},
	doi = {10.1007/s11113-023-09839-2},
	abstract = {The preparation of forecasts for small and local area populations involves many challenges. Standard cohort-component models are problematic because of small numbers, which make estimation of rates unreliable. Because of this, the Synthetic Migration Population Projection (SYMPOPP) model was designed to forecast local populations without need for detailed area-specific information. This model had been used successfully for small area forecasts in Australia. The objective of the paper is to assess its performance when applied to local areas in England. The model uses a bi-regional structure based on a movement population account. Sub-models of total population change are employed to control future change. Fertility, mortality and migration rates are borrowed from national statistics, constrained to small area indicators. The model uses an Excel workbook with VBA routines and is relatively easy and quick to use. Model inputs were calibrated for 2006–2011 and used to forecast for 2011–2021. Results were tested against the census-based 2021 mid-year populations. A new error statistic, Age Structure Error, was used to evaluate Basic and Refined model versions against official projections. The two versions of SYMPOPP posted lower errors. The simple models had fewer areas with errors of 10\% or more (12.3–12.6\%) compared with the official projections (14.5\% of areas). Investigation revealed that these errors occurred in local authorities with high military, student, prison, or ethnic minority populations, influenced by factors not captured in a projection model for the general population.},
	language = {en},
	number = {6},
	urldate = {2023-12-08},
	journal = {Population Research and Policy Review},
	author = {Rees, Philip and Wilson, Tom},
	month = nov,
	year = {2023},
	keywords = {Age Structure Errors, Cohort-component models, Forecast performance, Population forecasts, Simple models},
	pages = {91},
}

@article{ulloa-leon_15-minute_2023,
	title = {“15-{Minute} {City}” and {Elderly} {People}: {Thinking} about {Healthy} {Cities}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2624-6511},
	shorttitle = {“15-{Minute} {City}” and {Elderly} {People}},
	url = {https://www.mdpi.com/2624-6511/6/2/50},
	doi = {10.3390/smartcities6020050},
	abstract = {Considering the global scenario of population aging, which countries such as Chile are going through, the social problems that it means in terms of viability and quality of life for the elderly are increasing and are a cause for concern. For this reason, this study summarizes the results of investigating the accessibility of services and recreational spaces under the parameters of a “15-minute city” for the elderly people in the city of Santiago de Chile. The investigation employed a multivariate geostatistical analysis with a quantitative approach and was developed on a census block scale to test some of the principles of the 15-min city along with the principles on active aging of the elderly. The results are surprising, show a good territorial coverage for the study area and open the possibility of Santiago becoming a 15-min city for older adults. However, there are still several challenges in terms of public policies, from mental and physical health to the design of public spaces, which are fundamental to think about for cities of the future.},
	language = {en},
	number = {2},
	urldate = {2023-12-08},
	journal = {Smart Cities},
	author = {Ulloa-Leon, Felipe and Correa-Parra, Juan and Vergara-Perucich, Francisco and Cancino-Contreras, Francisca and Aguirre-Nuñez, Carlos},
	month = apr,
	year = {2023},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {15-min city, elderly people, public health, smart cities, urbanism, walkability},
	pages = {1043--1058},
}

@article{lei_geolinter_2023,
	title = {{GeoLinter}: {A} {Linting} {Framework} for {Choropleth} {Maps}},
	issn = {1941-0506},
	shorttitle = {{GeoLinter}},
	url = {https://ieeexplore.ieee.org/document/10273434},
	doi = {10.1109/TVCG.2023.3322372},
	abstract = {Visualization linting is a proven effective tool in assisting users to follow established visualization guidelines. Despite its success, visualization linting for choropleth maps, one of the most popular visualizations on the internet, has yet to be investigated. In this paper, we present GeoLinter, a linting framework for choropleth maps that assists in creating accurate and robust maps. Based on a set of design guidelines and metrics drawing upon a collection of best practices from the cartographic literature, GeoLinter detects potentially suboptimal design decisions and provides further recommendations on design improvement with explanations at each step of the design process. We perform a validation study to evaluate the proposed framework's functionality with respect to identifying and fixing errors and apply its results to improve the robustness of GeoLinter. Finally, we demonstrate the effectiveness of the GeoLinter - validated through empirical studies - by applying it to a series of case studies using real-world datasets.},
	urldate = {2023-12-08},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Lei, Fan and Fan, Arlen and MacEachren, Alan M. and Maciejewski, Ross},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1--16},
}

@article{spence_glasgow_2023,
	title = {The ‘{Glasgow} effect’: the controversial cultural life of a public health term},
	copyright = {© Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ.. https://creativecommons.org/licenses/by/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution 4.0 Unported (CC BY 4.0) license, which permits others to copy, redistribute, remix, transform and build upon this work for any purpose, provided the original work is properly cited, a link to the licence is given, and indication of whether changes were made. See: https://creativecommons.org/licenses/by/4.0/.},
	issn = {1468-215X, 1473-4265},
	shorttitle = {The ‘{Glasgow} effect’},
	url = {https://mh.bmj.com/content/early/2023/11/30/medhum-2022-012594},
	doi = {10.1136/medhum-2022-012594},
	abstract = {The question of why more people in Glasgow were dying, and younger, compared with English cities with almost identical levels of deprivation, was a hot topic in Scottish public health debates in the early 21st century. Public health researchers, particularly the Glasgow Centre of Population Health (GCPH), used the terms ‘Glasgow effect’ and ‘Scottish effect’ as placeholders while identifying the unknown factors behind Scotland’s excess mortality. Yet the terms took on a colourful life of their own in the press and larger culture and continue to circulate, despite GCPH’s attempts to retire them. This paper is the first to analyse the cultural life of the ‘Glasgow effect’ and ‘Scottish effect’ terms. Looking primarily at the Scottish press 1998–2022, I analyse the politically charged and often controversial debates and lay recommendations around the concepts. I also trace the terms’ parallel usage, and indeed origin, in contexts unrelated to health. I argue that the ‘Glasgow effect’ functions as a myth. This myth emphasises Scottish exceptionalism in public health and larger culture, at a time when devolution and the prospect of independence heightened optimism and anxiety about Scotland’s future. It overlaps with a larger and longstanding myth of Scottish cultural pathology, or the pathological Scot. The flexibility of the ‘Glasgow effect’ and ‘Scottish effect’ terms is exploited by journalists, academics and artists to serve competing agendas, establish their own expertise and influence public opinion. While it may now be challenging to eradicate these terms, especially in lay contexts, researchers and policy makers should avoid using these unstable terms uncritically. The example of the ‘Glasgow effect’ shows how health concepts can become wrapped in larger national or political narratives and highlights the difficulties for public health communicators in introducing complex and emerging public health ideas into a dynamic landscape of lay beliefs.},
	language = {en},
	urldate = {2023-12-06},
	journal = {Medical Humanities},
	author = {Spence, Fred},
	month = dec,
	year = {2023},
	pmid = {38050167},
	keywords = {Medical humanities, Public health, cultural history, journalism, literary studies},
}

@article{van_elzakker_caring_2008,
	title = {Caring for the {Users}},
	volume = {45},
	issn = {0008-7041, 1743-2774},
	url = {http://www.tandfonline.com/doi/full/10.1179/174327713X13789832881340},
	doi = {10.1179/174327713X13789832881340},
	language = {en},
	number = {2},
	urldate = {2023-11-29},
	journal = {The Cartographic Journal},
	author = {Van Elzakker, Corné and Nivala, Annu-Maaria and Pucher, Alexander and Forrest, David},
	month = may,
	year = {2008},
	pages = {84--86},
}

@article{robinson_representing_2019,
	title = {Representing the {Presence} of {Absence} in {Cartography}},
	volume = {109},
	issn = {2469-4452, 2469-4460},
	url = {https://www.tandfonline.com/doi/full/10.1080/24694452.2018.1473754},
	doi = {10.1080/24694452.2018.1473754},
	language = {en},
	number = {1},
	urldate = {2023-11-27},
	journal = {Annals of the American Association of Geographers},
	author = {Robinson, Anthony C.},
	month = jan,
	year = {2019},
	pages = {286--300},
}

@inproceedings{arundel_reimagining_2023,
	address = {New York, NY, USA},
	series = {{GeoAI} '23},
	title = {Reimagining standardization and geospatial interoperability in today's {GeoAI} culture},
	isbn = {9798400703485},
	url = {https://dl.acm.org/doi/10.1145/3615886.3627744},
	doi = {10.1145/3615886.3627744},
	abstract = {Integrating Geospatial Artificial Intelligence (GeoAI) into our technological landscape has revolutionized our capacity to understand and engage with the world. However, the burgeoning adoption of GeoAI applications has emphasized the priority of data, format, and conveyance standardization and improving geospatial interoperability. This vision paper examines the intricacies of the evolving GeoAI environment, emphasizing the vital role of standardized practices and elevated interoperability. By synthesizing insights from geography, computer science, and data ethics, this contribution envisions a future characterized by the seamless synergy between AI systems and geospatial data, driving impactful decision-making and transformative innovation.},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the 6th {ACM} {SIGSPATIAL} {International} {Workshop} on {AI} for {Geographic} {Knowledge} {Discovery}},
	publisher = {Association for Computing Machinery},
	author = {Arundel, Samantha T. and Li, Wenwen and Campbell, Bryan B.},
	month = nov,
	year = {2023},
	keywords = {Collaborative Innovation, Data Ethics, GeoAI, Geospatial Interoperability, Standardization},
	pages = {83--84},
}

@inproceedings{gramacki_srai_2023,
	address = {New York, NY, USA},
	series = {{GeoAI} '23},
	title = {{SRAI}: {Towards} {Standardization} of {Geospatial} {AI}},
	isbn = {9798400703485},
	shorttitle = {{SRAI}},
	url = {https://dl.acm.org/doi/10.1145/3615886.3627740},
	doi = {10.1145/3615886.3627740},
	abstract = {Spatial Representations for Artificial Intelligence (srai) is a Python library for working with geospatial data. The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures. It includes baseline models as well as more complex methods from published works. Those capabilities make it possible to use srai in a complete pipeline for geospatial task solving. The proposed library is the first step to standardize the geospatial AI domain toolset. It is fully open-source and published under Apache 2.0 licence.},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the 6th {ACM} {SIGSPATIAL} {International} {Workshop} on {AI} for {Geographic} {Knowledge} {Discovery}},
	publisher = {Association for Computing Machinery},
	author = {Gramacki, Piotr and Leśniara, Kacper and Raczycki, Kamil and Woźniak, Szymon and Przymus, Marcin and Szymański, Piotr},
	month = nov,
	year = {2023},
	keywords = {geospatial data processing, openstreetmap embeddings, python library, spatial embeddings, spatial representation learning, standardization in geospatial domain, urban data embeddings},
	pages = {43--52},
}

@inproceedings{xia_contrastive_2023,
	address = {New York, NY, USA},
	series = {{GeoAI} '23},
	title = {Contrastive {Pretraining} for {Railway} {Detection}: {Unveiling} {Historical} {Maps} with {Transformers}},
	isbn = {9798400703485},
	shorttitle = {Contrastive {Pretraining} for {Railway} {Detection}},
	url = {https://dl.acm.org/doi/10.1145/3615886.3627738},
	doi = {10.1145/3615886.3627738},
	abstract = {Detecting railways from historical maps is challenging due to their infrequent representation in a map sheet and their visual similarity with roads. Basically, both railways and roads are symbolised as two parallel black lines, with slight differences only in line thickness. Recent advancements in transformer models for computer vision tasks have sparked interest in utilizing them for processing historical maps. However, the success of transformers heavily relies on large-scale labelled datasets, predominantly available for ground imagery rather than historical maps. To overcome these challenges, we exploit the unique spatial characteristics of historical map data, where the same location can be depicted over different time spans across different map series. For example, each location in Switzerland is depicted in both the Siegfried map and the Old National map, each exhibiting distinct symbols and drawing styles. In this work, we address the scarcity of labelled data by generating positive pairs of the same scene from different map series and employ self-supervised contrastive learning to pre-train a dedicated transformer encoder optimized for map data. Subsequently, we finetune the entire transformer network for the downstream railway detection task. Experimental results demonstrate that our method achieves comparable performance to fully supervised approaches, while significantly reducing the amount of required labelled dataset to a mere 2.5\% after contrastive pretraining.},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the 6th {ACM} {SIGSPATIAL} {International} {Workshop} on {AI} for {Geographic} {Knowledge} {Discovery}},
	publisher = {Association for Computing Machinery},
	author = {Xia, Xue and Jiao, Chenjing and Hurni, Lorenz},
	month = nov,
	year = {2023},
	keywords = {Transformer, contrastive learning, map processing, neural networks, railway detection},
	pages = {30--33},
}

@article{janowicz_geoai_2020,
	title = {{GeoAI}: spatially explicit artificial intelligence techniques for geographic knowledge discovery and beyond},
	volume = {34},
	issn = {1365-8816},
	shorttitle = {{GeoAI}},
	url = {https://doi.org/10.1080/13658816.2019.1684500},
	doi = {10.1080/13658816.2019.1684500},
	number = {4},
	urldate = {2023-11-27},
	journal = {International Journal of Geographical Information Science},
	author = {Janowicz, Krzysztof and Gao, Song and McKenzie, Grant and Hu, Yingjie and Bhaduri, Budhendra},
	month = apr,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/13658816.2019.1684500},
	pages = {625--636},
}

@inproceedings{robinson_cartography_2023,
	address = {New York, NY, USA},
	series = {{GeoAI} '23},
	title = {Cartography in {GeoAI}: {Emerging} {Themes} and {Research} {Challenges}},
	isbn = {9798400703485},
	shorttitle = {Cartography in {GeoAI}},
	url = {https://dl.acm.org/doi/10.1145/3615886.3627734},
	doi = {10.1145/3615886.3627734},
	abstract = {The emergence of prompt-driven artificial intelligence (AI) techniques for the rapid creation and iterative ideation of text, images, and code has disrupted the trajectory of science, technology, and society. Geospatial AI (GeoAI) aims to develop approaches for AI that target spatio-temporal problem contexts. Cartography is a key constituent area of GeoAI, providing the mechanism by which visual exploration, analysis, synthesis, and communication are made possible. In a recent research workshop with 35 academic cartographers from institutions in the U.S., Europe, Australia, and Africa, we fielded 17 talks on emerging research areas in Cartography and AI, and in collaborative activities with participants we developed many new research questions. In this paper we highlight the key themes emerging from our workshop, characterizing ongoing work as well as new challenges that lie at the intersections of Cartography and AI.},
	urldate = {2023-11-27},
	booktitle = {Proceedings of the 6th {ACM} {SIGSPATIAL} {International} {Workshop} on {AI} for {Geographic} {Knowledge} {Discovery}},
	publisher = {Association for Computing Machinery},
	author = {Robinson, Anthony C. and Çöltekin, Arzu and Griffin, Amy L. and Ledermann, Florian},
	month = nov,
	year = {2023},
	keywords = {AI, Cartography, ethics, geovisualization, personalization},
	pages = {1--2},
}

@article{robinson_approaches_2021,
	title = {Approaches for {Visualizing} the {Presence} of {Absence} in {Cartography}},
	volume = {3},
	url = {https://ica-abs.copernicus.org/articles/3/251/2021/},
	doi = {10.5194/ica-abs-3-251-2021},
	language = {English},
	urldate = {2023-11-27},
	journal = {Abstracts of the ICA},
	author = {Robinson, Anthony C.},
	month = dec,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	keywords = {geovisual analytics, missing data, uncertainty, visualization},
	pages = {1--2},
}

@misc{goth_foundational_2023,
	title = {Foundational {Competencies} and {Responsibilities} of a {Research} {Software} {Engineer}},
	url = {http://arxiv.org/abs/2311.11457},
	doi = {10.48550/arXiv.2311.11457},
	abstract = {The term Research Software Engineer, or RSE, emerged a little over 10 years ago as a way to represent individuals working in the research community but focusing on software development. The term has been widely adopted and there are a number of high-level definitions of what an RSE is. However, the roles of RSEs vary depending on the institutional context they work in. At one end of the spectrum, RSE roles may look similar to a traditional research role. At the other extreme, they resemble that of a software engineer in industry. Most RSE roles inhabit the space between these two extremes. Therefore, providing a straightforward, comprehensive definition of what an RSE does and what experience, skills and competencies are required to become one is challenging. In this community paper we define the broad notion of what an RSE is, explore the different types of work they undertake, and define a list of fundamental competencies as well as values that define the general profile of an RSE. On this basis, we elaborate on the progression of these skills along different dimensions, looking at specific types of RSE roles, proposing recommendations for organisations, and giving examples of future specialisations. An appendix details how existing curricula fit into this framework.},
	urldate = {2023-11-24},
	publisher = {arXiv},
	author = {Goth, Florian and Alves, Renato and Braun, Matthias and Castro, Leyla Jael and Chourdakis, Gerasimos and Christ, Simon and Cohen, Jeremy and Erxleben, Fredo and Grad, Jean-Noël and Hagdorn, Magnus and Hodges, Toby and Juckeland, Guido and Kempf, Dominic and Lamprecht, Anna-Lena and Linxweiler, Jan and Schwarzmeier, Moritz and Seibold, Heidi and Thiele, Jan Philipp and von Waldow, Harald and Wittke, Samantha},
	month = nov,
	year = {2023},
	note = {arXiv:2311.11457 [physics]},
	keywords = {Computer Science - Computers and Society, Computer Science - Software Engineering, Physics - Computational Physics},
}

@article{openshaw_mark_1987,
	title = {A {Mark} 1 {Geographical} {Analysis} {Machine} for the automated analysis of point data sets},
	volume = {1},
	issn = {0269-3798},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02693798708927821},
	doi = {10.1080/02693798708927821},
	abstract = {Abstract This paper presents the first of a new generation of spatial analytical technology based on a fusion of statistical, GIS and computational thinking. It describes how to build what is termed a Geographical Analysis Machine (GAM), with high descriptive power. A GAM offers an imaginative new approach to the analysis of point pattern data based on a fully automated process whereby a point data set is explored for evidence of pattern without being unduly affected by predefined areal units or data error. No prior information or specification of particular location-specific hypotheses is required. If geographical data contain strong evidence of pattern in geographical space, then the GAM will find it. This technology is demonstrated by an analysis of data on cancer for northern England.},
	language = {en},
	number = {4},
	urldate = {2023-11-24},
	journal = {International journal of geographical information systems},
	author = {Openshaw, Stan and Charlton, Martin and Wymer, Colin and Craft, Alan},
	month = jan,
	year = {1987},
	pages = {335--358},
}

@incollection{rhind_analysis_1989,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The analysis of geographical data},
	isbn = {978-3-540-46045-9},
	url = {https://doi.org/10.1007/BFb0027527},
	abstract = {This paper describes the current and near-future situation in geographical data processing from several viewpoints. It commences with a summary of — so far as is known — user needs in relation to functionality and other characteristics of the necessary software (Geographical Information Systems or GIS). The burgeoning availability of certain types of spatially referenced data is described; the data structures used or developed to date are summarised, together with their advantages and disadvantages. The most powerful uses of GIS — modelling or spatial analysis — are presently rare but one example, based upon Openshaw's Geographical Analysis Machine, is presented; based on experience with this tool and much else, the shortcomings of existing software tools are described and future collaborative research to remedy these shortcomings is outlined.},
	language = {en},
	urldate = {2023-11-24},
	booktitle = {Statistical and {Scientific} {Database} {Management}: {Fourth} {International} {Working} {Conference} {SSDBM} {Rome}, {Italy}, {June} 21–23, 1988 {Proceedings}},
	publisher = {Springer},
	author = {Rhind, David and Openshaw, Stan and Green, Nick},
	editor = {Rafanelli, Maurizio and Klensin, John C. and Svensson, Per},
	year = {1989},
	doi = {10.1007/BFb0027527},
	keywords = {Birkbeck College, Geographic Information System, Modifiable Areal Unit Problem, Natural Environment Research Council, Ordnance Survey},
	pages = {427--454},
}

@article{luo_airborne_2019,
	title = {Airborne and spaceborne remote sensing for archaeological and cultural heritage applications: {A} review of the century (1907–2017)},
	volume = {232},
	issn = {0034-4257},
	shorttitle = {Airborne and spaceborne remote sensing for archaeological and cultural heritage applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425719302998},
	doi = {10.1016/j.rse.2019.111280},
	abstract = {Archaeological and cultural heritage (ACH), one of the core carriers of cultural diversity on our planet, has a direct bearing on the sustainable development of mankind. Documenting and protecting ACH is the common responsibility and duty of all humanity. It is governed by UNESCO along with the scientific communities that foster and encourage the use of advanced non-invasive techniques and methods for promoting scientific research into ACH and conservation of ACH sites. The use of remote sensing, a non-destructive tool, is increasingly popular by specialists around the world as it allows fast prospecting and mapping at multiple scales, rapid analysis of multisource datasets, and dynamic monitoring of ACH sites and their surrounding environments. The cost of using remote sensing is lower or even zero in practical applications. In this review, in order to discuss the advantages of airborne and spaceborne remote sensing (ASRS), the principles that make passive (photography, multispectral and hyperspectral) and active (synthetic aperture radar (SAR) and light detection and ranging radar (LiDAR)) imaging techniques suitable for ACH applications are first summarized and pointed out; a review of ASRS and the methodologies used over the past century is then presented together with relevant highlights from well-known research projects. Selected case studies from Mediterranean regions to East Asia illustrate how ASRS can be used effectively to investigate and understand archaeological features at multiple -scales and to monitor and assess the conservation status of cultural heritage sites in the context of sustainable development. An in-depth discussion on the limitations of ASRS and associated remaining challenges is presented along with conclusions and a look at future trends.},
	urldate = {2023-11-23},
	journal = {Remote Sensing of Environment},
	author = {Luo, Lei and Wang, Xinyuan and Guo, Huadong and Lasaponara, Rosa and Zong, Xin and Masini, Nicola and Wang, Guizhou and Shi, Pilong and Khatteli, Houcine and Chen, Fulong and Tariq, Shahina and Shao, Jie and Bachagha, Nabil and Yang, Ruixia and Yao, Ya},
	month = oct,
	year = {2019},
	keywords = {Airborne, Archaeological, Cultural heritage, DTM, LiDAR, Marks, Photography, Remote sensing, SAR, Site, Spaceborne, Spectral, UAVs},
	pages = {111280},
}

@misc{noauthor_relief_nodate,
	title = {Relief {Visualization} {Toolbox} — {QGIS} {Python} {Plugins} {Repository}},
	url = {https://plugins.qgis.org/plugins/rvt-qgis/},
	urldate = {2023-11-23},
}

@book{kokalj_airborne_2017,
	series = {Prostor, kraj, čas},
	title = {Airborne laser scanning raster data visualization},
	volume = {14},
	isbn = {978-961-254-984-8},
	url = {https://omp.zrc-sazu.si/zalozba/catalog/book/824},
	urldate = {2023-11-23},
	publisher = {ZRC SAZU, Založba ZRC},
	author = {Kokalj, Žiga and Hesse, Ralf},
	month = jul,
	year = {2017},
	doi = {10.3986/9789612549848},
}

@misc{noauthor_relief_nodate-1,
	title = {Relief {Visualization} {Toolbox} ({RVT}) {\textbar} {ZRC} {SAZU}},
	url = {https://iaps.zrc-sazu.si/en/rvt},
	abstract = {Relief Visualization Toolbox was developed to help scientist visualize raster elevation model datasets. We narrowed down the selection to include techniques that have proven to be effective for identification of small scale features. Default settings therefore assume working with high resolution digital elevation models, derived from airborne laser scanning missions (lidar).},
	language = {en},
	urldate = {2023-11-23},
}

@misc{noauthor_computational_nodate,
	title = {Computational modelling of terrains},
	url = {https://tudelft3d.github.io/terrainbook/},
	abstract = {An open book},
	language = {en-US},
	urldate = {2023-11-21},
	journal = {Open book},
}

@book{spillias_human-ai_2023,
	title = {Human-{AI} {Collaboration} to {Identify} {Literature} for {Evidence} {Synthesis}},
	abstract = {Systematic approaches to evidence synthesis can improve the rigour, transparency, and replicability of a traditional literature review. However, these systematic approaches are time and resource intensive. We evaluate the ability of OpenAI’s ChatGPT to undertake two initial stages of evidence syntheses (searching peer-reviewed literature and screening for relevance) and develop a novel collaborative framework to leverage the best of both human and AI intelligence. Using a scoping review of community-based fisheries management as a case study, we find that with substantial prompting, the AI can provide critical insight into the construction and content of a search string. Thereafter, we evaluate five strategies for synthesising AI output to screen articles based on predefined inclusion criteria. We find low omission rates ({\textless} 1\%) of relevant literature by the AI are achievable, which is comparable to that of human screeners. These findings show that generalised AI tools can assist reviewers with evidence synthesis to accelerate the implementation and improve the reliability of a review.},
	author = {Spillias, Scott and Tuohy, Paris and Andreotta, Matthew and Annand-Jones, Ruby and Boschetti, Fabio and Cvitanovic, Christopher and Duggan, Joe and Fulton, Elizabeth and Karcher, Denis and Paris, Cécile and Shellock, Rebecca and Trebilco, Rowan},
	month = jun,
	year = {2023},
	doi = {10.21203/rs.3.rs-3099291/v1},
}

@misc{jalali_towards_2023,
	title = {Towards {eXplainable} {AI} for {Mobility} {Data} {Science}},
	url = {http://arxiv.org/abs/2307.08461},
	doi = {10.48550/arXiv.2307.08461},
	abstract = {This paper presents our ongoing work towards XAI for Mobility Data Science applications, focusing on explainable models that can learn from dense trajectory data, such as GPS tracks of vehicles and vessels using temporal graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI studies, argue the need for comprehensible explanations with human-centered approaches, and outline a research path toward XAI for Mobility Data Science.},
	urldate = {2023-11-20},
	publisher = {arXiv},
	author = {Jalali, Anahid and Graser, Anita and Heistracher, Clemens},
	month = sep,
	year = {2023},
	note = {arXiv:2307.08461 [cs]},
	keywords = {Computer Science - Artificial Intelligence, F.2.2},
}

@book{jalali_towards_2023-1,
	title = {Towards {eXplainable} {AI} for {Mobility} {Data} {Science}},
	abstract = {This paper presents our ongoing work towards XAI for Mobility Data Science applications, focusing on explainable models that can learn from dense trajectory data, such as GPS tracks of vehicles and vessels using temporal graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI studies, argue the need for comprehensible explanations with human-centered approaches, and outline a research path toward XAI for Mobility Data Science.},
	author = {Jalali, Anahid and Graser, Anita and Heistracher, Clemens},
	month = jul,
	year = {2023},
}

@article{openshaw_applying_1999,
	title = {Applying {Geocomputation} to the {Analysis} of {Spatial} {Distributions}},
	volume = {1},
	journal = {Geographic information systems: Principles and technical issues},
	author = {Openshaw, S. and Alvanides, Seraphim},
	month = jan,
	year = {1999},
}

@article{openshaw_algorithms_1995,
	title = {Algorithms for {Reengineering} 1991 {Census} {Geography}},
	volume = {27},
	issn = {0308-518X},
	url = {https://doi.org/10.1068/a270425},
	doi = {10.1068/a270425},
	abstract = {The availability of GIS technology and digital boundaries of census output areas now makes it possible for users to design their own census geography. Three algorithms are described that can be used for this purpose. An Arc/Info implementation is briefly outlined and case studies presented to demonstrate some of the results of explicitly designing zoning systems for use with 1991 Census data.},
	language = {en},
	number = {3},
	urldate = {2023-11-20},
	journal = {Environment and Planning A: Economy and Space},
	author = {Openshaw, S and Rao, L},
	month = mar,
	year = {1995},
	note = {Publisher: SAGE Publications Ltd},
	pages = {425--446},
}

@article{openshaw_empirical_1978,
	title = {An {Empirical} {Study} of {Some} {Zone}-{Design} {Criteria}},
	volume = {10},
	issn = {0308-518X},
	url = {https://doi.org/10.1068/a100781},
	doi = {10.1068/a100781},
	abstract = {The results obtained from studies of spatially aggregated data are not independent of the choice of zoning system. The paper investigates the effects of different zone-design criteria on a linear-regression model. It is concluded that there is unlikely to be either a simple or general-purpose solution to the problem.},
	language = {en},
	number = {7},
	urldate = {2023-11-20},
	journal = {Environment and Planning A: Economy and Space},
	author = {Openshaw, S},
	month = jul,
	year = {1978},
	note = {Publisher: SAGE Publications Ltd},
	pages = {781--794},
}

@article{haddaway_eight_2020,
	title = {Eight problems with literature reviews and how to fix them},
	volume = {4},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-020-01295-x},
	doi = {10.1038/s41559-020-01295-x},
	language = {en},
	number = {12},
	urldate = {2023-11-15},
	journal = {Nature Ecology \& Evolution},
	author = {Haddaway, Neal R. and Bethel, Alison and Dicks, Lynn V. and Koricheva, Julia and Macura, Biljana and Petrokofsky, Gillian and Pullin, Andrew S. and Savilaakso, Sini and Stewart, Gavin B.},
	month = oct,
	year = {2020},
	pages = {1582--1589},
}

@article{raposo_scale_2017,
	title = {Scale and {Generalization}},
	volume = {2017},
	url = {http://gistbok.ucgis.org/bok-topics/scale-and-generalization-1},
	doi = {10.22224/gistbok/2017.4.3},
	number = {Q4},
	urldate = {2023-11-13},
	journal = {Geographic Information Science \& Technology Body of Knowledge},
	author = {Raposo, Paulo},
	month = oct,
	year = {2017},
}

@article{lockhart_scottish_1980,
	title = {Scottish village plans: {A} preliminary analysis},
	volume = {96},
	issn = {0036-9225},
	shorttitle = {Scottish village plans},
	url = {https://doi.org/10.1080/00369228008736468},
	doi = {10.1080/00369228008736468},
	abstract = {First, a review of previous publications indicates the nature of research over the last fifty years. The second and third parts outline the major sources of information, such as estate papers, plans, newspapers and printed books, and the research techniques used in the study. The fourth and major section examines the plan characteristics of kirktowns, fishing settlements, planned villages, and other planned settlement forms.},
	number = {3},
	urldate = {2023-11-13},
	journal = {Scottish Geographical Magazine},
	author = {Lockhart, Douglas G.},
	month = dec,
	year = {1980},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00369228008736468},
	pages = {141--157},
}

@article{roberts_village_1972,
	title = {Village {Plans} in {County} {Durham}: {A} {Preliminary} {Statement}},
	volume = {16},
	issn = {0076-6097},
	shorttitle = {Village {Plans} in {County} {Durham}},
	url = {https://doi.org/10.1080/00766097.1972.11735345},
	doi = {10.1080/00766097.1972.11735345},
	abstract = {THIS paper examines some facets of rural settlement-morphology within co. Durham and demonstrates the way in which all villages involve particular combinations of universal structural components. These provide grounds for a logical classification and a framework within which to explore the origins of some of the plan-types that can be identified. A close analysis of the most regular series, the two-row plans, indicates that a strong measure of deliberate ordering was involved and evidence is presented to suggest that some villages, which were first mapped in the 19th century, possess a plan-type that developed before 1200. Furthermore there are grounds for arguing that there may be traces of village-regulation, such is as found in Scandinavia and has been described by G. C. Homans. A possible date for the imposition of this regularity would be the years immediately after the widespread devastations in the north at the end of the 11th century.},
	number = {1},
	urldate = {2023-11-13},
	journal = {Medieval Archaeology},
	author = {Roberts, Brian K.},
	month = jan,
	year = {1972},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00766097.1972.11735345},
	pages = {33--56},
}

@article{aitchison_town_1996,
	title = {The town and village greens of {England} and {Wales}},
	volume = {21},
	issn = {0142-6397},
	url = {https://doi.org/10.1080/01426399608706477},
	doi = {10.1080/01426399608706477},
	abstract = {This paper considers the nature and significance of town and village greens within the landscape of England and Wales. It reflects on problems of definition and on the limited availability of data concerning the origins and geographical distribution of greens. The paper draws attention to the registration of greens following the Commons Registration Act 1965, and summarizes information collated from the registers subsequently prepared by local authorities. Details of numbers of greens and their spatial density at county level are presented, and reference is also made to rights of common on greens, and to the ownership of the greens themselves.},
	number = {1},
	urldate = {2023-11-13},
	journal = {Landscape Research},
	author = {Aitchison, John},
	month = mar,
	year = {1996},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/01426399608706477},
	keywords = {commons, geography, greens, ownership, rights},
	pages = {89--97},
}

@article{stamp_common_1964,
	title = {The {Common} {Lands} and {Village} {Greens} of {England} and {Wales}},
	volume = {130},
	issn = {0016-7398},
	url = {https://www.jstor.org/stable/1792256},
	doi = {10.2307/1792256},
	number = {4},
	urldate = {2023-11-13},
	journal = {The Geographical Journal},
	author = {Stamp, L. Dudley},
	year = {1964},
	note = {Publisher: [Wiley, Royal Geographical Society (with the Institute of British Geographers)]},
	pages = {457--468},
}

@phdthesis{shirley_village_1994,
	type = {Doctoral},
	title = {Village greens of {England}: a study in historical geography},
	shorttitle = {Village greens of {England}},
	url = {http://etheses.dur.ac.uk/6120/},
	abstract = {The thesis involves a study of the English village green from the viewpoint of historical geography on aspects of greens as rural settlement. The presence of village greens in the landscape poses three categories of questions; concerning their origins, their present status and their future. With these categories of questions in mind, the research focuses pricipally on three main areas, law and regulation - including common rights and registration, inclosure and disputes. These subjects are covered under the themes of nation and local (manorial) law with a historic aspect throughout the study. types of village green - an examination of the wide variety of physical forms and origins covers greens which have been planned partially planned or formed from the residuum of some other landscape feature. distribution - a national database of village greens has made possible the production of national ma ps of these different types of greens together with surviving common rights and greens sorted on ownership types. The principal original contributions take the form of a collation of the law concerning village greens from diverse sources, a classification of their various types and numerous national and regional distribution maps of the location and types of greens and common rights and classes of owners of the greens resulting from the compilation of a national database of registerd greens.},
	urldate = {2023-11-13},
	school = {Durham University},
	author = {Shirley, Rob},
	year = {1994},
}

@misc{noauthor_visvalingamwhyatt_2022,
	title = {Visvalingam–{Whyatt} algorithm},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Visvalingam%E2%80%93Whyatt_algorithm&oldid=1102570779},
	abstract = {The Visvalingam–Whyatt algorithm, also known as the Visvalingam's algorithm, is an algorithm that decimates a curve composed of line segments to a similar curve with fewer points.},
	language = {en},
	urldate = {2023-11-13},
	journal = {Wikipedia},
	month = aug,
	year = {2022},
	note = {Page Version ID: 1102570779},
}

@article{opheim_smoothing_1981,
	title = {{SMOOTHING} {A} {DIGITIZED} {CURVE} {BY} {DATA} {REDUCTION} {METHODS}},
	issn = {1017-4656},
	url = {https://diglib.eg.org/handle/10.2312/eg19811012},
	doi = {10.2312/EG.19811012},
	abstract = {In order to reduce storage and subsequent processing costs for a digitized cartographic curve, we may wish to reduce the data set. The data reduction method which is described in this paper, can be used on-line during the digitizing as well as afterwards on the total data set. We only wish to retain the salient points, and thus data reduction can be regarded as a tool for smoothing a curve. The paper also describes another way of smoothing, based on signal theory.},
	language = {en},
	urldate = {2023-11-13},
	journal = {Eurographics Conference Proceedings},
	author = {Opheim, Harald},
	year = {1981},
	note = {Artwork Size: 9 pages
Publisher: The Eurographics Association},
	pages = {9 pages},
}

@misc{mkordi_pygwr_2023,
	title = {pygwr: a simple {GWR} in {Python}},
	shorttitle = {pygwr},
	url = {https://github.com/mkordi/pygwr},
	abstract = {Geographically Weighted Regression (GWR) in Python},
	urldate = {2023-11-13},
	author = {mkordi},
	month = sep,
	year = {2023},
	note = {original-date: 2013-07-16T14:54:17Z},
}

@article{gibbs_harnessing_nodate,
	title = {Harnessing mobility data to capture changing work from home behaviours between censuses},
	volume = {n/a},
	copyright = {The information, practices and views in this article are those of the author(s) and do not necessarily reflect the opinion of the Royal Geographical Society (with IBG). © 2023 The Authors. The Geographical Journal published by John Wiley \& Sons Ltd on behalf of Royal Geographical Society (with the Institute of British Geographers).},
	issn = {1475-4959},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/geoj.12555},
	doi = {10.1111/geoj.12555},
	abstract = {This paper provides an analysis of working from home patterns in England using data from the 2021 Census to understand (1) how patterns of working from home (WFH) in England have shifted since the COVID-19 pandemic and (2) whether human mobility indicators, specifically Google Community Mobility Reports, provide a reliable proxy for WFH patterns recorded by the 2021 Census, providing a formal evaluation of the reliability of such datasets, whose applications have grown exponentially over the COVID-19 pandemic. We find that WFH patterns recorded by the 2021 Census were unique compared with previous UK censuses, reflecting an unprecedented increase likely caused by persistent changes to employment during the COVID-19 pandemic, with a clear social gradient emerging across the country. We also find that Google mobility in ‘Residential’ and ‘Workplace’ settings provides a reliable measurement of the distribution of WFH populations across Local Authorities, with varying uncertainties for mobility indicators collected in different settings. These findings provide insights into the utility of such datasets to support population research in intercensal periods, where shifts may be occurring, but can be difficult to quantify empirically.},
	language = {en},
	number = {n/a},
	urldate = {2023-11-13},
	journal = {The Geographical Journal},
	author = {Gibbs, Hamish and Ballantyne, Patrick and Cheshire, James and Singleton, Alex and Green, Mark A.},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/geoj.12555},
	keywords = {COVID-19, England, Google, census, mobility, working from home},
}

@article{luis_moreira_de_sousa_spatial_2023,
	title = {Spatial {Linked} {Data} {Infrastructures}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://zenodo.org/doi/10.5281/zenodo.10113504},
	doi = {10.5281/ZENODO.10113504},
	abstract = {An introduction to the Semantic Web for GIS professional and scientists. Presents a path to FAIR geo-spatial data with W3C and OGC standards.},
	language = {en},
	urldate = {2023-11-13},
	author = {Luís Moreira de Sousa},
	month = nov,
	year = {2023},
	note = {Publisher: Zenodo
Version Number: v0.2},
	keywords = {GIS, Geo-spatial data, GeoSPARQL, Linked Data, Semantic Web},
}

@article{zhao_editorial_2023,
	title = {Editorial: {Human} spatial perception, cognition, and behaviour in extended reality},
	volume = {4},
	issn = {2673-4192},
	shorttitle = {Editorial},
	url = {https://www.frontiersin.org/articles/10.3389/frvir.2023.1257230},
	urldate = {2023-11-11},
	journal = {Frontiers in Virtual Reality},
	author = {Zhao, Jiayan and Riecke, Bernhard E. and Kelly, Jonathan W. and Stefanucci, Jeanine and Klippel, Alexander},
	year = {2023},
}

@article{boeing_osmnx_2017,
	title = {{OSMnx}: {New} methods for acquiring, constructing, analyzing, and visualizing complex street networks},
	volume = {65},
	issn = {0198-9715},
	shorttitle = {{OSMnx}},
	url = {https://www.sciencedirect.com/science/article/pii/S0198971516303970},
	doi = {10.1016/j.compenvurbsys.2017.05.004},
	abstract = {Urban scholars have studied street networks in various ways, but there are data availability and consistency limitations to the current urban planning/street network analysis literature. To address these challenges, this article presents OSMnx, a new tool to make the collection of data and creation and analysis of street networks simple, consistent, automatable and sound from the perspectives of graph theory, transportation, and urban design. OSMnx contributes five significant capabilities for researchers and practitioners: first, the automated downloading of political boundaries and building footprints; second, the tailored and automated downloading and constructing of street network data from OpenStreetMap; third, the algorithmic correction of network topology; fourth, the ability to save street networks to disk as shapefiles, GraphML, or SVG files; and fifth, the ability to analyze street networks, including calculating routes, projecting and visualizing networks, and calculating metric and topological measures. These measures include those common in urban design and transportation studies, as well as advanced measures of the structure and topology of the network. Finally, this article presents a simple case study using OSMnx to construct and analyze street networks in Portland, Oregon.},
	urldate = {2023-11-10},
	journal = {Computers, Environment and Urban Systems},
	author = {Boeing, Geoff},
	month = sep,
	year = {2017},
	keywords = {Complex networks, GIS, OpenStreetMap, Python, Resilience, Street network, Transportation, Urban design, Urban form, Visualization},
	pages = {126--139},
}

@misc{boeing_osmnx_2023,
	title = {{OSMnx}},
	copyright = {MIT},
	url = {https://github.com/gboeing/osmnx},
	abstract = {OSMnx is a Python package to easily download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap.},
	urldate = {2023-11-10},
	author = {Boeing, Geoff},
	month = nov,
	year = {2023},
	note = {original-date: 2016-07-24T02:57:47Z},
	keywords = {geography, geospatial, gis, mapping, networks, networkx, openstreetmap, osm, osmnx, overpass-api, python, routing, spatial, spatial-analysis, spatial-data, street-networks, transport, transportation, urban, urban-planning},
}

@article{noauthor_research_nodate,
	title = {Research {Software} {Engineers}: {Creating} a {Career} {Path}—and a {Career}},
	shorttitle = {Research {Software} {Engineers}},
	url = {https://zenodo.org/records/10073233},
	doi = {10.5281/zenodo.10073233},
	language = {en},
	urldate = {2023-11-08},
}

@misc{jdrem_chernoff_2021,
	title = {Chernoff {Faces}},
	url = {https://github.com/jdrem/chernoff-faces},
	abstract = {Library to create Chernoff faces.},
	urldate = {2023-11-07},
	author = {jdrem},
	month = nov,
	year = {2021},
	note = {original-date: 2021-05-15T09:49:36Z},
	keywords = {chernoff-faces, data-visualization},
}

@misc{noauthor_python-packageschernoffface_nodate,
	title = {Python-packages/{ChernoffFace} at main · antononcube/{Python}-packages},
	url = {https://github.com/antononcube/Python-packages/tree/main/ChernoffFace},
	abstract = {Miscellaneous Python packages. Some correspond to the packages in MathematicaForPrediction. - antononcube/Python-packages},
	urldate = {2023-11-07},
	keywords = {Cartography, Chernoff faces, python},
}

@misc{singh_chernoff_2022,
	title = {Chernoff {Faces}},
	url = {https://github.com/gnarmis/chernoff-faces},
	abstract = {Multivariate data visualization using D3.js and React.js},
	urldate = {2023-11-07},
	author = {Singh, Gursimran},
	month = nov,
	year = {2022},
	note = {original-date: 2014-12-26T21:55:33Z},
}

@article{raciborski_graphical_2009,
	title = {Graphical representation of multivariate data using {Chernoff} faces},
	volume = {9},
	url = {https://econpapers.repec.org/article/tsjstataj/v_3a9_3ay_3a2009_3ai_3a3_3ap_3a374-387.htm},
	abstract = {Chernoff (1971, Technical Report 71, Department of Statistics, Stan- ford University; 1973, Journal of the American Statistical Association 68: 361– 368) proposed the use of cartoon-like faces to represent points in k dimensions. This article describes a Stata implementation of a face-generating algorithm using the method proposed by Flury (1980, Technical Report 3, Institute of Mathemati- cal Statistics and Actuarial Science, Bern University), Schu ̈pbach (1987, Technical Report 25, Institute of Mathematical Statistics and Acturial Science, Bern Univer- sity), and Friendly (1991, http://www.math.yorku.ca/SCS/sasmac/faces.html). I present examples of applying Chernoff faces to data clustering and outlier detection. Copyright 2009 by StataCorp LP.},
	number = {3},
	urldate = {2023-11-07},
	journal = {Stata Journal},
	author = {Raciborski, Rafal},
	year = {2009},
	note = {Publisher: StataCorp LP},
	keywords = {Chernoff faces, chernoff, graphs},
	pages = {374--387},
}

@article{di_maio_geophysical_2021,
	title = {Geophysical prospecting for the pre- and early-historical reconstruction of the subsurface underneath the {Paleochristian} {Basilica} of {Santa} {Maria} di {Compulteria} (northern {Campania}, {Italy})},
	volume = {38},
	issn = {2352409X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352409X21003035},
	doi = {10.1016/j.jasrep.2021.103091},
	language = {en},
	urldate = {2023-11-02},
	journal = {Journal of Archaeological Science: Reports},
	author = {Di Maio, Rosa and Frisetti, Alessia and Ferranti, Luigi and De Paola, Claudio and La Manna, Mauro and Piegari, Ester},
	month = aug,
	year = {2021},
	pages = {103091},
}

@incollection{hosny_parallel_2023,
	address = {Cham},
	title = {Parallel {Image} {Processing} {Applications} {Using} {Raspberry} {Pi}},
	volume = {1073},
	isbn = {978-3-031-18734-6 978-3-031-18735-3},
	url = {https://link.springer.com/10.1007/978-3-031-18735-3_6},
	language = {en},
	urldate = {2023-11-02},
	booktitle = {Recent {Advances} in {Computer} {Vision} {Applications} {Using} {Parallel} {Processing}},
	publisher = {Springer International Publishing},
	author = {Hosny, Khalid M. and Salah, Ahmad and Magdi, Amal},
	editor = {Hosny, Khalid M. and Salah, Ahmad},
	year = {2023},
	doi = {10.1007/978-3-031-18735-3_6},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {107--119},
}

@article{verbrugghe_routes_2017,
	title = {Routes across the {Civitas} {Menapiorum}: using least cost paths and {GIS} to locate the {Roman} roads of {Sandy} {Flanders}},
	volume = {57},
	issn = {0305-7488},
	shorttitle = {Routes across the {Civitas} {Menapiorum}},
	url = {https://www.sciencedirect.com/science/article/pii/S0305748817301123},
	doi = {10.1016/j.jhg.2017.06.006},
	abstract = {Despite a long research history, little is known about the Roman road network in the northern part of the Civitas Menapiorum. In late nineteenth- and early twentieth-century publications researchers established the idea that Roman roads ended at the transition between the loamy and sandy areas. Others indicated that such roads were not yet located. During the last twenty years researchers have presumed the existence of five supralocal Roman roads in Sandy Flanders. However, their exact routes remained unclear. In this study a landscape archaeological approach is applied to study these roads and to suggest a series of possible routes. Based on recent LiDAR data and soil maps, least cost path analyses are performed in ArcGIS. Recent archaeological findings and double linear marks, visible on both oblique and vertical aerial photographs, are used to test the accuracy of the least cost paths. Finally, the names of present-day streets are used to clarify the continuity of Roman roads in the landscape.},
	urldate = {2023-10-23},
	journal = {Journal of Historical Geography},
	author = {Verbrugghe, Gerben and De Clercq, Wim and Van Eetvelde, Veerle},
	month = jul,
	year = {2017},
	keywords = {Cost surface, Crop and soil marks, LiDAR, Roman roads, Sinuosity index},
	pages = {76--88},
}

@article{brewer_framing_2007,
	title = {Framing {Guidelines} for {Multi}-{Scale} {Map} {Design} {Using} {Databases} at {Multiple} {Resolutions}},
	volume = {34},
	issn = {1523-0406},
	url = {https://doi.org/10.1559/152304007780279078},
	doi = {10.1559/152304007780279078},
	abstract = {This paper extends European research on generating cartographic base maps at multiple scales from a single detailed database. Similar to the European work, we emphasize reference maps. In contrast to the Europeans' focus on data generalization, we emphasize changes to the map display, including symbol design or symbol modification. We also work from databases compiled at multiple resolutions rather than constructing all representations from a single high-resolution database. We report results demonstrating how symbol change combined with selection and elimination of subsets of features can produce maps through almost the entire range of map scales spanning 1:10,000 to 1:5,000,000. We demonstrate a method of establishing specific map display scales at which symbol modification should be imposed. We present a prototype decision tool called ScaleMaster that can guide multi-scale map design across a small or large range of data resolutions, display scales, and map purposes.},
	number = {1},
	urldate = {2023-11-01},
	journal = {Cartography and Geographic Information Science},
	author = {Brewer, Cynthia A. and Buttenfield, Barbara P.},
	month = jan,
	year = {2007},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1559/152304007780279078},
	pages = {3--15},
}

@article{nion-alvarez_methodological_2022,
	title = {A methodological approach to identify {Roman} roads using {LiDAR} sensing technology and aerial orthoimages. {The} case of viae {XIX} and {XX} ({NW} {Iberia})},
	volume = {45},
	issn = {2352-409X},
	url = {https://www.sciencedirect.com/science/article/pii/S2352409X22002759},
	doi = {10.1016/j.jasrep.2022.103612},
	abstract = {The following article proposes a methodology for the identification of Roman roads applied to the Northwest of the Iberian Peninsula, focused mainly on the conventus lucensis. Therefore, a methodology based on two aspects is presented: on the one hand, on the description of remote-sensing techniques, their relevance, and their validity; on the other hand, on the strategies that allow not only to identify historical roads, but also to differentiate and catalog them according to their structural characteristics. Finally, a brief context of the state of research on historical roads in the Northwest of the Peninsula is explained in order to apply the proposed methods in two cases of studies on the XIX and XX routes of the Itinerarium Antonini Augusti.},
	urldate = {2023-10-23},
	journal = {Journal of Archaeological Science: Reports},
	author = {Nión-Álvarez, Samuel},
	month = oct,
	year = {2022},
	keywords = {Ancient roads, GIS, LiDAR, NW Iberia, Remote- sensing, Roman roads},
	pages = {103612},
}

@article{lewis_explaining_2023,
	title = {Explaining {Known} {Past} {Routes}, {Underdetermination}, and the {Use} of {Multiple} {Cost} {Functions}},
	issn = {1573-7764},
	url = {https://doi.org/10.1007/s10816-023-09621-w},
	doi = {10.1007/s10816-023-09621-w},
	abstract = {Explaining material traces of movement as proxies for past movement is fundamental for understanding the processes behind why people in the past traversed the landscape in the way that they did. For this, least-cost path analysis and the use of slope-based cost functions for estimating the cost of movement when walking have become commonplace. Despite their prevalence, current approaches misrepresent what these cost functions are, their relationship to the hypotheses that they aim to represent, and their role in explanation. As a result, least-cost paths calculated using single cost functions are liable to spurious results with limited power for explaining known past routes, and by extension the decision-making processes of past people. Using the ideas of multiple model idealisation and robustness analysis, and applied via a tactical simulation, this study demonstrates that similar least-cost paths can be produced from slope-based cost functions representing both the same hypothesis and different hypotheses, suggesting that least-cost path results are robust but underdetermined under the tested environmental settings. The results from this tactical simulation are applied for the explanation of a Roman road in Sardinia. Using probabilistic least-cost paths as an approach for incorporating multiple cost functions representing the same hypothesis and error in the digital elevation model, it is shown that both model outcomes representing the minimisation of time and energy are unable to explain the placement of the Roman road. Rather, it is suggested that the Roman road was influenced by pre-existing routes and settlements.},
	language = {en},
	urldate = {2023-11-01},
	journal = {Journal of Archaeological Method and Theory},
	author = {Lewis, Joseph},
	month = sep,
	year = {2023},
	keywords = {Cost function, Least-cost path, Postdiction, Robustness, Roman roads, Underdetermination},
}

@article{di_maio_reconstruction_2023,
	title = {Reconstruction of archaeological contexts through the integrated use of airborne {LiDAR} and geophysical survey: {The} case study of {San} {Pietro} {Infine} ({Caserta}, southern {Italy})},
	volume = {49},
	issn = {2352409X},
	shorttitle = {Reconstruction of archaeological contexts through the integrated use of airborne {LiDAR} and geophysical survey},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352409X23001888},
	doi = {10.1016/j.jasrep.2023.104013},
	language = {en},
	urldate = {2023-11-01},
	journal = {Journal of Archaeological Science: Reports},
	author = {Di Maio, Rosa and Emolo, Antonio and Frisetti, Alessia and Abate, Nicodemo and La Manna, Mauro and Pierri, Ivano and Salone, Rosanna and Tarantino, Stefania},
	month = jun,
	year = {2023},
	pages = {104013},
}

@article{aamodt_simulation_2006,
	title = {A simulation study of three methods for detecting disease clusters},
	volume = {5},
	issn = {1476-072X},
	url = {https://doi.org/10.1186/1476-072X-5-15},
	doi = {10.1186/1476-072X-5-15},
	abstract = {Cluster detection is an important part of spatial epidemiology because it can help identifying environmental factors associated with disease and thus guide investigation of the aetiology of diseases. In this article we study three methods suitable for detecting local spatial clusters: (1) a spatial scan statistic (SaTScan), (2) generalized additive models (GAM) and (3) Bayesian disease mapping (BYM). We conducted a simulation study to compare the methods. Seven geographic clusters with different shapes were initially chosen as high-risk areas. Different scenarios for the magnitude of the relative risk of these areas as compared to the normal risk areas were considered. For each scenario the performance of the methods were assessed in terms of the sensitivity, specificity, and percentage correctly classified for each cluster.},
	number = {1},
	urldate = {2023-11-01},
	journal = {International Journal of Health Geographics},
	author = {Aamodt, Geir and Samuelsen, Sven O. and Skrondal, Anders},
	month = apr,
	year = {2006},
	keywords = {Cluster Detection, Cluster Type, Generalize Additive Model, Likelihood Ratio Statistic, Spatial Cluster},
	pages = {15},
}

@article{mclafferty_disease_2015,
	title = {Disease cluster detection methods: recent developments and public health implications},
	volume = {21},
	issn = {1947-5683},
	shorttitle = {Disease cluster detection methods},
	url = {https://doi.org/10.1080/19475683.2015.1008572},
	doi = {10.1080/19475683.2015.1008572},
	abstract = {Methods for detecting spatial and spatiotemporal clusters of health and disease have advanced significantly in the past decade. This article reviews recent advances in four areas: spatial search processes, network-based methods, statistical analysis and modelling of local clusters and space-time cluster detection. I then turn to a more critical discussion of the implications of hotspot mapping for public health policy and intervention, highlighting the need to incorporate process-based understandings that impact spatial and social inequalities in ill health for particular health issues in particular geographic contexts.},
	number = {2},
	urldate = {2023-11-01},
	journal = {Annals of GIS},
	author = {Mclafferty, Sara},
	month = apr,
	year = {2015},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/19475683.2015.1008572},
	keywords = {public health, space-time, spatial cluster detection},
	pages = {127--133},
}

@article{takahashi_detecting_2020,
	title = {Detecting multiple spatial disease clusters: information criterion and scan statistic approach},
	volume = {19},
	issn = {1476-072X},
	shorttitle = {Detecting multiple spatial disease clusters},
	url = {https://doi.org/10.1186/s12942-020-00228-y},
	doi = {10.1186/s12942-020-00228-y},
	abstract = {Detecting the geographical tendency for the presence of a disease or incident is, particularly at an early stage, a key challenge for preventing severe consequences. Given recent rapid advancements in information technologies, it is required a comprehensive framework that enables simultaneous detection of multiple spatial clusters, whether disease cases are randomly scattered or clustered around specific epicenters on a larger scale. We develop a new methodology that detects multiple spatial disease clusters and evaluates its performance compared to existing other methods.},
	number = {1},
	urldate = {2023-11-01},
	journal = {International Journal of Health Geographics},
	author = {Takahashi, Kunihiko and Shimadzu, Hideyasu},
	month = sep,
	year = {2020},
	keywords = {Cluster detection test, Generalized linear model, Information criteria, Multiple clustering, Scan statistic},
	pages = {33},
}

@book{alexander_methods_1996,
	title = {Methods for {Investigating} {Localized} {Clustering} of {Disease}},
	isbn = {978-92-832-2135-7},
	abstract = {Since the beginning of this century, "clusters" of certain forms of cancer--particularly leukemia in children and Hodgkin's disease--have been reported around locations of specific environmental hazards. Identification of such clusters is not an easy task, since there is no exact definition ofwhat a cluster is. This monograph describes the variety of statististical techniques cuurently in use, and their application to simulated data-sets chosen to represent a range of clustering scenarios. The scientists who developed these techniques were invited to apply their methodology to thesedata-sets and to share their conclusions in this volume. In addition, these researchers describe in complete detail how they proceeded with the analysis, since an element of subjectivity figures prominently in the application and interpretation of some of these methods. The identification andanalysis of disease clusters can yield significant clues in epidemiologic research, and as such will continue to be an important subject of cancer research and epidemiology for the foreseeable future.},
	language = {en},
	publisher = {International Agency for Research on Cancer, World Health Organization},
	author = {Alexander, F. E. and Boyle, Peter},
	year = {1996},
	note = {Google-Books-ID: 4jcQAQAAMAAJ},
	keywords = {Medical / Diagnosis, Medical / Epidemiology, Science / Life Sciences / Botany},
}

@article{vesanto_clustering_nodate,
	title = {Clustering of the self-organizing map},
	volume = {11},
	url = {https://ieeexplore.ieee.org/document/846731},
	doi = {https://doi.org/10.1109/72.846731},
	number = {3},
	urldate = {2023-11-01},
	journal = {IEEE Transactions of Neural Networks},
	author = {Vesanto, J and Alhoniemi, E},
}

@article{thorpe_herbicides_2005,
	title = {Herbicides and {Nitrates} in {Groundwater} of {Maryland} and {Childhood} {Cancers}: {A} {Geographic} {Information} {Systems} {Approach}},
	volume = {23},
	issn = {1059-0501},
	shorttitle = {Herbicides and {Nitrates} in {Groundwater} of {Maryland} and {Childhood} {Cancers}},
	url = {https://doi.org/10.1080/10590500500235001},
	doi = {10.1080/10590500500235001},
	abstract = {This hypothesis-generating study explores spatial patterns of childhood cancers in Maryland and investigates their potential associations with herbicides and nitrates in groundwater. The Maryland Cancer Registry (MCR) provided data for bone and brain cancers, leukemia, and lymphoma, for ages 0–17, during the years 1992–1998. Cancer clusters and relative risks generated in the study indicate higher relative risk areas and potential clusters in several counties. Contingency table analysis indicates a potential association with several herbicides and nitrates. Cancer rates for the four types have a crude odds ratio (OR) = 1.10 (0.78–1.56) in relationship to atrazine, and an OR = 1.54 (1.14–2.07) for metolachlor. Potential association to mixtures of three compounds give an OR = 7.56 (4.16–13.73). A potential association is indicated between leukemia and nitrates, OR = 1.81 (1.35–2.42), and bone cancer with metolachlor, OR = 2.26 (0.97–5.24). These results give insight to generate a hypothesis of the potential association between exposure to these herbicides and nitrates and specific types of childhood cancer.},
	number = {2},
	urldate = {2023-11-01},
	journal = {Journal of Environmental Science and Health, Part C},
	author = {Thorpe, Nancy and SHIRMOHAMMADI, ADEL},
	month = jul,
	year = {2005},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10590500500235001},
	keywords = {Atrazine, Childhood Cancers, Groundwater and Cancer, Herbicides and Cancer, Water Quality},
	pages = {261--278},
}

@article{sabel_modelling_2000,
	title = {Modelling exposure opportunities: estimating relative risk for motor neurone disease in {Finland}},
	volume = {50},
	issn = {0277-9536},
	shorttitle = {Modelling exposure opportunities},
	url = {https://www.sciencedirect.com/science/article/pii/S0277953699003603},
	doi = {10.1016/S0277-9536(99)00360-3},
	abstract = {This paper addresses the issues surrounding an individual's exposure to potential environmental risk factors, which can be implicated in the aetiology of a disease. We hope to further elucidate the ‘lag’ or latency period between the initial exposure to potential pathogens and the physical emergence of the disease, with specific reference to the rare neurological condition, motor neurone disease (MND), using a dataset obtained from the Finnish Death Certificate registry, for MND deaths between the period 1985–1995. A space–time approach is adopted, whereby patterns in both time and space are considered. No prior assumptions about the aetiology of MND are adopted. By using methods for the analysis of point processes, which preserve the continuous nature of the data, we resolve some of the problems of analysis that are often based on arbitrary areal units, such as postcode boundaries, or political boundaries. We use kernel estimation to model space–time patterns. Raised relative risk is assessed by adopting appropriate adjustments for the underlying population at risk, with the use of controls. Significance of the results is assessed using Monte Carlo simulation, and comparisons are made with results obtained from Openshaw's geographical analysis machine (GAM). Our results demonstrate the utility of kernel estimation as a visualisation tool. Small areas of elevated risk are identified, which need to be more closely examined before any firm conclusions can be drawn. We highlight a number of issues concerning the inadequacies of the data, and possibly of the techniques themselves.},
	number = {7},
	urldate = {2023-11-01},
	journal = {Social Science \& Medicine},
	author = {Sabel, Clive E and Gatrell, Anthony C and Löytönen, Markku and Maasilta, Paula and Jokelainen, Matti},
	month = apr,
	year = {2000},
	keywords = {Cluster detection, Finland, GIS, Kernel estimation, Motor neurone disease (MND), Space–time clustering},
	pages = {1121--1137},
}

@book{stoica_automating_2007,
	title = {Automating {Creation} of {Hierarchical} {Faceted} {Metadata} {Structures}},
	abstract = {We describe Castanet, an algorithm for auto- matically generating hierarchical faceted meta- data from textual descriptions of items, to be in- corporated into browsing and navigation inter- faces for large information collections. From an existing lexical database (such as WordNet), Castanet carves out a structure that reflects the contents of the target information collec- tion; moderate manual modifications improve the outcome. The algorithm is simple yet ef- fective: a study conducted with 34 information architects finds that Castanet achieves higher quality results than other automated category creation algorithms, and 85\% of the study par- ticipants said they would like to use the system for their work.},
	author = {Stoica, Emilia and Hearst, Marti and Richardson, Megan},
	month = jan,
	year = {2007},
	note = {Pages: 251},
}

@article{besag_detection_1991,
	title = {The {Detection} of {Clusters} in {Rare} {Diseases}},
	volume = {154},
	copyright = {© 1991 Royal Statistical Society},
	issn = {1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/2982708},
	doi = {10.2307/2982708},
	abstract = {Tests for clustering of rare diseases investigate whether an observed pattern of cases in one or more geographical regions could reasonably have arisen by chance alone, bearing in mind the variation in background population density. In contrast, tests for the detection of clusters are concerned with screening a large region for evidence of individual ‘hot spots’ of disease but without any preconception about their likely locations; the results of such tests may form the basis for subsequent small area investigations, statistical or non-statistical, but will rarely be an end in themselves. The main intention of the paper is to describe and illustrate a new technique for the identification of small clusters of disease. A secondary purpose is to discuss some common pitfalls in the application of tests of clustering to epidemiological data.},
	language = {en},
	number = {1},
	urldate = {2023-11-01},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Besag, Julian and Newell, James},
	year = {1991},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/2982708},
	keywords = {clustering, clusters, leukaemia, monte carlo methods, screening},
	pages = {143--155},
}

@article{kulldorff_spatial_1995,
	title = {Spatial disease clusters: {Detection} and inference},
	volume = {14},
	copyright = {Copyright © 1995 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	shorttitle = {Spatial disease clusters},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780140809},
	doi = {10.1002/sim.4780140809},
	abstract = {We present a new method of detection and inference for spatial clusters of a disease. To avoid ad hoc procedures to test for clustering, we have a clearly defined alternative hypothesis and our test statistic is based on the likelihood ratio. The proposed test can detect clusters of any size, located anywhere in the study region. It is not restricted to clusters that conform to predefined administrative or political borders. The test can be used for spatially aggregated data as well as when exact geographic co-ordinates are known for each individual. We illustrate the method on a data set describing the occurrence of leukaemia in Upstate New York.},
	language = {en},
	number = {8},
	urldate = {2023-11-01},
	journal = {Statistics in Medicine},
	author = {Kulldorff, Martin and Nagarwalla, Neville},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780140809},
	pages = {799--810},
}

@article{assuncao_fast_2006,
	title = {Fast detection of arbitrarily shaped disease clusters},
	volume = {25},
	copyright = {Copyright © 2006 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2411},
	doi = {10.1002/sim.2411},
	abstract = {Disease cluster detection and evaluation have commonly used spatial statistics methods that scan the map with a fixed circular window to locate candidate clusters. Recently, there has been interest in searching for clusters with arbitrary shape. The circular scan test retains high power of detecting a cluster, but does not necessarily identify the exact regions contained in a non-circular cluster particularly well. We propose, implement and evaluate a new procedure that is fast and produces clusters estimates of arbitrary shape in a rich class of possible cluster candidates. We showed that our methods contain the so-called upper level set method as a particular case. We present a power study of our method and, among other results, the main conclusion is that the likelihood-based arbitrarily shaped scan method is not appropriate to find a cluster estimate. When the parameter space includes the set of all possible spatial clusters in a map, a large and discrete parameter space, maximum likely cluster estimates tend to overestimate the true cluster by a large extent. This calls for a new approach different from the maximum likelihood method for this important public health problem. Copyright © 2006 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {5},
	urldate = {2023-11-01},
	journal = {Statistics in Medicine},
	author = {Assunção, R. and Costa, M. and Tavares, A. and Ferreira, S.},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2411},
	keywords = {disease clusters, scan statistics, spatial cluster, spatial statistics},
	pages = {723--742},
}

@article{ozonoff_cluster_2005,
	title = {Cluster detection methods applied to the {Upper} {Cape} {Cod} cancer data},
	volume = {4},
	issn = {1476-069X},
	url = {https://doi.org/10.1186/1476-069X-4-19},
	doi = {10.1186/1476-069X-4-19},
	abstract = {A variety of statistical methods have been suggested to assess the degree and/or the location of spatial clustering of disease cases. However, there is relatively little in the literature devoted to comparison and critique of different methods. Most of the available comparative studies rely on simulated data rather than real data sets.},
	number = {1},
	urldate = {2023-11-01},
	journal = {Environmental Health},
	author = {Ozonoff, Al and Webster, Thomas and Vieira, Veronica and Weinberg, Janice and Ozonoff, David and Aschengrau, Ann},
	month = sep,
	year = {2005},
	keywords = {Bernoulli Model, Breast Cancer Data, Cold Spot, Generalize Additive Model, Interpoint Distance},
	pages = {19},
}

@article{openshaw_developing_1995,
	title = {Developing {Automated} and {Smart} {Spatial} {Pattern} {Exploration} {Tools} for {Geographical} {Information} {Systems} {Applications}},
	volume = {44},
	copyright = {© 1995 Royal Statistical Society},
	issn = {1467-9884},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/2348611},
	doi = {10.2307/2348611},
	abstract = {The paper examines some of the problems that users of geographical information systems (GISs) face in attempting to perform spatial analysis. A case is made for the development of new types of smart exploratory analysis tools able to explore spatial data effectively while also coping with the problems associated with the data and the skill levels of the end-users. Some suggestions are made about how artificial intelligence methods borrowed from artificial life can be used to create spatial pattern hunting creatures that may provide the basis for more effective spatial analysis procedures for use with GISs.},
	language = {en},
	number = {1},
	urldate = {2023-11-01},
	journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
	author = {Openshaw, Stan},
	year = {1995},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/2348611},
	keywords = {Artificial life, Geographical information systems, Spatial analysis},
	pages = {3--16},
}

@article{marshall_review_1991,
	title = {A {Review} of {Methods} for the {Statistical} {Analysis} of {Spatial} {Patterns} of {Disease}},
	volume = {154},
	copyright = {© 1991 Royal Statistical Society},
	issn = {1467-985X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/2983152},
	doi = {10.2307/2983152},
	abstract = {A review of methods for the analysis of the geographical distribution of disease is presented. The topic is of increasing interest to statisticians, though much groundwork has been done by epidemiologists and medical geographers. Methods for the detection and testing for apparent clusters of disease, including those near a possible environmental hazard, are reviewed. Estimating regional mortality rates, possibly to construct disease maps, and methods for investigating the association between disease rates and social, demographic and environmental factors are discussed. In addition, statistical analysis of the spatial spread of epidemics is mentioned.},
	language = {en},
	number = {3},
	urldate = {2023-11-01},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Marshall, Roger J.},
	year = {1991},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/2983152},
	keywords = {bayesian mapping, disease clustering, disease mapping: disease rates, ecological analysis, empirical bayes estimation, environmental hazard, spatial autocorrelation, spatial dynamics},
	pages = {421--441},
}

@article{mackinnon_detecting_2007,
	title = {Detecting an {Association} between {Socioeconomic} {Status} and {Late} {Stage} {Breast} {Cancer} {Using} {Spatial} {Analysis} and {Area}-{Based} {Measures}},
	volume = {16},
	issn = {1055-9965},
	url = {https://doi.org/10.1158/1055-9965.EPI-06-0392},
	doi = {10.1158/1055-9965.EPI-06-0392},
	abstract = {Objectives: To assess the relationship between socioeconomic status (SES) and late stage breast cancer using the cluster detection software SaTScan and U.S. census–derived area-based socioeconomic measures.Materials and Methods: Florida's 18,683 women diagnosed with late stage breast cancer (regional or distant stage) between 1998 and 2002 as identified by Florida's population–based, statewide, incidence registry were analyzed by SaTScan to identify areas of higher-than-expected incidence. The relationship between SES and late stage breast cancer was assessed at the neighborhood (block group) level by combining the SaTScan results with area-based SES data.Results: SaTScan identified 767 of Florida's 9,112 block groups that had higher-than-expected incidence of late stage breast cancer. After controlling for patient level insurance status, county level mammography prevalence, and urban/rural residence in the logistic regression model, women living in neighborhoods of severe and near poverty were respectively 3.0 and 1.6 times more likely to live in areas of higher-than-expected incidence of late stage breast cancer when compared with women living in nonpoverty. Additionally, areas in the lowest quartile of mammography usage were almost seven times more likely to have higher-than-expected incidence than areas in the higher quartiles.Conclusions: In addition to confirming the importance of mammography, results from the present study suggest that “where” you live plays an important role in defining the risk of presenting with late stage breast cancer. Additional research is urgently needed to understand this risk and to leverage the strengths and resources present in all communities to lower the late stage breast cancer burden. (Cancer Epidemiol Biomarkers Prev 2007;16(4):756–62)},
	number = {4},
	urldate = {2023-11-01},
	journal = {Cancer Epidemiology, Biomarkers \& Prevention},
	author = {MacKinnon, Jill Amlong and Duncan, Robert C. and Huang, Youjie and Lee, David J. and Fleming, Lora E. and Voti, Lydia and Rudolph, Mark and Wilkinson, James D.},
	month = apr,
	year = {2007},
	pages = {756--762},
}

@article{gaudart_space-time_2006,
	title = {Space-time clustering of childhood malaria at the household level: a dynamic cohort in a {Mali} village},
	volume = {6},
	issn = {1471-2458},
	shorttitle = {Space-time clustering of childhood malaria at the household level},
	url = {https://doi.org/10.1186/1471-2458-6-286},
	doi = {10.1186/1471-2458-6-286},
	abstract = {Spatial and temporal heterogeneities in the risk of malaria have led the WHO to recommend fine-scale stratification of the epidemiological situation, making it possible to set up actions and clinical or basic researches targeting high-risk zones. Before initiating such studies it is necessary to define local patterns of malaria transmission and infection (in time and in space) in order to facilitate selection of the appropriate study population and the intervention allocation. The aim of this study was to identify, spatially and temporally, high-risk zones of malaria, at the household level (resolution of 1 to 3 m).},
	number = {1},
	urldate = {2023-11-01},
	journal = {BMC Public Health},
	author = {Gaudart, Jean and Poudiougou, Belco and Dicko, Alassane and Ranque, Stéphane and Toure, Ousmane and Sagara, Issaka and Diallo, Mouctar and Diawara, Sory and Ouattara, Amed and Diakite, Mahamadou and Doumbo, Ogobara K.},
	month = nov,
	year = {2006},
	keywords = {Falciparum Infection, Geographical Information System, Malaria, Malaria Transmission, Malaria Vaccine},
	pages = {286},
}

@article{chowell_19181919_2007,
	title = {The 1918–1919 influenza pandemic in {England} and {Wales}: spatial patterns in transmissibility and mortality impact},
	volume = {275},
	shorttitle = {The 1918–1919 influenza pandemic in {England} and {Wales}},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspb.2007.1477},
	doi = {10.1098/rspb.2007.1477},
	abstract = {Spatial variations in disease patterns of the 1918–1919 influenza pandemic remain poorly studied. We explored the association between influenza death rates, transmissibility and several geographical and demographic indicators for the autumn and winter waves of the 1918–1919 pandemic in cities, towns and rural areas of England and Wales. Average measures of transmissibility, estimated by the reproduction number, ranged between 1.3 and 1.9, depending on model assumptions and pandemic wave and showed little spatial variation. Death rates varied markedly with urbanization, with 30–40\% higher rates in cities and towns compared with rural areas. In addition, death rates varied with population size across rural settings, where low population areas fared worse. By contrast, we found no association between transmissibility, death rates and indicators of population density and residential crowding. Further studies of the geographical mortality patterns associated with the 1918–1919 influenza pandemic may be useful for pandemic planning.},
	number = {1634},
	urldate = {2023-11-01},
	journal = {Proceedings of the Royal Society B: Biological Sciences},
	author = {Chowell, Gerardo and Bettencourt, Luís M.A and Johnson, Niall and Alonso, Wladimir J and Viboud, Cécile},
	month = dec,
	year = {2007},
	note = {Publisher: Royal Society},
	keywords = {England and Wales, Spanish flu pandemic, demographics, influenza, reproduction number, scaling laws},
	pages = {501--509},
}

@article{briggs_geographic_1995,
	title = {Geographic variation in primary care visits in {Iowa}.},
	volume = {30},
	issn = {0017-9124},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1070083/},
	abstract = {OBJECTIVE. This study investigates the determinants of primary care office visit rates. DATA SOURCES. Blue Cross and Blue Shield of Iowa subscriber information was sorted by residence into geographic health service areas. Cost-sharing information was also obtained from Blue Cross. Physician supply data were obtained from The University of Iowa, Office of Community-Based Programs. Hospital data were reported by the Iowa Hospital Association. STUDY DESIGN. Cases were classified into ambulatory care groups (ACGs). Use rates were computed for each group in each service area. Ordinary least squares regression models were developed to model geographic variation in each ACG-specific primary care visit rate. PRINCIPAL FINDINGS. Regression models were not significant for five out of eleven ACGs studied. Out-of-pocket expense significantly affected utilization in three out of six. The number of primary care practices per capita had a significant effect on utilization in two ACGs. The supply of hospital outpatient services was significant in one ACG. CONCLUSIONS. Study findings reveal that some ACGs are price-sensitive and some are not. Policies aimed at changing levels of primary care use should taken into account whether varying cost-sharing will influence consumer behavior in the desired direction.},
	number = {5},
	urldate = {2023-11-01},
	journal = {Health Services Research},
	author = {Briggs, L W and Rohrer, J E and Ludke, R L and Hilsenrath, P E and Phillips, K T},
	month = dec,
	year = {1995},
	pmid = {8537225},
	pmcid = {PMC1070083},
	pages = {657--671},
}

@phdthesis{bhat_toxics_2007,
	address = {School of Public Health},
	title = {Toxics {Release} {Inventory} facilities and childhood cancer: {Geographic} information systems based approach},
	shorttitle = {Toxics {Release} {Inventory} facilities and childhood cancer},
	school = {The University of Texas},
	author = {Bhat, Samrat},
	year = {2007},
}

@article{cagiltay_teaching_2007,
	title = {Teaching software engineering by means of computer-game development: {Challenges} and opportunities},
	volume = {38},
	issn = {1467-8535},
	shorttitle = {Teaching software engineering by means of computer-game development},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8535.2007.00705.x},
	doi = {10.1111/j.1467-8535.2007.00705.x},
	abstract = {Software-engineering education programs are intended to prepare students for a field that involves rapidly changing conditions and expectations. Thus, there is always a danger that the skills and the knowledge provided may soon become obsolete. This paper describes results and draws on experiences from the implementation of a computer game-development course whose design addresses problems in software-engineering education by improving students' abilities in four areas: (1) problem solving; (2) the application of previously learned knowledge; (3) the use of independent learning; and (4) learning by doing. In order to better understand this course's effect on students' performance in a software-development project, I investigated 125 students' performance in a 1-year senior-project course. Results of this study show that the students who had taken the computer game-development course became more successful in the senior-project course than the students who had not taken it.},
	language = {en},
	number = {3},
	urldate = {2023-11-01},
	journal = {British Journal of Educational Technology},
	author = {Cagiltay, Nergiz Ercil},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8535.2007.00705.x},
	pages = {405--415},
}

@article{fuldain_gonzalez_ndvi_2019,
	title = {{NDVI} {Identification} and {Survey} of a {Roman} {Road} in the {Northern} {Spanish} {Province} of Álava},
	volume = {11},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/11/6/725},
	doi = {10.3390/rs11060725},
	abstract = {The Iter 34 (Antonine Itinerary XXXIV) is the name of the Roman road that crosses the province of Álava from west to east. Since no specific path was officially recognized before our study, the remains of the road did not benefit from heritage protection. In 2017, we made a project to determine the course of the road through rural Álava. In addition to traditional archaeological excavation and prospecting techniques, we used UAVs (unmanned aerial vehicle) to produce NDVI (normalized difference vegetation index) orthomosaic plans of ten cultivated areas through which the road is conjectured to pass. NDVI orthomosaics let us see crop marks better than with conventional photography, allowing us to detect the crop marks during times of the year and in places where conventional photography would fail to show them. Thanks to the NDVI orthomosaics, remains of the road were identified not only in places where we knew it existed, but also in previously unknown locations. Furthermore, other archaeological features were identified close to the roadway. This technique heralds a great advance in non-invasive methods of archaeological surveying. By using precision farming techniques we have identified the course of the Roman road Iter 34 in several locations in a short period of time and with few resources.},
	language = {en},
	number = {6},
	urldate = {2023-10-25},
	journal = {Remote Sensing},
	author = {Fuldain González, Juan and Varón Hernández, Félix},
	month = mar,
	year = {2019},
	pages = {725},
}

@article{yi_dust_nodate,
	title = {Dust {Magnet}: multivariate information visualization using a magnet metaphor},
	volume = {4},
	issn = {1473-8716},
	shorttitle = {Dust {Magnet}},
	doi = {10.1057/palgrave.ivs.9500099},
	number = {4},
	journal = {Information Visualization},
	author = {Yi, Ji},
	pages = {239--256},
}

@article{turton_high-performance_1998,
	title = {High-performance computing and geography: developments, issues, and case studies},
	volume = {30},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1068/a301839},
	doi = {10.1068/a301839},
	abstract = {In this paper we outline some of the results that were obtained by the application of a Cray T3D parallel supercomputer to human geography problems. We emphasise the fundamental importance of high-performance computing (HPC) as a future relevant paradigm for doing geography. We offer an introduction to recent developments and illustrate how new computational intelligence technologies can start to be used to make use of opportunities created by data riches from geographic information systems, artificial intelligence tools, and HPC in geography.},
	number = {10},
	journal = {Environment and Planning A},
	author = {Turton, I. and Openshaw, S.},
	year = {1998},
	note = {Publisher: Pion Ltd
tex.citeulike-article-id: 6874083
tex.citeulike-linkout-0: http://dx.doi.org/10.1068/a301839
tex.citeulike-linkout-1: http://www.envplan.com/abstract.cgi?id=a301839
tex.posted-at: 2010-03-18 19:06:41
tex.priority: 0},
	keywords = {hpc, parallel},
	pages = {1839--1856},
}

@article{rodrigues_spatial-perceptual_2007,
	title = {The {Spatial}-{Perceptual} {Design} {Space}: {A} {New} {Comprehension} for {Data} {Visualization}},
	volume = {6},
	issn = {1473-8716},
	shorttitle = {The {Spatial}-{Perceptual} {Design} {Space}},
	url = {https://doi.org/10.1057/palgrave.ivs.9500161},
	doi = {10.1057/palgrave.ivs.9500161},
	abstract = {We revisit the design space of visualizations aiming at identifying and relating its components. In this sense, we establish a model to examine the process through which visualizations become expressive for users. This model has lead us to a taxonomy oriented to the human visual perception. The essence of this taxonomy provides natural criteria in order to delineate a novel understanding for the design space of visualizations. From such understanding, we elaborate a model for generalized design. The model poses an intuitive comprehension for the visualization design space departing from fundamental pre-attentive stimuli and from perceptual phenomena. The paper is presented as a survey, its structure introduces an alternative conceptual organization for the space of techniques concerning visual analysis.},
	language = {en},
	number = {4},
	urldate = {2023-11-01},
	journal = {Information Visualization},
	author = {Rodrigues, José Fernando and Traina, Agma JM and de Oliveira, Maria Cristina F. and Traina, Caetano},
	month = dec,
	year = {2007},
	note = {Publisher: SAGE Publications},
	pages = {261--279},
}

@article{kumasaka_high-dimensional_2008,
	title = {High-dimensional data visualisation: {The} textile plot},
	volume = {52},
	issn = {0167-9473},
	shorttitle = {High-dimensional data visualisation},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947307004513},
	doi = {10.1016/j.csda.2007.11.016},
	abstract = {The textile plot is a parallel coordinate plot in which the ordering, locations and scales of the axes are simultaneously chosen so that the connecting lines, each of which represents a case, are aligned as horizontally as possible. Plots of this type can accommodate numerical data as well as ordered or unordered categorical data, or a mixture of these different data types. Knots and parallel wefts are features of the textile plot which greatly aid the interpretation of the data. Several practical examples are presented which illustrate the potential usefulness of the textile plot as an aid to the interpretation of multivariate data.},
	number = {7},
	urldate = {2023-11-01},
	journal = {Computational Statistics \& Data Analysis},
	author = {Kumasaka, Natsuhiko and Shibata, Ritei},
	month = mar,
	year = {2008},
	pages = {3616--3644},
}

@article{guo_visualization_2006,
	title = {A {Visualization} {System} for {Space}-{Time} and {Multivariate} {Patterns} ({VIS}-{STAMP})},
	volume = {12},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/document/1703367},
	doi = {10.1109/TVCG.2006.84},
	abstract = {The research reported here integrates computational, visual and cartographic methods to develop a geovisual analytic approach for exploring and understanding spatio-temporal and multivariate patterns. The developed methodology and tools can help analysts investigate complex patterns across multivariate, spatial and temporal dimensions via clustering, sorting and visualization. Specifically, the approach involves a self-organizing map, a parallel coordinate plot, several forms of reorderable matrices (including several ordering methods), a geographic small multiple display and a 2-dimensional cartographic color design method. The coupling among these methods leverages their independent strengths and facilitates a visual exploration of patterns that are difficult to discover otherwise. The visualization system we developed supports overview of complex patterns and through a variety of interactions, enables users to focus on specific patterns and examine detailed views. We demonstrate the system with an application to the IEEE InfoVis 2005 contest data set, which contains time-varying, geographically referenced and multivariate data for technology companies in the US},
	number = {6},
	urldate = {2023-11-01},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Guo, Diansheng and Chen, Jin and MacEachren, A.M. and Liao, Ke},
	month = nov,
	year = {2006},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {1461--1474},
}

@inproceedings{hassan-montero_improving_2006,
	title = {Improving {Tag}-{Clouds} as {Visual} {Information} {Retrieval} {Interfaces}},
	author = {Hassan-Montero},
	month = oct,
	year = {2006},
}

@article{heer_software_2006,
	title = {Software {Design} {Patterns} for {Information} {Visualization}},
	volume = {12},
	issn = {1941-0506},
	url = {https://ieeexplore.ieee.org/abstract/document/4015439},
	doi = {10.1109/TVCG.2006.178},
	abstract = {Despite a diversity of software architectures supporting information visualization, it is often difficult to identify, evaluate, and re-apply the design solutions implemented within such frameworks. One popular and effective approach for addressing such difficulties is to capture successful solutions in design patterns, abstract descriptions of interacting software components that can be customized to solve design problems within a particular context. Based upon a review of existing frameworks and our own experiences building visualization software, we present a series of design patterns for the domain of information visualization. We discuss the structure, context of use, and interrelations of patterns spanning data representation, graphics, and interaction. By representing design knowledge in a reusable form, these patterns can be used to facilitate software design, implementation, and evaluation, and improve developer education and communication},
	number = {5},
	urldate = {2023-11-01},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Heer, Jeffrey and Agrawala, Maneesh},
	month = sep,
	year = {2006},
	note = {Conference Name: IEEE Transactions on Visualization and Computer Graphics},
	pages = {853--860},
}

@article{liu_knowledge-based_2007,
	title = {Knowledge-based query expansion to support scenario-specific retrieval of medical free text},
	volume = {10},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/10.1007/s10791-006-9020-6},
	doi = {10.1007/s10791-006-9020-6},
	language = {en},
	number = {2},
	urldate = {2023-11-01},
	journal = {Information Retrieval},
	author = {Liu, Zhenyu and Chu, Wesley W.},
	month = feb,
	year = {2007},
	pages = {173--202},
}

@article{tukiainen_cloudnetpy_2020,
	title = {{CloudnetPy}: {A} {Python} package for processing cloud remote sensing data},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {{CloudnetPy}},
	url = {https://joss.theoj.org/papers/10.21105/joss.02123},
	doi = {10.21105/joss.02123},
	number = {53},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Tukiainen, Simo and O’Connor, Ewan and Korpinen, Anniina},
	month = sep,
	year = {2020},
	pages = {2123},
}

@article{wu_lidar_2021,
	title = {lidar: {A} {Python} package for delineating nested surface depressions from digital elevation data},
	volume = {6},
	issn = {2475-9066},
	shorttitle = {lidar},
	url = {https://joss.theoj.org/papers/10.21105/joss.02965},
	doi = {10.21105/joss.02965},
	number = {59},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Wu, Qiusheng},
	month = mar,
	year = {2021},
	pages = {2965},
}

@article{riedel_utopia_2020,
	title = {Utopia: {A} {Comprehensive} and {Collaborative} {ModelingFramework} for {Complex} and {Evolving} {Systems}},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {Utopia},
	url = {https://joss.theoj.org/papers/10.21105/joss.02165},
	doi = {10.21105/joss.02165},
	number = {53},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Riedel, Lukas and Herdeanu, Benjamin and Mack, Harald and Sevinchan, Yunus and Weninger, Julian},
	month = sep,
	year = {2020},
	pages = {2165},
}

@article{meyer_epiworldr_2023,
	title = {{epiworldR}: {Fast} {Agent}-{Based} {Epi} {Models}},
	volume = {8},
	issn = {2475-9066},
	shorttitle = {{epiworldR}},
	url = {https://joss.theoj.org/papers/10.21105/joss.05781},
	doi = {10.21105/joss.05781},
	number = {90},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Meyer, Derek and Yon, George G Vega},
	month = oct,
	year = {2023},
	pages = {5781},
}

@article{rusu_abmarl_2021,
	title = {Abmarl: {Connecting} {Agent}-{Based} {Simulations} with {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {6},
	issn = {2475-9066},
	shorttitle = {Abmarl},
	url = {https://joss.theoj.org/papers/10.21105/joss.03424},
	doi = {10.21105/joss.03424},
	number = {64},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Rusu, Edward and Glatt, Ruben},
	month = aug,
	year = {2021},
	pages = {3424},
}

@article{benedetti_black-it_2022,
	title = {Black-it: {A} {Ready}-to-{Use} and {Easy}-to-{Extend} {CalibrationKit} for {Agent}-based {Models}},
	volume = {7},
	issn = {2475-9066},
	shorttitle = {Black-it},
	url = {https://joss.theoj.org/papers/10.21105/joss.04622},
	doi = {10.21105/joss.04622},
	number = {79},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Benedetti, Marco and Catapano, Gennaro and Sclavis, Francesco De and Favorito, Marco and Glielmo, Aldo and Magnanimi, Davide and Muci, Antonio},
	month = nov,
	year = {2022},
	pages = {4622},
}

@article{foramitti_agentpy_2021,
	title = {{AgentPy}: {A} package for agent-based modeling in {Python}},
	volume = {6},
	issn = {2475-9066},
	shorttitle = {{AgentPy}},
	url = {https://joss.theoj.org/papers/10.21105/joss.03065},
	doi = {10.21105/joss.03065},
	number = {62},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Foramitti, Joël},
	month = jun,
	year = {2021},
	pages = {3065},
}

@article{thelen_villager_2022,
	title = {villager: {A} framework for designing and executingagent-based models in {R}},
	volume = {7},
	issn = {2475-9066},
	shorttitle = {villager},
	url = {https://joss.theoj.org/papers/10.21105/joss.04562},
	doi = {10.21105/joss.04562},
	number = {79},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Thelen, Thomas and Thomson, Marcus and Aldana, Gerardo and Gonzalez, Toni},
	month = nov,
	year = {2022},
	pages = {4562},
}

@article{yu_melodie_2023,
	title = {Melodie: {Agent}-based {Modeling} in {Python}},
	volume = {8},
	issn = {2475-9066},
	shorttitle = {Melodie},
	url = {https://joss.theoj.org/papers/10.21105/joss.05100},
	doi = {10.21105/joss.05100},
	number = {83},
	urldate = {2023-10-31},
	journal = {Journal of Open Source Software},
	author = {Yu, Songmin and Hou, Zhanyi},
	month = mar,
	year = {2023},
	pages = {5100},
}

@article{kingston_web-based_2000,
	title = {Web-based public participation geographical information systems: an aid to local environmental decision-making},
	volume = {24},
	copyright = {All rights reserved},
	issn = {01989715},
	shorttitle = {Web-based public participation geographical information systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0198971599000496},
	doi = {10.1016/S0198-9715(99)00049-6},
	language = {en},
	number = {2},
	urldate = {2023-10-27},
	journal = {Computers, Environment and Urban Systems},
	author = {Kingston, R. and Carver, S. and Evans, A. and Turton, I.},
	month = mar,
	year = {2000},
	pages = {109--125},
}

@article{ballas_building_2007,
	title = {Building a {Spatial} {Microsimulation}-{Based} {Planning} {Support} {System} for {Local} {Policy} {Making}},
	volume = {39},
	issn = {0308-518X, 1472-3409},
	url = {http://journals.sagepub.com/doi/10.1068/a38441},
	doi = {10.1068/a38441},
	abstract = {This paper presents a spatial microsimulation modelling and predictive policy analysis system called Micro-MaPPAS, a Planning Support System (PSS) constructed for a local strategic partnership in a large metropolitan area of the UK. The innovative feature of this system is the use of spatial microsimulation techniques for the enhancement of local policy decision making in connection with the neighbourhood renewal strategy. The paper addresses the relevant data issues and technical aspects of the linkage of spatial microsimulation modelling frameworks to PSS and deals with the wider implications that such a linkage may have to local policy and planning procedures. Finally, the paper presents some illustrative examples of the policy relevance and policy analysis potential of the software.},
	language = {en},
	number = {10},
	urldate = {2023-10-27},
	journal = {Environment and Planning A: Economy and Space},
	author = {Ballas, Dimitris and Kingston, Richard and Stillwell, John and Jin, Jianhui},
	month = oct,
	year = {2007},
	pages = {2482--2499},
}

@article{citeulike:2688869,
	title = {Public participation, {GIS}, and cyberdemocracy: evaluating on-line spatial decision support systems},
	volume = {28},
	copyright = {All rights reserved},
	url = {http://www.envplan.com/abstract.cgi?id=b2751t},
	doi = {https://doi.org/10.1068/b2751t},
	abstract = {In this paper we describe the development of Internet-based approaches to public participation and on-line spatial decision support systems in particular. Two case studies in developing web-based public participation GIS (PPGIS), one local and one regional, are described in detail. Results from the live testing of these systems are shown. These are discussed in the light of recent developments in 'cyberdemocracy' and conclusions are drawn about principles of on-line PPGIS and problems associated with public participation, user interaction, and familiarity with IT, copyright issues, access to the Internet, and relevant political structures.},
	number = {6},
	journal = {Environment and Planning B: Planning and Design},
	author = {Carver, Steve and Evans, Andrew and Kingston, Richard and Turton, Ian},
	year = {2001},
	note = {tex.citeulike-article-id: 2688869
tex.citeulike-linkout-0: http://www.envplan.com/abstract.cgi?id=b2751t
tex.posted-at: 2008-04-18 19:03:31
tex.priority: 0},
	keywords = {decision, gis, internet, ppgis, spatial},
	pages = {907--921},
}

@book{imhof_cartographic_2007,
	address = {Redlands, UNITED STATES},
	title = {Cartographic {Relief} {Presentation}},
	isbn = {978-1-58948-327-9},
	url = {http://ebookcentral.proquest.com/lib/gla/detail.action?docID=3238273},
	urldate = {2023-10-26},
	publisher = {Esri Press},
	author = {Imhof, Eduard},
	year = {2007},
	keywords = {Topographic maps.},
}

@misc{noauthor_chapter_nodate,
	title = {“{Chapter} 5 - {Map} {Symboles}, {Visual} {Variables}, {Color}” in “{Introduction} to {Cartography}” on {OpenALG}},
	url = {https://alg.manifoldapp.org/read/introduction-to-cartography/section/21773ec3-7eb3-4197-b46d-8a7c2ebaa25f},
	abstract = {Start reading this text on OpenALG},
	language = {en-US},
	urldate = {2023-10-26},
	journal = {OpenALG},
}

@article{white_symbolization_2017,
	title = {Symbolization and the {Visual} {Variables}},
	volume = {2017},
	url = {http://gistbok.ucgis.org/bok-topics/symbolization-and-visual-variables},
	doi = {10.22224/gistbok/2017.2.3},
	number = {Q2},
	urldate = {2023-10-26},
	journal = {Geographic Information Science \& Technology Body of Knowledge},
	author = {White, Travis},
	month = jun,
	year = {2017},
}

@techreport{esri_maplexautomatic_1998,
	title = {Maplex—{Automatic} {Cartographic} {Name} {Placement} {Software}},
	url = {https://web.archive.org/web/20030526154847/http://www.esri.com/library/whitepapers/pdfs/maplexwp.pdf},
	urldate = {2023-10-26},
	institution = {ESRI},
	author = {ESRI},
	month = aug,
	year = {1998},
}

@book{swiss_society_of_cartography_cartographic_1977,
	series = {Cartographic {Publication} {Series}},
	title = {Cartographic {Generalization}: {Topographic} {Maps}},
	number = {2},
	publisher = {Swiss Society of Cartography},
	author = {Swiss Society of Cartography},
	year = {1977},
}

@inproceedings{hurni_cartographic_2010,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Earth} {Sciences}},
	title = {Cartographic {Relief} {Presentation} {Revisited} – {Forty} {Years} after {Eduard} {Imhof}},
	isbn = {978-3-540-75761-0},
	doi = {10.1007/978-3-540-75761-0_1},
	abstract = {Topographic maps represent in a symbolised way the main features of the Earth’s surface shape as well as the major, mostly visible objects covering the topography. The clear and readable depiction of the relief is one of the main challenges in topographic cartography. The Swiss cartographer Eduard Imhof had a significant influence on the development of modern topographic cartography. He published his trend-setting findings 40 years ago in his textbook “Cartographic Relief Presentation”. This overview paper reviews his major contributions to topographic cartography and presents today’s state of cartographic relief depiction, illustrated by current map examples and projects elaborated at Imhof’s Alma Mater, the Institute of Cartography of ETH Zurich.},
	language = {en},
	booktitle = {Landform - {Structure}, {Evolution}, {Process} {Control}},
	publisher = {Springer},
	author = {Hurni, Lorenz},
	editor = {Otto, Jan-Christoph and Dikau, Richard},
	year = {2010},
	keywords = {Analytical shading, Cartography, Contour lines, Eduard Imhof, Hill shading, Relief presentation, Rock drawing, Swiss-style colour relief shading, Topographic maps},
	pages = {1--20},
}

@article{kadmon_automated_1972,
	title = {Automated {Selection} of {Settlements} in {Map} {Generalisation}},
	volume = {9},
	issn = {0008-7041},
	url = {https://www.tandfonline.com/doi/abs/10.1179/000870472787352442},
	doi = {10.1179/000870472787352442},
	abstract = {In the field of automated cartography the cartographic data bank is occupying an increasingly more prominent position. Within such data banks settlements occupy an important position and this paper discusses ways in which the automated selection of settlements may be achieved according to a number of varying criteria, each of which may or may not be included in the selection program.},
	number = {2},
	urldate = {2023-10-26},
	journal = {The Cartographic Journal},
	author = {Kadmon, Naflali},
	month = dec,
	year = {1972},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1179/000870472787352442},
	pages = {93--98},
}

@article{davies_cold_2023,
	title = {Cold war satellite images reveal hundreds of unknown {Roman} forts},
	url = {https://amp.theguardian.com/science/2023/oct/26/cold-war-satellite-images-hundreds-unknown-roman-forts},
	urldate = {2023-10-26},
	journal = {The Guardian},
	author = {Davies, Caroline},
	month = oct,
	year = {2023},
}

@article{yoeli_logic_1972,
	title = {The {Logic} of {Automated} {Map} {Lettering}},
	volume = {9},
	issn = {0008-7041},
	url = {https://www.tandfonline.com/doi/abs/10.1179/000870472787352505},
	doi = {10.1179/000870472787352505},
	abstract = {The logic behind the placement of names on maps is investigated as part of a study, the object of which is the automation of map lettering. After describing the basic principles the author illustrates his solution.},
	number = {2},
	urldate = {2023-10-26},
	journal = {The Cartographic Journal},
	author = {Yoeli, Pinhas},
	month = dec,
	year = {1972},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1179/000870472787352505},
	pages = {99--108},
}

@article{jones_conflict_1990,
	title = {Conflict resolution in cartographic name placement},
	volume = {22},
	issn = {0010-4485},
	url = {https://www.sciencedirect.com/science/article/pii/001044859090076O},
	doi = {10.1016/0010-4485(90)90076-O},
	abstract = {The increasing application of computer cartography in geographical information systems and in professional map production has highlighted the requirement for automatic positioning of cartographic text in a manner that ensures legibility. An important aspect of the implementation of automated name placement is the avoidance of conflict, in particular due to overlap between names, and between names and map features. This paper reviews the automated strategies that have been adopted for conflict resolution. The majority of these may be classified as either iterative or graph-search procedures. In general the problem may be seen as one in which initially trial positions are generated for each of a selected set of names. These trial positions should obey rules relating to text characteristics and to the relationships between names and other map features. Conflict resolution then consists in finding, for each name, a trial position that obeys the cartographic rules governing the relationships between names.},
	number = {3},
	urldate = {2023-10-26},
	journal = {Computer-Aided Design},
	author = {Jones, Christopher B.},
	month = apr,
	year = {1990},
	keywords = {conflict resolution, digital cartography, name placement},
	pages = {173--183},
}

@article{hirsch_algorithm_1982,
	title = {An {Algorithm} for {Automatic} {Name} {Placement} {Around} {Point} {Data}},
	volume = {9},
	issn = {0094-1689},
	url = {https://doi.org/10.1559/152304082783948367},
	doi = {10.1559/152304082783948367},
	abstract = {An algorithm for automatic name placement around point data requires both the examination of the cartographic name placement process and the utilization of spatial search techniques. The name placement process consists of selection, layout, and placement phases. Spatial search techniques are utilized to simulate the difficult layout phase. These techniques have resulted in the development of two movement methodologies, each intended to simulate a separate part of the name placement process. The automatic placement of names around point data is a feasible concept for use in map making.},
	number = {1},
	urldate = {2023-10-26},
	journal = {The American Cartographer},
	author = {Hirsch, Stephen A.},
	month = jan,
	year = {1982},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1559/152304082783948367},
	pages = {5--17},
}

@article{kern_automation_2008,
	title = {Automation and the {Map} {Label} {Placement} {Problem}: {A} {Comparison} of {Two} {GIS} {Implementations} of {Label} {Placement}},
	copyright = {Copyright (c)},
	issn = {1048-9053},
	shorttitle = {Automation and the {Map} {Label} {Placement} {Problem}},
	url = {https://cartographicperspectives.org/index.php/journal/article/view/cp60-kern-brewer},
	doi = {10.14714/CP60.230},
	abstract = {The placement of feature name labels on maps has challenged mapmakers throughout history. Before the development of mapping software, placing labels in manual map production could consume up to half or more of overall map production time. This paper explores the extent to which current GIS software can place labels legibly, without overlap, and with good visual association between features and labels. This evaluation takes place in the context of a densely featured municipal sewer utility map book. The primary research objective is to evaluate the ability of current GIS software to automate label placement; the research also identifies factors that make manual refinement of automated label placement necessary in order to complete the labeling process. The research compares map-labeling tools from ESRI TM ’s ArcMap TM 9.2: the Standard Labeling Engine and the Maplex TM labeling extension. Label placement success is assessed by both quantity and quality metrics, using a methodology developed and tailored specifically for evaluation of sewer map label placement. The research found that Maplex placed almost seven percent more labels overall than the Standard Labeling Engine. For the labels they did place, both products provided equally good quality label placement: About 93 percent of labels were placed with no overlap, and virtually 100 percent of labels were placed in their preferred position. After conversion to annotation, manual label position refinement eliminated all overlaps but at the cost of a nine percent decline in the preferred position metric.},
	language = {en},
	number = {60},
	urldate = {2023-10-26},
	journal = {Cartographic Perspectives},
	author = {Kern, Jill Phelps and Brewer, Cynthia A.},
	month = jun,
	year = {2008},
	note = {Number: 60},
	keywords = {GIS mapping},
	pages = {22--45},
}

@article{imhof_positioning_1975,
	title = {Positioning {Names} on {Maps}},
	volume = {2},
	abstract = {Prof. Dr. Eduard Imhof, dean of European cartographers, has been an astute stu- dent of the esthetic-scientific characteristics of the cartographic method. In this paper pub- lished 13 years ago, he draws upon his long experience in map design and production to formulate a series of precepts about positioning or locating the lettering on maps in relation to the various functional aspects of the map and the individual named features. Noting that legibility and clarity of the map depend on good name positioning—each name having only one optimum position on the map—he encourages the use of a graphic draft of lettering to determine this position.},
	number = {2},
	journal = {The American Cartographer},
	author = {Imhof, Eduard},
	year = {1975},
	pages = {128--144},
}

@article{wood_descriptive_2000,
	title = {A {Descriptive} and {Illustrated} {Guide} for {Type} {Placement} on {Small} {Scale} {Maps}},
	volume = {37},
	issn = {0008-7041},
	url = {https://doi.org/10.1179/caj.2000.37.1.5},
	doi = {10.1179/caj.2000.37.1.5},
	abstract = {One of the essential ingredients in any map is the lettering. Just as point, line and area symbols are designed to depict various geographic features, lettering, also referred to as labels, type, text, etc., is used as symbols to make various features. Selecting an appropriate style and size for map lettering (Shortridge, 1979) is equally important to efficient cartographic communication as the placement of map lettering. This paper provides an extensive, descriptive rationale and a graphic solution for the many difficult problems associated with map lettering placement that are not treated specifically in Imhof's (1975) landmark article.},
	number = {1},
	urldate = {2023-10-26},
	journal = {The Cartographic Journal},
	author = {Wood, Clifford H.},
	month = jun,
	year = {2000},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1179/caj.2000.37.1.5},
	pages = {5--18},
}

@incollection{taylor_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 17 - {Perspectives} on {Visualization} and {Modern} {Cartography}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500247},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Taylor, D. R. FRASER},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50024-7},
	pages = {333--341},
}

@incollection{van_der_wel_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 16 - {Visualization} of {Data} {Quality}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500235},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Van der wel, FRANS J. M. and Hootsmans, ROB M. and Ormeling, FERJAN},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50023-5},
	pages = {313--331},
}

@incollection{dibiase_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 15 - {Multivariate} {Display} of {Geographic} {Data}: {Applications} in {Earth} {System} {Science}},
	volume = {2},
	shorttitle = {Chapter 15 - {Multivariate} {Display} of {Geographic} {Data}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500223},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Dibiase, DAVID and Reeves, CATHERINE and Maceachren, ALAN M. and Wyss, MARTIN VON and Krygier, JOHN B. and Sloan, JAMES L. and Detweiler, MARK C.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50022-3},
	pages = {287--312},
}

@incollection{kraak_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 14 - {Interactive} {Modelling} {Environment} for {Three}-dimensional {Maps}: {Functionality} and {Interface} {Issues}},
	volume = {2},
	shorttitle = {Chapter 14 - {Interactive} {Modelling} {Environment} for {Three}-dimensional {Maps}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500211},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Kraak, MENNO-JAN},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50021-1},
	pages = {269--285},
}

@incollection{koussoulakou_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 13 - {Spatial}–{Temporal} {Analysis} of {Urban} {Air} {Pollution}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B978008042415650020X},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Koussoulakou, ALEXANDRA},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50020-X},
	pages = {243--267},
}

@incollection{asche_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 12 - {Designing} {Interactive} {Maps} for {Planning} and {Education}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500193},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Asche, HARTMUT and Herrmann, CHRISTIAN M.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50019-3},
	pages = {215--242},
}

@incollection{monmonier_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 11 - {Graphic} {Narratives} for {Analyzing} {Environmental} {Risks}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500181},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Monmonier, MARK},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50018-1},
	pages = {201--213},
}

@incollection{mcguinness_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 10 - {Expert}/{Novice} {Use} of {Visualization} {Tools}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B978008042415650017X},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Mcguinness, CAROL},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50017-X},
	pages = {185--199},
}

@incollection{lindholm_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 9 - {Designing} a {Visualization} {User} {Interface}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500168},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Lindholm, MIKKO and Sarjakoski, TAPANI},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50016-8},
	pages = {167--184},
}

@incollection{krygier_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 8 - {Sound} and {Geographic} {Visualization}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500156},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Krygier, JOHN B.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50015-6},
	pages = {149--166},
}

@incollection{slocum_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 6 - {Visualization} {Software} {Tools}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500132},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Slocum, TERRY A.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50013-2},
	pages = {91--122},
}

@incollection{cartwright_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 5 - {Interactive} {Multimedia} for {Mapping}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500120},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Cartwright, WILLIAM},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50012-0},
	pages = {63--89},
}

@incollection{artimo_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 4 - {The} {Bridge} {Between} {Cartographic} and {Geographic} {Information} {Systems}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500119},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Artimo, KIRSI},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50011-9},
	pages = {45--61},
}

@incollection{peterson_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 3 - {Cognitive} {Issues} in {Cartographic} {Visualization}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500107},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Peterson, MICHAEL P.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50010-7},
	pages = {27--43},
}

@incollection{wood_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 2 - {Visualization} in {Historical} {Context}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500090},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Wood, MICHAEL},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50009-0},
	pages = {13--26},
}

@incollection{maceachren_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 1 - {Visualization} in {Modern} {Cartography}: {Setting} the {Agenda}},
	volume = {2},
	shorttitle = {Chapter 1 - {Visualization} in {Modern} {Cartography}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500089},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Maceachren, ALAN M.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50008-9},
	pages = {1--12},
}

@incollection{brewer_chapter_1994,
	series = {Visualization in {Modern} {Cartography}},
	title = {Chapter 7 - {Color} {Use} {Guidelines} for {Mapping} and {Visualization}},
	volume = {2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080424156500144},
	urldate = {2023-10-26},
	booktitle = {Modern {Cartography} {Series}},
	publisher = {Academic Press},
	author = {Brewer, CYNTHIA A.},
	editor = {Maceachren, ALAN M. and Taylor, D. R. FRASER},
	month = jan,
	year = {1994},
	doi = {10.1016/B978-0-08-042415-6.50014-4},
	pages = {123--147},
}

@article{schnur_measured_2018,
	title = {Measured and perceived visual complexity: a comparative study among three online map providers},
	volume = {45},
	issn = {1523-0406, 1545-0465},
	shorttitle = {Measured and perceived visual complexity},
	url = {https://www.tandfonline.com/doi/full/10.1080/15230406.2017.1323676},
	doi = {10.1080/15230406.2017.1323676},
	language = {en},
	number = {3},
	urldate = {2023-10-26},
	journal = {Cartography and Geographic Information Science},
	author = {Schnur, Susan and Bektaş, Kenan and Çöltekin, Arzu},
	month = may,
	year = {2018},
	pages = {238--254},
}

@article{maceachren_map_1982,
	title = {Map {Complexity}: {Comparison} and {Measurement}},
	volume = {9},
	issn = {0094-1689},
	shorttitle = {Map {Complexity}},
	url = {https://www.tandfonline.com/doi/full/10.1559/152304082783948286},
	doi = {10.1559/152304082783948286},
	language = {en},
	number = {1},
	urldate = {2023-10-26},
	journal = {The American Cartographer},
	author = {MacEachren, Alan M.},
	month = jan,
	year = {1982},
	pages = {31--46},
}

@article{yermo_fast_2022,
	title = {A fast and optimal pathfinder using airborne {LiDAR} data},
	volume = {183},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S0924271621003129},
	doi = {10.1016/j.isprsjprs.2021.11.014},
	abstract = {Determining the optimal path between two points in a 3D point cloud is a problem that have been addressed in many different situations: from road planning and escape routes determination, to network routing and facility layout. This problem is addressed using different input information, being 3D point clouds one of the most valuables. Its main utility is to save costs, whatever the field of application is. In this paper, we present a fast algorithm to determine the least cost path in an Airborne Laser Scanning point cloud. In some situations, like finding escape routes for instance, computing the solution in a very short time is crucial, and there are not many works developed in this theme. State of the art methods are mainly based on a digital terrain model (DTM) for calculating these routes, and these methods do not reflect well the topography along the edges of the graph. Also, the use of a DTM leads to a significant loss of both information and precision when calculating the characteristics of possible routes between two points. In this paper, a new method that does not require the use of a DTM and is suitable for airborne point clouds, whether they are classified or not, is proposed. The problem is modeled by defining a graph using the information given by a segmentation and a Voronoi Tessellation of the point cloud. The performance tests show that the algorithm is able to compute the optimal path between two points by processing up to 678,820 points per second in a point cloud of 40,000,000 points and 16km2 of extension.},
	urldate = {2023-10-25},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Yermo, Miguel and Rivera, Francisco F. and Cabaleiro, José C. and Vilariño, David L. and Pena, Tomás F.},
	month = jan,
	year = {2022},
	keywords = {Airborne point cloud, DTM, Parallel computing, Pathfinding},
	pages = {482--495},
}

@article{fiorucci_deep_2022,
	title = {Deep {Learning} for {Archaeological} {Object} {Detection} on {LiDAR}: {New} {Evaluation} {Measures} and {Insights}},
	volume = {14},
	issn = {2072-4292},
	shorttitle = {Deep {Learning} for {Archaeological} {Object} {Detection} on {LiDAR}},
	url = {https://www.mdpi.com/2072-4292/14/7/1694},
	doi = {10.3390/rs14071694},
	abstract = {Machine Learning-based workflows are being progressively used for the automatic detection of archaeological objects (intended as below-surface sites) in remote sensing data. Despite promising results in the detection phase, there is still a lack of a standard set of measures to evaluate the performance of object detection methods, since buried archaeological sites often have distinctive shapes that set them aside from other types of objects included in mainstream remote sensing datasets (e.g., Dataset of Object deTection in Aerial images, DOTA). Additionally, archaeological research relies heavily on geospatial information when validating the output of an object detection procedure, a type of information that is not normally considered in regular machine learning validation pipelines. This paper tackles these shortcomings by introducing two novel automatic evaluation measures, namely ‘centroid-based’ and ‘pixel-based’, designed to encode the salient aspects of the archaeologists’ thinking process. To test their usability, an experiment with different object detection deep neural networks was conducted on a LiDAR dataset. The experimental results show that these two automatic measures closely resemble the semi-automatic one currently used by archaeologists and therefore can be adopted as fully automatic evaluation measures in archaeological remote sensing detection. Adoption will facilitate cross-study comparisons and close collaboration between machine learning and archaeological researchers, which in turn will encourage the development of novel human-centred archaeological object detection tools.},
	language = {en},
	number = {7},
	urldate = {2023-10-25},
	journal = {Remote Sensing},
	author = {Fiorucci, Marco and Verschoof-van Der Vaart, Wouter B. and Soleni, Paolo and Le Saux, Bertrand and Traviglia, Arianna},
	month = mar,
	year = {2022},
	pages = {1694},
}

@article{lozic_documentation_2021,
	title = {Documentation of {Archaeology}-{Specific} {Workflow} for {Airborne} {LiDAR} {Data} {Processing}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3263},
	url = {https://www.mdpi.com/2076-3263/11/1/26},
	doi = {10.3390/geosciences11010026},
	abstract = {Airborne LiDAR is a widely accepted tool for archaeological prospection. Over the last decade an archaeology-specific data processing workflow has been evolving, ranging from raw data acquisition and processing, point cloud processing and product derivation to archaeological interpretation, dissemination and archiving. Currently, though, there is no agreement on the specific steps or terminology. This workflow is an interpretative knowledge production process that must be documented as such to ensure the intellectual transparency and accountability required for evidence-based archaeological interpretation. However, this is rarely the case, and there are no accepted schemas, let alone standards, to do so. As a result, there is a risk that the data processing steps of the workflow will be accepted as a black box process and its results as “hard data”. The first step in documenting a scientific process is to define it. Therefore, this paper provides a critical review of existing archaeology-specific workflows for airborne LiDAR-derived topographic data processing, resulting in an 18-step workflow with consistent terminology. Its novelty and significance lies in the fact that the existing comprehensive studies are outdated and the newer ones focus on selected aspects of the workflow. Based on the updated workflow, a good practice example for its documentation is presented.},
	language = {en},
	number = {1},
	urldate = {2023-10-25},
	journal = {Geosciences},
	author = {Lozić, Edisa and Štular, Benjamin},
	month = jan,
	year = {2021},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {airborne LiDAR, archaeological prospection, documentation, method, workflow},
	pages = {26},
}

@article{romera_new_2020,
	title = {A new method for locating {Roman} transport infrastructure},
	volume = {43},
	issn = {1296-2074},
	url = {https://www.sciencedirect.com/science/article/pii/S1296207419304935},
	doi = {10.1016/j.culher.2019.10.004},
	abstract = {Roman cities and roads, once correctly identified, can be appropriately conserved. Moreover, the correct identification of Roman transport routes will vindicate the accuracy of recent studies on the network of Roman transport infrastructure and its connectivity, functionality, and impact. With this aim in mind, a novel method is presented for computing the most likely location, from among the various proposed locations that may exist, of any Roman city that is cited both in a Roman itinerary and in Ptolemy's Geographia. In the first phase, the geographical area where the city was located is demarcated by means of the itinerary. In the second phase, Ptolemy's coordinates of well-known Roman cities from the province of the Roman Empire that is under consideration are correlated with those of the WGS 84 reference system by means of simple linear regressions. Having confirmed the normality of the regression error distribution, the bivariate normal distribution is computed, and the confidence intervals are determined. This method is implemented, to identify the most probable location of the Vaccaean city of Intercatia in Hispania, and to propose a new route for the Roman road that once passed through it.},
	urldate = {2023-10-25},
	journal = {Journal of Cultural Heritage},
	author = {Romera, Jesús María and Pérez-Acebo, Heriberto},
	month = may,
	year = {2020},
	keywords = {Antonine Itinerary, Bivariate normal distribution, Hispania, Linear regression, Ptolemy's Geographia, Roman roads},
	pages = {175--185},
}

@article{nuninger_developing_2020,
	title = {Developing {FAIR} {Ontological} {Pathways}: {Linking} {Evidence} of {Movement} in {Lidar} to {Models} of {Human} {Behaviour}},
	volume = {3},
	issn = {2514-8362},
	shorttitle = {Developing {FAIR} {Ontological} {Pathways}},
	url = {http://journal.caa-international.org/articles/10.5334/jcaa.46/},
	doi = {10.5334/jcaa.46},
	language = {en},
	number = {1},
	urldate = {2023-10-25},
	journal = {Journal of Computer Applications in Archaeology},
	author = {Nuninger, Laure and Opitz, Rachel and Verhagen, Philip and Libourel, Thérèse and Laplaige, Clément and Leturcq, Samuel and Le Voguer, Nathanael and Fruchart, Catherine and Kokalj, Žiga and Rodier, Xavier},
	month = apr,
	year = {2020},
	pages = {63--75},
}

@article{stular_visualization_2012,
	title = {Visualization of lidar-derived relief models for detection of archaeological features},
	volume = {39},
	issn = {0305-4403},
	url = {https://www.sciencedirect.com/science/article/pii/S0305440312002373},
	doi = {10.1016/j.jas.2012.05.029},
	abstract = {This paper presents visualisation techniques of high-resolution digital elevation models (DEMs) for visual detection of archaeological features. The methods commonly used in archaeology are reviewed and improvements are suggested. One straightforward technique that has so far not been used in archaeology – the shift method – is presented. The main purpose of this article is to compare and evaluate different visualisation methods. Two conclusions have been reached. Where a single method must be chosen – for printing or producing digital images for non-professionals – the use of sky view factor or slope gradient is endorsed, both presented in greyscale. Otherwise interpreters should choose different techniques on different terrain types: shift on flat terrain, sky view factor on mixed terrain, slope gradient on sloped terrain and sky view factor (preferably as a composite image with slope gradient) on rugged terrain.},
	number = {11},
	urldate = {2023-10-25},
	journal = {Journal of Archaeological Science},
	author = {Štular, Benjamin and Kokalj, Žiga and Oštir, Krištof and Nuninger, Laure},
	month = nov,
	year = {2012},
	keywords = {Archaeology, High-resolution DEM, Lidar, Methodology, Visualisation},
	pages = {3354--3360},
}

@article{hanson_roman_2019,
	title = {The {Roman} {Military} {Presence} at {Dalswinton}, {Dumfriesshire}: a {Reassessment} of the {Evidence} from {Aerial}, {Geophysical} and {LiDAR} {Survey}},
	volume = {50},
	issn = {0068-113X, 1753-5352},
	shorttitle = {The {Roman} {Military} {Presence} at {Dalswinton}, {Dumfriesshire}},
	url = {https://www.cambridge.org/core/product/identifier/S0068113X1900031X/type/journal_article},
	doi = {10.1017/S0068113X1900031X},
	abstract = {ABSTRACT
            The Roman military presence at Dalswinton is reassessed using a range of remote sensing techniques (geophysical survey, LiDAR and aerial photography). At Bankfoot the absence of internal buildings suggests the postulated vexillation fortress was a more temporary structure; while numerous pits/ovens were identified across the interior of the large Stracathro-type camp. The primary fort at Bankhead was provided with in-turned entrances and two small annexes attached to the north-west and south-east quadrants of the fort. A third much larger annexe extended southwards down to the river. Only pits and furnaces were recorded within the annexes, two of which were expanded in Phase 2. Various buildings, including legionary and auxiliary barracks, were identified in the expanded fort of Phase 2, whose orientation remained unchanged. A mixed garrison of legionaries and auxiliary cavalry is indicated for both periods of occupation. Finally, the fort was deliberately demolished. The Roman attribution of the three nearby enclosures at Butterhole Brae can no longer be supported.},
	language = {en},
	urldate = {2023-10-25},
	journal = {Britannia},
	author = {Hanson, W.S. and Jones, R.E. and Jones, R.H.},
	month = nov,
	year = {2019},
	pages = {285--320},
}

@article{palmu_lidar_2015,
	title = {{LiDAR} {DEM} detection and classification of postglacial faults and seismically-induced landforms in {Finland}: a paleoseismic database},
	volume = {137},
	issn = {1103-5897},
	shorttitle = {{LiDAR} {DEM} detection and classification of postglacial faults and seismically-induced landforms in {Finland}},
	url = {https://doi.org/10.1080/11035897.2015.1068370},
	doi = {10.1080/11035897.2015.1068370},
	abstract = {During the last decades, postglacial faults (PGFs) have been found in northern Fennoscandia, the first fault scarps being discovered in western Finnish Lapland in the 1960s. With LiDAR-based digital elevation models (DEMs), a new and accurate remote sensing mapping methodology has been acquired. It allows the relatively rapid and low-cost detection and mapping of late- or PGFs and, for instance, mapping of landslides from areas where they have not previously been recognized. We describe the approach of the Geological Survey of Finland to the systematic search for (screening) and mapping of PGFs, paleolandslides, and other morphological features of Quaternary deposits related to post- and late-glacial seismic activity in Finland. The observations have been collected and classified into a file geodatabase with ArcGis (© ESRI) using a procedure that includes several steps. We also provide examples from western and northern Finland of how sites of late- and postglacial fault scarps and landslides have been detected and described from LiDAR DEM data.},
	number = {4},
	urldate = {2023-10-25},
	journal = {GFF},
	author = {Palmu, Jukka-Pekka and Ojala, Antti E.K. and Ruskeeniemi, Timo and Sutinen, Raimo and Mattila, Jussi},
	month = oct,
	year = {2015},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/11035897.2015.1068370},
	keywords = {Finland, Quaternary deposits, bedrock, fault, landslide, postglacial, seismic activity},
	pages = {344--352},
}

@article{ortiz-villarejo_low-cost_2021,
	title = {A {Low}-{Cost}, {Easy}-{Way} {Workflow} for {Multi}-{Scale} {Archaeological} {Features} {Detection} {Combining} {LiDAR} and {Aerial} {Orthophotography}},
	volume = {13},
	url = {https://www.mdpi.com/2072-4292/13/21/4270},
	doi = {https://doi.org/10.3390/rs13214270},
	number = {21},
	urldate = {2023-10-25},
	journal = {Remote Sensing},
	author = {Ortiz-Villarejo, Antonio and Gutiérrez Soler, Luis-M.},
	month = oct,
	year = {2021},
}

@article{wang_linda_1993,
	title = {Linda — {A} {System} for {Automated} {Linear} {Feature} {Detection} and {Analysis}},
	volume = {19},
	issn = {0703-8992},
	url = {https://doi.org/10.1080/07038992.1993.10855146},
	doi = {10.1080/07038992.1993.10855146},
	abstract = {La reconnaissance et l'interprétation automatisées de formes linéaires à partir de données de télédétection et d'autres données cartographiques sont des opérations importantes dans un grand nombre d'activités liées à l'environnement, telles que la mise à jour des réseaux routiers et des données sur l'utilisation des sols, la cartographie des linéaments géologiques et les applications hydrologiques. Cet article présente un nouveau système articulé sur X-Window et appelé LINDA (Linear-feature Network Detection and Analysis). Différent des systèmes actuels de traitement d'images numériques et des SIG (système d'informations géographiques), ce système permet la reconnaissance et l'analyse automatisées de structures telles les lignes et les contours. Toute image numérique contenant des lignes et (ou) des contours peut constituer une donnée d'entrée. Le système LINDA est composé de plusieurs modules conçus notamment pour: • la détection des lignes — identification automatisée des routes, du réseau hydrographique, etc.;• la détection des contours — identification automatisée des limites des champs, des frontières entre la terre et l'eau, etc.;• l'exécution d'une transformée de Hough — détection automatisée des linéaments géologiques (contours rectilignes);• la reconnaissance des structures — génération automatisée des mesures des structures et des statistiques correspondantes à partir des cartes des structures linéaires qui ont été extraites;• d'autres fonctions. Le présent article décrit l'architecture du système LINDA; des exemples illustrant son mode d'utilisation sont présentés et certains aspects liés à son implantation sont décrits. Grâce à son interface graphique, LINDA est un système piloté par menus, facile d'utilisation. On peut l'utiliser en combinaison avec d'autres systèmes de traitement d'images ou avec des SIG existants. Il se prête à de multiples applications environnementales.},
	number = {1},
	urldate = {2023-10-25},
	journal = {Canadian Journal of Remote Sensing},
	author = {Wang, Jinfei},
	month = jan,
	year = {1993},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/07038992.1993.10855146},
	pages = {009--021},
}

@incollection{Turton1993,
	address = {Berlin},
	series = {Lecture notes in earth sciences},
	title = {Paleomagnetic investigations of {Lago} {Grande} di {Monticchio} ({Southern} {Italy})},
	copyright = {All rights reserved},
	number = {49},
	booktitle = {Paleolimonogy of european maar lakes},
	publisher = {Springer},
	author = {Turton, Ian},
	editor = {Negendank, J. and Zolitska, B.},
	year = {1993},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {377},
}

@article{citeulike:7192517,
	title = {Health {GeoJunction}: place-time-concept browsing of health publications},
	volume = {9},
	copyright = {All rights reserved},
	issn = {1476-072X},
	url = {http://dx.doi.org/10.1186/1476-072X-9-23},
	doi = {10.1186/1476-072X-9-23},
	abstract = {BACKGROUND:The volume of health science publications is escalating rapidly, thus keeping up with developments is becoming harder as is the task of finding important cross-field connections. When geographic location is a relevant component in the publication, these tasks are more difficult because standard search and indexing facilities have limited or no ability to identify geographic foci in documents. This paper introduces HEALTH GeoJunction, a web application that supports researchers in the task of quickly finding scientific publications that are relevant geographically and temporally as well as thematically. RESULTS:HEALTH GeoJunction is a geovisual analytics-enabled web application providing: (a) web services using computational reasoning methods to extract place-time-concept information from bibliographic data for documents and (b) visually-enabled place-time-concept query, filtering, and contextualizing tools that apply to both the documents and their extracted content. This paper focuses specifically on strategies for visually-enabled, iterative, facet-like, place-time-concept filtering that allow analysts to quickly drill down to scientific findings of interest in PubMed abstracts and to explore relations among abstracts and extracted concepts in place and time. The approach enables analysts to: find publications without knowing all relevant query parameters, recognize unanticipated geographic relations within and among documents in multiple health domains, identify the thematic emphasis of research targeting particular places, and notice changes in concepts over time and changes in places where concepts are emphasized.CONCLUSIONS:PubMed is a database of over 19 million biomedical abstracts and citations maintained by the National Center for Biotechnology Information; achieving quick filtering is an important contribution due to the database size and the recent, rapid escalation in attention to geographic factors in public health. The implementation of mechanisms for place-time-concept cross-filtering and for hierarchical application of that filtering makes it possible to narrow down efficiently and quickly from thousands of documents to a small subset that meet place-time-concept constraints. Support for a more-like-this query creates the potential to identify unexpected connections across diverse areas of research. Multi-view visualization methods support understanding of the place, time, and concept components of document collections and enable comparison of filtered query results to the full set of publications.},
	number = {1},
	journal = {International Journal of Health Geographics},
	author = {MacEachren, Alan and Stryker, Michael and Turton, Ian and Pezanowski, Scott},
	year = {2010},
	note = {tex.citeulike-article-id: 7192517
tex.citeulike-linkout-0: http://dx.doi.org/10.1186/1476-072X-9-23
tex.citeulike-linkout-1: http://view.ncbi.nlm.nih.gov/pubmed/20482806
tex.citeulike-linkout-2: http://www.hubmed.org/display.cgi?uids=20482806
tex.posted-at: 2010-05-18 23:20:13
tex.priority: 0},
	keywords = {pubmed, tagging, webmapping},
	pages = {23+},
}

@inproceedings{citeulike:4186975,
	address = {Park City, Utah},
	title = {Health {GeoJunction}: {Geovisualization} of news and scientific publications to support situation awareness},
	copyright = {All rights reserved},
	url = {http://geoanalytics.net/GeoVisualAnalytics08/a18.pdf},
	booktitle = {{GIScience}'08},
	author = {Stryker, M. and Turton, Ian and MacEachren, A.},
	year = {2008},
	note = {tex.citeulike-article-id: 4186975
tex.citeulike-linkout-0: http://geoanalytics.net/GeoVisualAnalytics08/a18.pdf
tex.posted-at: 2009-03-17 16:48:38
tex.priority: 0},
	keywords = {citations, clustering, geographic, pubmed, webmapping},
}

@article{hannon_antonine_2017,
	title = {The {Antonine} {Wall}'s distance-slabs: {LiDAR} as metric survey},
	volume = {30},
	issn = {1047-7594, 2331-5709},
	shorttitle = {The {Antonine} {Wall}'s distance-slabs},
	url = {https://www.cambridge.org/core/journals/journal-of-roman-archaeology/article/antonine-walls-distanceslabs-lidar-as-metric-survey/19EF5A1ABE544D0BDCC6CD19164848F2},
	doi = {10.1017/S1047759400074201},
	abstract = {The “Hidden Landscape of a Roman Frontier” is a collaborative research project run and jointly funded by Canterbury Christ Church University (CCCU) and Historic Environment Scotland (HES). Intended to run for a 3-year period, it began in October 2015. The project focuses on the landscape archaeology, history, and heritage management of the Roman frontier in Scotland, part of the “Frontiers of the Roman Empire” transnational UNESCO World Heritage Site since 2008. The project's primary data-set is comprised of aerial LiDAR at 0.5-m resolution covering the World Heritage Site, combined with terrestrial laser-scanning coverage for the forts at Bar Hill and Rough Castle and the fortlet at Kinneil. All data was commissioned under the auspices of the Scottish Ten Project; the aerial data was captured in spring 2010, the terrestrial data in July 2013 and April 2016. The project also draws upon a number of supplemental data sources, including the National Monuments Record of Scotland (https://canmore.org.uk/), geophysical survey data, archive aerial images, colour infra-red imagery, and additional LiDAR data from the UK Environment Agency.},
	language = {en},
	urldate = {2023-10-23},
	journal = {Journal of Roman Archaeology},
	author = {Hannon, Nick and Rohl, Darrell J. and Wilson, Lyn},
	month = jan,
	year = {2017},
	note = {Publisher: Journal of Roman Archaeology L.L.C.},
	pages = {447--468},
}

@article{hunter_2_2021,
	title = {2. {SCOTLAND}},
	volume = {52},
	issn = {0068-113X, 1753-5352},
	url = {https://www.cambridge.org/core/journals/britannia/article/2-scotland/D760D3B6D6BFC64DA69872352D7F0CF0},
	doi = {10.1017/S0068113X21000350},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0068113X21000350/resource/name/firstPage-S0068113X21000350a.jpg},
	language = {en},
	urldate = {2023-10-23},
	journal = {Britannia},
	author = {Hunter, Fraser},
	month = nov,
	year = {2021},
	note = {Publisher: Cambridge University Press},
	pages = {392--395},
}

@article{walas_england_2021,
	title = {{ENGLAND} 3. {HADRIAN}'{S} {WALL}},
	volume = {52},
	issn = {0068-113X, 1753-5352},
	url = {https://www.cambridge.org/core/journals/britannia/article/england-3-hadrians-wall/6430B13166D0288AAD69D994FEB22A4A},
	doi = {10.1017/S0068113X21000362},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0068113X21000362/resource/name/firstPage-S0068113X21000362a.jpg},
	language = {en},
	urldate = {2023-10-23},
	journal = {Britannia},
	author = {Walas, Anna H.},
	month = nov,
	year = {2021},
	note = {Publisher: Cambridge University Press},
	pages = {395--396},
}

@article{walas_4_2022,
	title = {4. {NORTHERN} {ENGLAND}},
	volume = {53},
	issn = {0068-113X, 1753-5352},
	url = {https://www.cambridge.org/core/journals/britannia/article/4-northern-england/CB3D67E4FDD5EF8805FE34C66318106F},
	doi = {10.1017/S0068113X22000411},
	abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0068113X22000411/resource/name/firstPage-S0068113X22000411a.jpg},
	language = {en},
	urldate = {2023-10-23},
	journal = {Britannia},
	author = {Walas, Anna H.},
	month = nov,
	year = {2022},
	note = {Publisher: Cambridge University Press},
	pages = {421--440},
}

@book{margary_roman_1973,
	address = {London},
	edition = {3rd},
	title = {Roman roads in {Britain}},
	url = {https://go.exlibris.link/ndXRf8Xm},
	language = {English},
	number = {Book, Whole},
	publisher = {J. Baker},
	author = {Margary, Ivan Donald},
	year = {1973},
	keywords = {Roads, Roman},
}

@article{gasparini_identifying_2019,
	title = {Identifying the {Roman} road from {Corduba} to {Emerita} in the {Puente} {Nuevo} reservoir ({Espiel}-{Córdoba}/{Spain})},
	volume = {24},
	issn = {2352-409X},
	url = {https://www.sciencedirect.com/science/article/pii/S2352409X1830693X},
	doi = {10.1016/j.jasrep.2019.01.026},
	abstract = {A stretch of the Roman road between Corduba and Emerita runs through the modern region of Alto Guadiato in the province of Córdoba (Spain). The structure of this Roman road is well known in the most mountainous part of this territory. However, there are four reservoirs along the course of this road in the flattest part of the region. Therefore, usually the Roman road is submerged and becomes hard to study it. In December 2017 we have been taken advantage of a drought period in order to document the road using remote sensing, photogrammetry and ground investigations. The first step of this research was the identification of this Roman road from several aerial imagery datasets of Spain available in the CNIG. Later, diverse DTM processed from the PNOA LiDAR point clouds were used to obtain the maximum of information in order to design our own photogrammetric acquisitions. Then, we have compared the performance of both photogrammetric DEMs. Finally, this dataset was checked with a ground survey along the Roman road. In this paper we present the earth observation, remote sensing and ground prospection methods that can be used to study a Roman road in an overall way. Moreover, these resources are very useful when agility is required to get high-quality documentation, as is the case of this submerged stretch of Roman road. Finally, these methods are also helpful to preserve the memory of threated archaeological sites.},
	urldate = {2023-10-23},
	journal = {Journal of Archaeological Science: Reports},
	author = {Gasparini, Massimo and Moreno-Escribano, Juan Carlos and Monterroso-Checa, Antonio},
	month = apr,
	year = {2019},
	keywords = {Aerial imagery, Archaeological prospection, Corduba-Emerita road, Endangered heritage, Photogrammetric acquisitions, Roman imperial age, Roman republican age, Roman road},
	pages = {363--372},
}

@misc{zhao_geo_2023,
	title = {Geo {SAM}: {A} {QGIS} plugin using {Segment} {Anything} {Model} ({SAM}) to accelerate geospatial image segmentation},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	shorttitle = {Geo {SAM}},
	url = {https://zenodo.org/record/8191039},
	abstract = {Geo SAM is a QGIS plugin that aims to help people segment, delineate or label landforms efficiently when using large-size geospatial raster images. Segment Anything Model (SAM) is a foundation AI model with the superpower, but the model size is huge, and using it to process images can take a long time, even with a modern GPU. Our tool uses the strategies of encoding image features in advance and trimming the SAM model. The interactive segmentation process can be run in real-time on a laptop by only using a CPU, making it a convenient and efficient tool for dealing with satellite images. The Geo SAM plugin includes two separate tools, the encoding tool and the segmentation tool. The encoding tool is designed to generate and save the image features using the SAM image encoder, and the encoding process only needs to run once per image. The segmentation tool is for interactively segmenting landforms, and it can only be used to segment preprocessed images (whose features have been generated in advance using the encoding tool, as the included demo image).},
	urldate = {2023-10-22},
	publisher = {Zenodo},
	author = {Zhao, Zhuoyi and Fan, Chengyan and Liu, Lin},
	month = jul,
	year = {2023},
	doi = {10.5281/ZENODO.8191039},
	note = {Language: en},
	keywords = {Deep learning, Geospatial, QGIS Plugin, Segment Anything Model, Segmentation, feature extraction},
}

@article{llobera_zigzagging_2007,
	title = {Zigzagging: {Theoretical} insights on climbing strategies},
	volume = {249},
	issn = {00225193},
	shorttitle = {Zigzagging},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022519307003542},
	doi = {10.1016/j.jtbi.2007.07.020},
	language = {en},
	number = {2},
	urldate = {2023-10-19},
	journal = {Journal of Theoretical Biology},
	author = {Llobera, M. and Sluckin, T.J.},
	month = nov,
	year = {2007},
	keywords = {modelling, movement, travel},
	pages = {206--217},
}

@inproceedings{albrecht_learning_2019,
	address = {Los Angeles, CA, USA},
	title = {Learning and {Recognizing} {Archeological} {Features} from {LiDAR} {Data}},
	isbn = {978-1-72810-858-2},
	url = {https://ieeexplore.ieee.org/document/9005548/},
	doi = {10.1109/BigData47090.2019.9005548},
	urldate = {2023-10-19},
	booktitle = {2019 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	publisher = {IEEE},
	author = {Albrecht, Conrad M and Fisher, Chris and Freitag, Marcus and Hamann, Hendrik F and Pankanti, Sharathchandra and Pezzutti, Florencia and Rossi, Francesca},
	month = dec,
	year = {2019},
	keywords = {archeology, feature extraction, lidar},
	pages = {5630--5636},
}

@article{tarolli_features_2019,
	title = {From features to fingerprints: {A} general diagnostic framework for anthropogenic geomorphology},
	volume = {43},
	issn = {0309-1333, 1477-0296},
	shorttitle = {From features to fingerprints},
	url = {http://journals.sagepub.com/doi/10.1177/0309133318825284},
	doi = {10.1177/0309133318825284},
	abstract = {Human societies have been reshaping the geomorphology of landscapes for thousands of years, producing anthropogenic geomorphic features ranging from earthworks and reservoirs to settlements, roads, canals, ditches and plough furrows that have distinct characteristics compared with landforms produced by natural processes. Physical geographers have long recognized the widespread importance of these features in altering landforms and geomorphic processes, including hydrologic flows and stores, to processes of soil erosion and deposition. In many of the same landscapes, archaeologists have also utilized anthropogenic geomorphic features to detect and analyse human societal activities, including symbolic formations, agricultural systems, settlement patterns and trade networks. This paper provides a general framework aimed at integrating geophysical and archaeological approaches to observing, identifying and interpreting the full range of anthropogenic geomorphic features based on their structure and functioning, both individually and as components of landscape-scale management strategies by different societies, or “sociocultural fingerprints”. We then couple this framework with new algorithms developed to detect anthropogenic geomorphic features using precisely detailed three-dimensional reconstructions of landscape surface structure derived from LiDAR and computer vision photogrammetry. Human societies are now transforming the geomorphology of landscapes at increasing rates and scales across the globe. To understand the causes and consequences of these transformations and contribute to building sustainable futures, the science of physical geography must advance towards empirical and theoretical frameworks that integrate the natural and sociocultural forces that are now the main shapers of Earth’s surface processes.},
	language = {en},
	number = {1},
	urldate = {2023-10-20},
	journal = {Progress in Physical Geography: Earth and Environment},
	author = {Tarolli, Paolo and Cao, Wenfang and Sofia, Giulia and Evans, Damian and Ellis, Erle C},
	month = feb,
	year = {2019},
	keywords = {feature extraction, lidar},
	pages = {95--128},
}

@incollection{verhagen_modelling_2019,
	address = {Cham},
	title = {Modelling of {Pathways} and {Movement} {Networks} in {Archaeology}: {An} {Overview} of {Current} {Approaches}},
	isbn = {978-3-030-04575-3 978-3-030-04576-0},
	shorttitle = {Modelling of {Pathways} and {Movement} {Networks} in {Archaeology}},
	url = {https://link.springer.com/10.1007/978-3-030-04576-0_11},
	abstract = {Abstract
            This chapter presents and discusses current approaches and trends in computer-based modelling of pathways and movement networks in archaeology. After an introduction to the theoretical concepts involved, we present a state of the art of methodologies applied for reconstructing pathways and movement in ancient landscapes and discuss the various difficulties in using these methods as well as the most important technical hurdles involved. The problems of integrating optimal pathfinding algorithms with ‘softer’ socio-cultural variables are highlighted, as well as the limitations of modelling connections between places using least-cost path techniques. Network analysis reconstruction and analysis approaches are then reviewed as tools to better understand the overall structure of movement and communication in ancient landscapes. It is concluded that, while the potential of current approaches for understanding ancient movement is considerable, improvement is still needed in three main areas: the integration of approaches, sensitivity analysis and validation, and the theoretical underpinning of models of ancient movement.},
	language = {en},
	urldate = {2023-10-19},
	booktitle = {Finding the {Limits} of the {Limes}},
	publisher = {Springer International Publishing},
	author = {Verhagen, Philip and Nuninger, Laure and Groenhuijzen, Mark R.},
	editor = {Verhagen, Philip and Joyce, Jamie and Groenhuijzen, Mark R.},
	year = {2019},
	doi = {10.1007/978-3-030-04576-0_11},
	note = {Series Title: Computational Social Sciences},
	keywords = {lcp, least cost path, modelling, roman},
	pages = {217--249},
}

@article{herzog_least-cost_2014,
	title = {Least-cost {Paths} – {Some} {Methodological} {Issues}},
	issn = {13635387},
	url = {http://intarch.ac.uk/journal/issue36/herzog_index.html},
	doi = {10.11141/ia.36.5},
	number = {36},
	urldate = {2023-10-19},
	journal = {Internet Archaeology},
	author = {Herzog, Irmela},
	year = {2014},
	keywords = {lcp, least cost path, modelling},
}

@article{breeze_roman_1985,
	title = {Roman {Military} {Deployment} in {North} {England}},
	volume = {16},
	issn = {0068113X},
	url = {https://www.jstor.org/stable/526389?origin=crossref},
	doi = {10.2307/526389},
	urldate = {2023-10-19},
	journal = {Britannia},
	author = {Breeze, David J. and Dobson, Brian},
	year = {1985},
	keywords = {North England, roman},
	pages = {1},
}

@article{ratledge_lancasters_2020,
	title = {Lancaster's {Roman} roads: {A} {LiDAR} {Reapraisal}},
	volume = {38},
	abstract = {This paper updates our understanding of the Roman roads around Lancaster, using LiDAR data.},
	journal = {Contrebis},
	author = {Ratledge, David},
	year = {2020},
	keywords = {lancashire, lidar, roman},
}

@incollection{verhagen_footprints_2019,
	address = {Cham},
	title = {Footprints and {Cartwheels} on a {Pixel} {Road}: {On} the {Applicability} of {GIS} for the {Modelling} of {Ancient} ({Roman}) {Routes}},
	isbn = {978-3-030-04575-3 978-3-030-04576-0},
	shorttitle = {Footprints and {Cartwheels} on a {Pixel} {Road}},
	url = {https://link.springer.com/10.1007/978-3-030-04576-0_14},
	abstract = {Abstract
            GIS-based digital modelling tools, such as the well-known least cost paths (LCP), have been widely used in archaeology in recent years as ways of approaching forms of mobility in the past. Roman roads are among the best-known examples of ancient networks of paths and have been widely studied using such approaches. In this paper, we shall make a general reflection on the applicability of those tools for the modelling and analysis of ancient routes, with a special focus on Roman roads. Drawing from a case study in the NW Iberian Peninsula, we shall discuss certain aspects related to the potential and limits of Cumulative Costs, LCP and other related tools for the modelling and analysis of ancient roads. We will illustrate how the use of tools which explore potential mobility in less restricted ways can help to overcome some of the limitations of LCP.},
	language = {en},
	urldate = {2023-10-16},
	booktitle = {Finding the {Limits} of the {Limes}},
	publisher = {Springer International Publishing},
	author = {Parcero-Oubiña, César and Güimil-Fariña, Alejandro and Fonte, João and Costa-García, José Manuel},
	editor = {Verhagen, Philip and Joyce, Jamie and Groenhuijzen, Mark R.},
	year = {2019},
	doi = {10.1007/978-3-030-04576-0_14},
	note = {Series Title: Computational Social Sciences},
	keywords = {gis, lidar, modelling, roman},
	pages = {291--311},
}

@article{parcero-oubina_remote_2023,
	title = {Remote {Sensing} and {GIS} {Modelling} of {Roman} {Roads} in {South} {West} {Britain}},
	volume = {6},
	issn = {2514-8362},
	url = {http://journal.caa-international.org/articles/10.5334/jcaa.109/},
	doi = {10.5334/jcaa.109},
	language = {en},
	number = {1},
	urldate = {2023-10-16},
	journal = {Journal of Computer Applications in Archaeology},
	author = {Parcero-Oubina, Cesar and Smart, Chris and Fonte, João},
	month = jul,
	year = {2023},
	keywords = {lcp, lidar, modelling, roman},
	pages = {62--78},
}

@article{lewis_probabilistic_2021,
	title = {Probabilistic {Modelling} for {Incorporating} {Uncertainty} in {Least} {Cost} {Path} {Results}: a {Postdictive} {Roman} {Road} {Case} {Study}},
	volume = {28},
	issn = {1072-5369, 1573-7764},
	shorttitle = {Probabilistic {Modelling} for {Incorporating} {Uncertainty} in {Least} {Cost} {Path} {Results}},
	url = {https://link.springer.com/10.1007/s10816-021-09522-w},
	doi = {10.1007/s10816-021-09522-w},
	abstract = {Abstract
            
              The movement of past peoples in the landscape has been studied extensively through the use of least cost path (LCP) analysis. Although methodological issues of applying LCP analysis in archaeology have frequently been discussed, the effect of DEM error on LCP results has not been fully assessed. Due to this, the reliability of the LCP result is undermined, jeopardising how well the method can confidently be used to model past movement. To strengthen the reliability of LCP results, this research proposes the use of Monte Carlo simulation as a method for incorporating and propagating the effects of error on LCP results. Focusing on vertical error, random error fields are calculated and incorporated into the documented and reproducible LCP modelling process using the R package
              leastcostpath
              . By graphically communicating the impact of vertical error using probabilistic LCPs, uncertainty in the results can be taken into account when interpreting LCPs. The method is applied to a Roman road case study, finding that the incorporation of vertical error results in the identification of multiple ‘least cost’ routes within the landscape. Furthermore, the deviation between the roman road and the probabilistic LCP suggests that the location of the roman road was influenced by additional factors other than minimising energy expenditure. This research finds that the probabilistic LCP derived using Monte Carlo simulation is a viable method for the graphical communication of the uncertainty caused by error within the input data used within the LCP modelling process. Therefore, it is recommended that probabilistic LCPs become the default approach when modelling movement using input data that contains errors.},
	language = {en},
	number = {3},
	urldate = {2023-10-16},
	journal = {Journal of Archaeological Method and Theory},
	author = {Lewis, Joseph},
	month = sep,
	year = {2021},
	keywords = {lidar, modelling, roman},
	pages = {911--924},
}

@book{verhagen_finding_2019,
	address = {Cham},
	series = {Computational {Social} {Sciences}},
	title = {Finding the {Limits} of the {Limes}: {Modelling} {Demography}, {Economy} and {Transport} on the {Edge} of the {Roman} {Empire}},
	isbn = {978-3-030-04575-3 978-3-030-04576-0},
	shorttitle = {Finding the {Limits} of the {Limes}},
	url = {https://link.springer.com/10.1007/978-3-030-04576-0},
	language = {en},
	urldate = {2023-10-16},
	publisher = {Springer International Publishing},
	editor = {Verhagen, Philip and Joyce, Jamie and Groenhuijzen, Mark R.},
	year = {2019},
	doi = {10.1007/978-3-030-04576-0},
	keywords = {lidar, modelling, roman},
}

@inproceedings{Xu2009,
	address = {Las Vegas, NV},
	title = {Regional linguistic difference, route description, spatial orientation, language pattern, cardinal/relative direction usage},
	copyright = {All rights reserved},
	booktitle = {{AAG}},
	author = {Xu, S. and Klippel, A. and MacEachren, A. M. and Mitra, P. and Turton, I. and Zhang, J.},
	year = {2009},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
}

@incollection{Turton1997,
	address = {London},
	title = {A genetic programming approach to building new spatial interaction models relevant to {GIS}},
	copyright = {All rights reserved},
	booktitle = {Innovations in {GIS} 4},
	publisher = {Taylor and Francis},
	author = {Turton, I. and Openshaw, S. and Diplock, G.},
	editor = {Kemp, Z.},
	year = {1997},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
}

@inproceedings{Turton1996,
	address = {Berlin},
	title = {Some geographical applications of genetic programming on the {Cray} {T3D} supercomputer},
	copyright = {All rights reserved},
	booktitle = {{UK} parallel'96},
	publisher = {Springer},
	author = {Turton, I. and Openshaw, S. and Diplock, G.},
	editor = {Jesshope, C. and Shafarenko, A.},
	year = {1996},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {135--150},
}

@incollection{Turton2000a,
	address = {London},
	title = {Exploring geographical hyperspaces},
	copyright = {All rights reserved},
	booktitle = {Innovations in {GIS} 7},
	publisher = {Taylor and Francis},
	author = {Turton, I. and Openshaw, S. and Brunsden, C. and Turner, A. and Macgill, J.},
	editor = {Atkinson, P. and Martin, D.},
	year = {2000},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {87--100},
}

@incollection{Turton1996a,
	address = {Berlin},
	title = {Modelling and optimising flows using parallel spatial interaction models},
	volume = {2},
	copyright = {All rights reserved},
	booktitle = {Euro-par'96 parallel processing},
	publisher = {Springer},
	author = {Turton, I. and Openshaw, S.},
	editor = {Bougé, L. and Fraigniaud, P. and Mignotte, A. and Robert, Y.},
	year = {1996},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {270--275},
}

@inproceedings{Turton2008,
	address = {Shepherdstown, WV, USA},
	title = {A web based tool for the detection and analysis of avian influenza outbreaks from internet news sources},
	copyright = {All rights reserved},
	booktitle = {{AutoCarto}},
	author = {Turton, I. and Murdoch, A.},
	year = {2008},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
}

@inproceedings{texttree,
	address = {Park City, Utah},
	title = {Visualizing unstructured text documents using trees and maps},
	copyright = {All rights reserved},
	url = {http://geoanalytics.net/GeoVisualAnalytics08/a06.pdf},
	booktitle = {{GIScience}'08},
	author = {Turton, Ian and MacEachren, A.},
	month = sep,
	year = {2008},
	note = {tex.owner: ijt1
tex.timestamp: 2011.04.26},
}

@incollection{citeulike:2847357,
	address = {London, UK},
	title = {Testing space-time and more complex hyperspace geographical analysis tools},
	copyright = {All rights reserved},
	booktitle = {{GIS} and geocomputation},
	publisher = {Taylor and Francis},
	author = {Turton, I. and Openshaw, S. and Brunsdon, C. and Turner, A. and Macgill, J.},
	editor = {Atkinson, Peter M. and Martin, David},
	year = {2000},
	note = {tex.citeulike-article-id: 2847357
tex.citeulike-linkout-0: http://books.google.com/books?hl=en{\textbackslash}\&{\textbackslash}\#38;lr={\textbackslash}\&{\textbackslash}\#38;id=SVbNsMklmOgC{\textbackslash}\&{\textbackslash}\#38;oi=fnd{\textbackslash}\&{\textbackslash}\#38;pg=PA87{\textbackslash}\&{\textbackslash}\#38;ots=JNe9dRTT4a{\textbackslash}\&{\textbackslash}\#38;sig=9NWVru2OGNlUds-7zQZFboq2mhQ
tex.posted-at: 2008-05-30 14:54:18
tex.priority: 0},
	keywords = {analysis, clustering, exploratory, gam, geographic, space, space-time, testing, time},
	pages = {87--102},
}

@article{citeulike:7899606,
	title = {Putting the 1991 census sample of anonymised records on your {Unix} workstation},
	volume = {27},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1068/a270391},
	doi = {10.1068/a270391},
	abstract = {The authors describe the development of a customised computer software package for easing the analysis of the UK 1991 Sample of Anonymised Records. The resulting USAR package is designed to be portable within the Unix environment. It offers a number of features such as interactive table design, intelligent data interpretation, and fuzzy query. An example of SAR analysis is provided.},
	number = {3},
	journal = {Environment and Planning A},
	author = {Turton, I. and Openshaw, S.},
	year = {1995},
	note = {Publisher: Pion Ltd
tex.citeulike-article-id: 7899606
tex.citeulike-linkout-0: http://dx.doi.org/10.1068/a270391
tex.citeulike-linkout-1: http://www.envplan.com/abstract.cgi?id=a270391
tex.posted-at: 2010-09-26 03:17:28
tex.priority: 2},
	keywords = {census, opensource, sar, unix, usar},
	pages = {391--411},
}

@article{Turton1997a,
	title = {Parallel spatial interaction models},
	volume = {1.2},
	copyright = {All rights reserved},
	journal = {Geographical and Environmental Modelling},
	author = {Turton, I. and Openshaw, S.},
	year = {1997},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {179--198},
}

@incollection{Turton2001,
	title = {Automated crime pattern analysis using the geographical analysis machine},
	copyright = {All rights reserved},
	booktitle = {Mapping and analysing crime data},
	publisher = {Taylor and Francis},
	author = {Turton, Ian and Openshaw, S.},
	editor = {Hirchsfield, A and Bowers, K.},
	year = {2001},
	note = {Section: 2
tex.owner: ijt1
tex.timestamp: 2010.09.25},
	pages = {11--26},
}

@inproceedings{citeulike:1789944,
	title = {Building a standards based collaborative {GIS}},
	copyright = {All rights reserved},
	url = {http://www.geocomputation.org/2005/Turton.pdf},
	booktitle = {{GeoComputation}},
	author = {Turton, Ian and Macgill, James},
	year = {2005},
	note = {tex.citeulike-article-id: 1789944
tex.citeulike-linkout-0: http://www.geocomputation.org/2005/Turton.pdf
tex.posted-at: 2007-10-19 18:57:44
tex.priority: 0},
	keywords = {collaborative, gis, ogc, standards},
}

@inproceedings{Turton2009a,
	address = {Sydney, Australia},
	title = {Why geolocating written routes is harder than it looks},
	copyright = {All rights reserved},
	booktitle = {{GeoComputation} 2009},
	author = {Turton, I.},
	year = {2009},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.25},
}

@inproceedings{Ballas1999,
	title = {Exploring microsimulation methodologies for the estimation of household attributes},
	copyright = {All rights reserved},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.34.8164&rep=rep1&type=pdf},
	booktitle = {4th {GeoComputation} conference},
	author = {Ballas, D and Clarke, G and Turton, I},
	year = {1999},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.25},
}

@inproceedings{Turton2009,
	address = {Sydney, Australia},
	title = {Experiences teaching {GIS} with open source software},
	copyright = {All rights reserved},
	booktitle = {{GeoComputation} 2009},
	author = {Turton, I.},
	year = {2009},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.25},
}

@incollection{Turton2002,
	address = {Chichester},
	title = {On-line tabulation for the {Samples} of {Anonymised} {Records}},
	copyright = {All rights reserved},
	booktitle = {The census data system},
	publisher = {John Wiley},
	author = {Turton, Ian},
	editor = {Rees, P. and Martin, D. and Williamson, P.},
	year = {2002},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {213--219},
}

@incollection{Turton2000,
	title = {Parallel processing in geography},
	copyright = {All rights reserved},
	booktitle = {{GeoComputation}},
	publisher = {Gordon and Breach},
	author = {Turton, Ian},
	editor = {Openshaw, S. and Harris, T. and Abrahart, R.},
	year = {2000},
	note = {Section: 4
tex.owner: ijt1
tex.timestamp: 2010.09.25},
	pages = {49--66},
}

@incollection{Hall2008,
	edition = {1},
	title = {{GeoTools}},
	copyright = {All rights reserved},
	isbn = {3-540-74830-X},
	url = {http://www.worldcat.org/isbn/354074830X},
	abstract = {This book focuses on the nature and characteristics of open source geospatial(OSG) software. The role of OSG approaches in spatial data handling is thecross-cutting theme of the book. Various sub-themes are explored are exploredthat introduce readers unfamiliar to OSG software to the nature, purpose andapplications of OS programming, and to the key new OS tools and theirapplication within the geospatial data domain. The book also includes adiscussion of new tools, approaches and applications for those already usingOS approaches to software development.},
	booktitle = {Open source approaches in spatial data handling (advances in geographic information science)},
	publisher = {Springer},
	author = {Turton, Ian},
	editor = {Hall, Brent G. and Leahy, Michael G.},
	year = {2008},
	note = {tex.citeulike-article-id: 3501710
tex.citeulike-linkout-0: http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\textbackslash}\&amp;path=ASIN/354074830X
tex.citeulike-linkout-1: http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21{\textbackslash}\&amp;path=ASIN/354074830X
tex.citeulike-linkout-2: http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21{\textbackslash}\&amp;path=ASIN/354074830X
tex.citeulike-linkout-3: http://www.amazon.jp/exec/obidos/ASIN/354074830X
tex.citeulike-linkout-4: http://www.amazon.co.uk/exec/obidos/ASIN/354074830X/citeulike00-21
tex.citeulike-linkout-5: http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20{\textbackslash}\&path=ASIN/354074830X
tex.citeulike-linkout-6: http://www.worldcat.org/isbn/354074830X
tex.citeulike-linkout-7: http://books.google.com/books?vid=ISBN354074830X
tex.citeulike-linkout-8: http://www.amazon.com/gp/search?keywords=354074830X{\textbackslash}\&index=books{\textbackslash}\&linkCode=qs
tex.citeulike-linkout-9: http://www.librarything.com/isbn/354074830X
tex.howpublished: Hardcover
tex.posted-at: 2008-11-10 19:22:22
tex.priority: 4},
	keywords = {geocomputation, geovistastudio, opensource},
}

@incollection{citeulike:4237774,
	address = {New York},
	title = {Application of pattern recognition to concept discovery},
	copyright = {All rights reserved},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.4591},
	abstract = {This paper explores the ways that pattern matching techniques can be applied to raster datasets to develop new concepts for geography. It is argued that it is becoming increasingly important for geographers to develop new ways of generalising datasets that will allow them to overcome the increasing data richness of the geocybersphere. The data set used for this study is a population density surface derived from the 1991 census of population by Bracken and Martin (1989). Using this data set the aim is to take the data-poor geographical theories of urban social structure of the first half of the century and make use of the data-rich environments of the 1990s to test the theories in a general and robust manner. To achieve this pattern matching techniques used in computer vision and other fields will be applied to raster data of population density and social and economic variables for Great Britain. Initially the raster data is segmented by the application of image analysis techniques to identify 129 urban areas in Britain. These urban areas are then compared to templates of the theoretical models of Burgess (1925) and Hoyt (1939) using methods developed in the fields of computer vision and medical imaging. Several urban areas are found that are similar in social structure to the theoretical models developed earlier in the century. The urban areas are then compared to themselves to determine if there were any other groupings of modern British cities that can be made in terms of their social structure. Several such groups are discovered and will be briefly discussed.},
	booktitle = {High-performance computing},
	publisher = {Plenum Press},
	author = {Turton, Ian},
	editor = {Allan, R.J. and Guest, M.F. and Simpson, A.D. and Henty, D.S.},
	year = {1999},
	note = {tex.citeulike-article-id: 4237774
tex.citeulike-linkout-0: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.4591
tex.posted-at: 2009-03-30 19:08:07
tex.priority: 0},
	keywords = {data-mining, geography, gis, patternrecognition},
	pages = {467--478},
}

@inproceedings{Tomaszewski2006,
	address = {dg.o2006 ,San Diego, CA},
	title = {Supporting humanitarian relief logistics operations through online geocollaborative knowledge management},
	copyright = {All rights reserved},
	booktitle = {Proceedings of the national conference on digital government research},
	author = {Tomaszewski, B and MacEachren, A.M. and Pezanowski, S and Liu, X and Turton, I},
	year = {2006},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
}

@article{Rees1998,
	title = {Geocomputation: {Solving} geographical problems with new computing power},
	volume = {30.10},
	copyright = {All rights reserved},
	journal = {Environment And Planning A},
	author = {Rees, P. and Turton, I},
	year = {1998},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {1835--1838},
}

@incollection{Openshaw1999,
	address = {London},
	title = {Putting the geographical analysis machine on the internet},
	copyright = {All rights reserved},
	booktitle = {Innovations in {GIS} 6},
	publisher = {Taylor and Francis},
	author = {Openshaw, S. and Turton, I. and Macgill, J. and Davy, J.},
	editor = {Gittings, B.},
	year = {1999},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {121--131},
}

@article{Openshaw1996a,
	title = {A parallel {Kohonen} algorithm for the classification of large spatial datasets},
	volume = {22},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1016/S0098-3004(96)00040-4},
	doi = {10.1016/S0098-3004(96)00040-4},
	abstract = {The paper describes the development of Kohonen-net-based methods suitable for the classification of large spatial datasets suitable for parallel processing. Parallelising the Kohonen net is not easy because the degree of natural parallelism is finely grained. This paper presents a new algorithm and demonstrates its performance on the Cray T3D parallel supercomputer.},
	number = {9},
	journal = {Computers and Geosciences},
	author = {Openshaw, Stan and Turton, Ian},
	month = nov,
	year = {1996},
	note = {Publication title: Neural network applications in the geosciences
tex.citeulike-article-id: 1067467
tex.citeulike-linkout-0: http://dx.doi.org/10.1016/S0098-3004(96)00040-4
tex.citeulike-linkout-1: http://www.sciencedirect.com/science/article/B6V7D-3VWKMT7-T/2/ce2c95078a6cceba269b0bd8bc334ae1
tex.posted-at: 2007-01-25 20:10:05
tex.priority: 0},
	keywords = {algorithm, classification, clustering, data, data-mining, geocomputation, geospatial},
	pages = {1019--1026},
}

@article{Openshaw1996,
	title = {New opportunities for geographical census analysis using individual level data},
	volume = {28.2},
	copyright = {All rights reserved},
	journal = {Area},
	author = {Openshaw, S. and Turton, I.},
	year = {1996},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {167--176},
}

@article{citeulike:4779866,
	title = {Using a geographical explanations machine to explore spatial factors relating to primary school performance},
	volume = {5},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1080/13615930120032635},
	doi = {10.1080/13615930120032635},
	abstract = {The development and application of an automated geographical data explorer designed to look for potentially interesting geographical associations in a GIS database without any prior hypotheses as to what to look for or where they may be applied are described. The method is briefly described and then demonstrated via a case study. This case study examines how the geographical variations in primary school performance in northern England can be related to other variables. Finally, suggestions are made for its further development by the addition of a smart search capability.},
	number = {1},
	journal = {Geographical and Environmental Modelling},
	author = {Openshaw, Stan and Turton, Ian},
	year = {2001},
	note = {Publisher: Routledge
tex.citeulike-article-id: 4779866
tex.citeulike-linkout-0: http://dx.doi.org/10.1080/13615930120032635
tex.posted-at: 2009-06-08 18:37:08
tex.priority: 0},
	keywords = {explanation, exploration, gam, gem},
	pages = {85--101},
}

@incollection{Openshaw1995,
	address = {Cambridge, UK},
	title = {A functional specification of a {Unix} based system for accessing the 1991 census {SAR}},
	copyright = {All rights reserved},
	booktitle = {The census users handbook},
	publisher = {GeoInformation International},
	author = {Openshaw, S. and Turton, I.},
	editor = {Openshaw, S.},
	year = {1995},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {253--261},
}

@article{Pellegrino2008,
	title = {Grand challenge award: {Data} integration visualization and collaboration in the {VAST} 2008 {Challenge}},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1109/VAST.2008.4677384},
	doi = {10.1109/VAST.2008.4677384},
	abstract = {The VAST 2008 Challenge consisted of four heterogeneous synthetic data sets each organized into separate mini-challenges. The Grand Challenge required integrating the raw data from these four data sets as well as integrating results and findings from team members working on specific mini-challenges. Modeling the problem with a semantic network provided a means for integrating both the raw data and the subjective findings.},
	journal = {Visual Analytics Science and Technology, 2008. (VAST '08) IEEE Symposium on},
	author = {Pellegrino, D. and Pan, Chi-Chun and Robinson, A. and Stryker, M. and Luo, Junyan and Weaver, C. and Mitra, P. and Chen, Chaomei and Turton, I. and MacEachren, A.},
	year = {2008},
	note = {Publication title: Visual analytics science and technology, 2008. VAST '08. IEEE symposium on
tex.citeulike-article-id: 4237760
tex.citeulike-linkout-0: http://dx.doi.org/10.1109/VAST.2008.4677384
tex.citeulike-linkout-1: http://ieeexplore.ieee.org/xpls/abs{\textbackslash}\_all.jsp?arnumber=4677384
tex.posted-at: 2009-03-30 18:59:53
tex.priority: 0},
	keywords = {data-mining, infovis, visualization},
	pages = {197--198},
}

@article{Openshaw1999a,
	title = {Using the geographical analysis machine to analyze limiting long-term illness},
	volume = {3.1},
	copyright = {All rights reserved},
	journal = {Geographical and Environmental Modelling},
	author = {Openshaw, S. and Turton, I. and Macgill, J},
	year = {1999},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {83--99},
}

@book{citeulike:4369316,
	address = {New York, NY, 10001},
	title = {High performance computing and the art of parallel programming: {An} introduction for geographers, social scientists, and engineers},
	copyright = {All rights reserved},
	isbn = {0-415-15692-0},
	url = {http://portal.acm.org/citation.cfm?id=553188},
	abstract = {From the Publisher: High Perfomance Computing and the Art of Parallel Computing provides a non-technical introduction to High Performance Computing applications together with advice about how beginners can start to write parallel programs. Using case studies to show where HPC has already been used and providing examples for other areas of application to geography, GIS and the social sciences, it provides a plain and practical introduction to a subject that is usually surrounded by complex computer jargon.},
	publisher = {Routledge},
	author = {Openshaw, Stan and Turton, Ian},
	year = {1999},
	note = {tex.citeulike-article-id: 4369316
tex.citeulike-linkout-0: http://portal.acm.org/citation.cfm?id=553188
tex.posted-at: 2009-04-20 16:36:37
tex.priority: 0},
	keywords = {geocomputation, hpc, parallel},
}

@inproceedings{citeulike:595495,
	address = {William and Mary College, Virginia, USA},
	title = {Web-based multi-agent spatial analysis tools},
	copyright = {All rights reserved},
	url = {http://www.geovista.psu.edu/sites/geocomp99/Gc99/069/gc_069.htm},
	booktitle = {{GeoComputation}},
	author = {Macgill, James and Openshaw, Stan and Turton, Ian},
	year = {1999},
	note = {tex.citeulike-article-id: 595495
tex.citeulike-linkout-0: http://www.geovista.psu.edu/sites/geocomp99/Gc99/069/gc{\textbackslash}\_069.htm
tex.posted-at: 2006-04-22 20:56:43
tex.priority: 0},
	keywords = {analysis, boids, clustering, flock, spatial, tool, web-based},
}

@incollection{Koua2010,
	address = {NY},
	title = {Conceptualizing a user-support task structure for geocollaborative disaster management environments},
	copyright = {All rights reserved},
	booktitle = {Information systems for emergency management - advances in management information systems monograph series},
	publisher = {Sharpe},
	author = {Koua, E. and MacEachren, A. and Turton, I. and Pezanowski, S. and Tomaszewski, B. and Frazier, T.},
	editor = {Van de Walle, B. and Turnoff, M. and Hiltz, S.},
	year = {2010},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.25},
}

@article{citeulike:4369297,
	title = {Using computational intelligence techniques to model subglacial water systems},
	volume = {1},
	copyright = {All rights reserved},
	url = {http://dx.doi.org/10.1007/s101090050004},
	doi = {10.1007/s101090050004},
	abstract = {Abstract.\&nbsp;\&nbsp; Measurements of water pressure beneath Trapridge Glacier, Yukon Territory, Canada show that the basal water system is highly heterogeneous. Three types of behaviour were recorded: pressure records which are strongly correlated, records which are strongly anticorrelated, and records which alternate between strong correlation and strong anticorrelation. We take the pressure in bore-holes that are connected to the evacuation route for basal water as the forcing, and the other pressures as the response to this forcing. Previous work (Murray and Clarke 1995) has shown that these relationships can be modelled using low-order nonlinear differential equations optimized by inversion. However, despite optimizing the model parameters we cannot be sure that the final model forms are themselves optimal. Computational intelligence techniques provide alternative methods for fitting models and are robust to missing or noisy data, applicable to non-smooth models, and attempt to derive optimal model forms as well as optimal model parameters. Four computational intelligence techniques have been used and the results compared with the more conventional mathematical model. These methods were genetic programming, artificial neural networks, fuzzy logic and self-organizing maps. We compare each technique and offer an evaluation of their suitability for modelling the pressure data. The evaluation criteria are threefold: (1) goodness of fit and an ability to predict subsequent data under different surface weather conditions; (2) interpretability, and the extent and significance of any new insights offered into the physics of the glacier; (3) computation time. The results suggest that the suitability of the computational intelligence techniques to model these data increases with the complexity of the system to be modelled.},
	number = {1},
	journal = {Journal of Geographical Systems},
	author = {Corne, Simon and Murray, Tavi and Openshaw, Stan and See, Linda and Turton, Ian},
	month = mar,
	year = {1999},
	note = {tex.citeulike-article-id: 4369297
tex.citeulike-linkout-0: http://dx.doi.org/10.1007/s101090050004
tex.citeulike-linkout-1: http://www.springerlink.com/content/ynuer3t1b8jakteh
tex.posted-at: 2009-04-20 16:31:40
tex.priority: 0},
	keywords = {ai, geocomputation},
	pages = {37--60},
}

@incollection{Carver1997,
	address = {London},
	title = {Open spatial decision making: {Evaluation} of the potential of the world wide web},
	copyright = {All rights reserved},
	booktitle = {Innovations in {GIS} 4},
	publisher = {Taylor and Francis},
	author = {Carver, S. and Blake, M. and Turton, I. and Duke-Williams, O.},
	editor = {Kemp, Z.},
	year = {1997},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
}

@inproceedings{Klippel2008,
	address = {Workshop held in conjunction with GIScience 2008, Park City, Utah.},
	title = {Wayfinding choremes 2.0: {Conceptual} primitives as a basis for translating natural into formal language},
	copyright = {All rights reserved},
	booktitle = {Moving objects: {From} natural to formal language},
	author = {Klippel, A. and MacEachren, A. M. and Mitra, P. and Turton, I. and Jaiswal, A. and Soon, K.},
	editor = {van der Weghe, N. and Billen, R. and Kuijpers, B. and Bogaert, P.},
	year = {2008},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
}

@article{Kingston2000,
	title = {Combining a {GIS} with the accessibility of the {Internet} can give a wide audience a say in environmental decision-making},
	volume = {ISSU 5579},
	copyright = {All rights reserved},
	journal = {Surveyor},
	author = {Kingston, R. and Carver, S. and Evans, A. and Turton, I.},
	year = {2000},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {12--15},
}

@techreport{citeulike:3097127,
	title = {Virtual slaithwaite case study report},
	copyright = {All rights reserved},
	url = {http://www.geog.leeds.ac.uk/papers/99-8/},
	institution = {School of Geography, University of Leeds},
	author = {Carver, Steve and Evans, Andy and Kingston, Richard and Turton, Ian},
	year = {1999},
	note = {tex.citeulike-article-id: 3097127
tex.citeulike-linkout-0: http://www.geog.leeds.ac.uk/papers/99-8/
tex.posted-at: 2008-08-07 23:52:23
tex.priority: 0},
	keywords = {geotools, ppgis, webmapping},
}

@article{Carver2000,
	title = {Accessing geographical information systems over the world wide web: {Improving} public participation in environmental decision-making},
	volume = {6.3},
	copyright = {All rights reserved},
	journal = {Information Infrastructure And Policy},
	author = {Carver, S. and Evans, A. and Kingston, R. and Turton, I.},
	year = {2000},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {157--170},
}

@incollection{Ballas2003,
	address = {Netherlands},
	title = {A spatial microsimulation model for social policy evaluation},
	copyright = {All rights reserved},
	booktitle = {Modelling geographical systems},
	publisher = {Kluwer},
	author = {Ballas, D. and Clarke, G.P. and Turton, I.},
	year = {2003},
	note = {tex.owner: ijt1
tex.timestamp: 2010.09.27},
	pages = {143--168},
}

@incollection{klippel_geographic_2008,
	title = {Geographic analysis of linguistically encoded movement patterns},
	url = {http://www.geovista.psu.edu/publications/2008/Klippel_GIScience_08.pdf},
	booktitle = {Geographic information science. 5th international conference, {GIScience} 2008, {Park} {City}, {Utah}, {September} 23-26, 2008},
	author = {Klippel, A. and MacEachren, A M and Mitra, Prasenjit and Turton, Ian and Zhang, Xiao and Jaiswal, Anuj and Soon, Kean and Oyler, Jared and Li, Rui},
	editor = {Cova, Thomas J and Miller, Harvey J and Beard, Kate and Frank, A. U and Goodchild, M. F},
	year = {2008},
	pages = {113--117},
}

@book{klippel_analyzing_2008,
	address = {Park City, Utah},
	title = {Analyzing {Behavioral} {Similarity} {Measures} in {Linguistic} and {Non}-linguistic {Conceptualization} of {Spatial} {Information} and the {Question} of {Individual} {Differences}},
	url = {http://www.cogsci.uni-osnabrueck.de/~isga08/Klippel.pdf},
	author = {Klippel, A. and Weaver, C.},
	year = {2008},
}

@article{wieczorek_point-radius_2004,
	title = {The point-radius method for georeferencing locality descriptions and calculating associated uncertainty},
	volume = {18},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810412331280211},
	abstract = {Natural history museums store millions of specimens of geological, biological, and cultural entities. Data related to these objects are in increasing demand for investigations of biodiversity and its relationship to the environment and anthropogenic disturbance. A major barrier to the use of these data in GIS is that collecting localities have typically been recorded as textual descriptions, without geographic coordinates. We describe a method for georeferencing locality descriptions that accounts for the idiosyncrasies, sources of uncertainty, and practical maintenance requirements encountered when working with natural history collections. Each locality is described as a circle, with a point to mark the position most closely described by the locality description, and a radius to describe the maximum distance from that point within which the locality is expected to occur. The calculation of the radius takes into account aspects of the precision and specificity of the locality description, as well as the map scale, datum, precision and accuracy of the sources used to determine coordinates. This method minimizes the subjectivity involved in the georeferencing process. The resulting georeferences are consistent, reproducible, and allow for the use of uncertainty in analyses that use these data.},
	number = {8},
	journal = {International Journal of Geographical Information Science},
	author = {Wieczorek, John and Guo, Qinghua and Hijmans, Robert},
	year = {2004},
	keywords = {geocoding, georeferencing, uncertainty},
	pages = {745},
}

@inproceedings{li_location_2002,
	title = {Location normalization for information extraction},
	url = {http://dx.doi.org/10.3115/1072228.1072355},
	booktitle = {Proceedings of the 19th international conference on {Computational} linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Li, Huifeng and Srihari, Rohini and Niu, Cheng and Li, Wei},
	year = {2002},
	keywords = {geocoding, information-retrieval, web},
	pages = {7, 1},
}

@article{agnew_gham:_2005,
	title = {{GHAM}: {A} compact global geocode suitable for sorting},
	volume = {31},
	url = {http://dx.doi.org/10.1016/j.cageo.2005.02.007},
	abstract = {The GHAM code is a technique for labeling geographic locations based on their positions. It defines addresses for equal-area cells bounded by constant latitude and longitude, with arbitrarily fine precision. The cell codes are defined by applying Morton ordering to a recursive division into a 16 by 16 grid, with the resulting numbers encoded into letter-number pairs. A lexical sort of lists of points so labeled will bring near neighbors (usually) close together; tests on a variety of global datasets show that in most cases the actual closest point is adjacent in the list 50\% of the time, and within 5 entries 80\% of the time.},
	number = {8},
	journal = {Computers \& Geosciences},
	author = {Agnew, Duncan},
	month = oct,
	year = {2005},
	keywords = {geocoding, global\_grid, morton\_ordering},
	pages = {1047, 1042},
}

@inproceedings{padmanabhan_investigation_2001,
	title = {An investigation of geographic mapping techniques for internet hosts},
	volume = {31},
	isbn = {1-58113-411-8},
	url = {http://dx.doi.org/10.1145/383059.383073},
	booktitle = {{SIGCOMM} '01: {Proceedings} of the 2001 conference on {Applications}, technologies, architectures, and protocols for computer communications},
	publisher = {ACM Press},
	author = {Padmanabhan, Venkata and Subramanian, Lakshminarayanan},
	month = oct,
	year = {2001},
	keywords = {geocoding, internet, map, webmapping},
	pages = {185, 173},
}

@inproceedings{lee_optimization_2003,
	title = {Optimization of geographic area to a {Web} page for two-dimensional range query processing},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1286781},
	abstract = {We have been developing a geographic Web search system to retrieve proper Web pages for a user-specified geographic area. One of location specification methods possible in the system is a two-dimensional range query, which is specified by a rectangle region corresponding to a geographic area. In order to handle such a geographic query, we propose: i) a geographic Web index method which is possible to manage each Web pages by optimized geographic scope; and ii) geometric operations such as 'Contain' as a function to retrieve geographic Web pages. However, recall and precision of search results by raw geometric search operations become very low, if we do not optimize a geographic area to a Web page. By considering of geowords' characterics, we can improve the result by geometric operations in range query processing.},
	booktitle = {Web {Information} {Systems} {Engineering} {Workshops}, 2003. {Proceedings}. {Fourth} {International} {Conference} on},
	author = {Lee, R and Shiina, H and Takakura, H and Kwon, YJ and Kambayashi, Y},
	year = {2003},
	keywords = {geocoding, geographic, geography, georeferencing, information-retrieval, search},
	pages = {17, 9},
}

@inproceedings{jones_spatial_2002,
	title = {Spatial information retrieval and geographical ontologies an overview of the {SPIRIT} project},
	isbn = {1-58113-561-0},
	url = {http://dx.doi.org/10.1145/564376.564457},
	booktitle = {{SIGIR} '02: {Proceedings} of the 25th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Jones, Christopher and Purves, R and Ruas, A and Sanderson, M and Sester, M and van Kreveld, M and Weibel, R},
	year = {2002},
	keywords = {geocoding, information-retrieval, ontology},
	pages = {388, 387},
}

@article{hill_geographic_1999,
	title = {Geographic {Names} {The} {Implementation} of a {Gazetteer} in a {Georeferenced} {Digital} {Library}},
	volume = {5},
	url = {http://dx.doi.org/10.1045/january99-hill},
	abstract = {The Alexandria Digital Library (ADL) Project has developed a content standard for gazetteer objects and a hierarchical type scheme for geographic features. Both of these developments are based on ADL experience with an earlier gazetteer component for the Library, based on two gazetteers maintained by the U.S. federal government. We define the minimum components of a gazetteer entry as (1) a geographic name, (2) a geographic location represented by coordinates, and (3) a type designation. With these attributes, a gazetteer can function as a tool for indirect spatial location identification through names and types. The ADL Gazetteer Content Standard supports contribution and sharing of gazetteer entries with rich descriptions beyond the minimum requirements. This paper describes the content standard, the feature type thesaurus, and the implementation and research issues.},
	number = {1},
	journal = {D-Lib Magazine},
	author = {Hill, Linda and Frew, James and Zheng, Qi},
	month = jan,
	year = {1999},
	keywords = {gazetteer, geocoding},
}

@article{purves_geographic_2006,
	title = {Geographic {Information} {Retrieval} (\{{GIR}\})},
	volume = {30},
	url = {http://dx.doi.org/10.1016/j.compenvurbsys.2005.12.001},
	number = {4},
	journal = {Computers, Environment and Urban Systems},
	author = {Purves, Ross and Jones, Chris},
	month = jul,
	year = {2006},
	keywords = {editorial, geocoding, geographic, gir, information-retrieval},
	pages = {377, 375},
}

@article{arampatzis_web-based_2006,
	title = {Web-based delineation of imprecise regions},
	volume = {30},
	url = {http://dx.doi.org/10.1016/j.compenvurbsys.2005.08.001},
	abstract = {This paper describes several steps in the derivation of boundaries of imprecise regions using the Web as the information source. We discuss how to use the Web to obtain locations that are part of and locations that are not part of the region to be delineated, and then we propose methods to compute the region algorithmically. The methods introduced are evaluated to judge the potential of the approach.},
	number = {4},
	journal = {Computers, Environment and Urban Systems},
	author = {Arampatzis, Avi and van Kreveld, Marc and Reinbacher, Iris and Jones, Christopher and Vaid, Subodh and Clough, Paul and Joho, Hideo and Sanderson, Mark},
	month = jul,
	year = {2006},
	keywords = {geocoding, information-retrieval},
	pages = {459, 436},
}

@article{joho_spirit_2004,
	title = {The \{{SPIRIT}\} collection: an overview of a large web collection},
	volume = {38},
	issn = {0163-5840},
	url = {http://dx.doi.org/10.1145/1041394.1041395},
	number = {2},
	journal = {SIGIR Forum},
	author = {Joho, Hideo and Sanderson, Mark},
	month = dec,
	year = {2004},
	keywords = {document, geocoding, information-retrieval, text},
	pages = {61, 57},
}

@inproceedings{mccurley_geospatial_2001,
	title = {Geospatial mapping and navigation of the web},
	isbn = {1-58113-348-0},
	url = {http://dx.doi.org/10.1145/371920.372056},
	booktitle = {{WWW} '01: {Proceedings} of the 10th international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Mccurley, Kevin},
	year = {2001},
	keywords = {geocoding, geospatial, information-retrieval},
	pages = {229, 221},
}

@article{curtis_spatial_2006,
	title = {Spatial confidentiality and {GIS}: re-engineering mortality locations from published maps about {Hurricane} {Katrina}},
	volume = {5},
	issn = {1476-072X},
	url = {http://dx.doi.org/10.1186/1476-072X-5-44},
	journal = {International Journal of Health Geographics},
	author = {Curtis, Andrew and Mills, Jackie and Leitner, Michael},
	month = oct,
	year = {2006},
	keywords = {disaster, geocoding, location, privacy, spatial},
	pages = {44},
}

@misc{densham_geo-coding_nodate,
	title = {A {Geo}-{Coding} {Service} {Encompassing} a {Geo}-{Parsing} {Tool} and {Integrated} {Digital} {Gazetteer} {Service}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.2363},
	abstract = {We describe a basic Geo-coding service encompassing a geo-parsing tool and integrated digital gazetteer service. The development of a geo-parser comes from the need to explicitly georeference large resource collections such as the Statistical Accounts of Scotland which currently only contain implicit georeferences in the form of placennames thus making such collections inherently geographically searchable.},
	author = {Densham, Ian and Reid, James},
	keywords = {gazetteer, geocoding, geospatial, information-retrieval},
}

@misc{axelrod_building_nodate,
	title = {On {Building} a {High} {Performance} {Gazetteer} {Database}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.2877},
	abstract = {We define a data model for storing geographic information from multiple sources that enables the efficient production of customizable gazetteers. The GazDB separates names from features while storing the relationships between them. Geographic names are stored in a variety of resolutions to allow for i18n and for multiplicity of naming. Geographic features are categorized along several axes to facilitate selection and filtering.},
	author = {Axelrod, Amittai},
	keywords = {database, gazetteer, geocoding, geographic},
}

@article{mehler_spatial_2006,
	title = {Spatial {Analysis} of {News} {Sources}},
	volume = {12},
	abstract = {People in different places talk about different things. This interest distribution is reflected by the newspaper articles
circulated in a particular area. We use data from our large-scale newspaper analysis system (Lydia) to make entity datamaps, a
spatial visualization of the interest in a given named entity. Our goal is to identify entities which display regional biases. We develop
a model of estimating the frequency of reference of an entity in any given city from the reference frequency centered in surrounding
cities, and techniques for evaluating the spatial significance of this distribution.},
	number = {5},
	journal = {IEEE TRANSACTIONSON VISUALIZATIONAND COMPUTER GRAPHICS},
	author = {Mehler, Andrew and Bao, Yunfan and Li, Xin and Wang, Yue and Skiena, Steven},
	year = {2006},
	keywords = {analysis, geocoding, geography, spatial, spatial\_analysis},
}

@incollection{lloyd_lydia:_2005,
	title = {Lydia: {A} {System} for {Large}-{Scale} {News} {Analysis}},
	volume = {377},
	url = {http://dx.doi.org/10.1007/11575832_18},
	booktitle = {: {String} {Processing} and {Information} {Retrieval}},
	author = {Lloyd, Levon and Kechagias, Dimitrios and Skiena, Steven},
	year = {2005},
	keywords = {automated, data-mining, entities, geocoding, information-retrieval, knowledge-discovery, visualization},
	pages = {166, 161},
}

@article{lovasi_comparing_2007,
	title = {Comparing a single-stage geocoding method to a multi-stage geocoding method: how much and where do they disagree?},
	volume = {6},
	issn = {1476-072X},
	url = {http://dx.doi.org/10.1186/1476-072X-6-12},
	journal = {International Journal of Health Geographics},
	author = {Lovasi, Gina and Weiss, Jeremy and Hoskins, Richard and Whitsel, Eric and Rice, Kenneth and Erickson, Craig and Psaty, Bruce},
	month = mar,
	year = {2007},
	keywords = {geocoding, geocomputation, geographic},
	pages = {12},
}

@article{delboni_semantic_2007,
	title = {Semantic {Expansion} of {Geographic} {Web} {Queries} {Based} on {Natural} {Language} {Positioning} {Expressions}},
	volume = {11},
	url = {http://dx.doi.org/10.1111/j.1467-9671.2007.01051.x},
	abstract = {Abstract The need for better Web search tools is getting increasing attention nowadays. About 20\% of the queries currently submitted to search engines include geographic references. Thus, it is particularly important to work with the semantics of such queries, both by understanding the terminology and by recognizing geographic references in natural language text. In this paper, we explore the use of natural language expressions, which we call positioning expressions, to perform geographic searches on the Web, without resorting to geocoded data or gazetteers. Such positioning expressions denote the location of a subject of interest with respect to a landmark. Our approach leads to a query expansion technique that can be explored by virtually any keyword-based search engine. Results obtained in our experiments show an expressive improvement over the traditional keyword-based search and a potential path for tackling many kinds of common geographic queries.},
	number = {3},
	journal = {Transactions in GIS},
	author = {Delboni, Tiago and Borges, Karla and Laender, Alberto and Davis, Clodoveu},
	year = {2007},
	keywords = {entities, gazetteer, geocoding, georeferencing, information-retrieval, search},
	pages = {397, 377},
}

@inproceedings{ahern_world_2007,
	title = {World explorer: visualizing aggregate data from unstructured text in geo-referenced collections},
	isbn = {978-1-59593-644-8},
	url = {http://dx.doi.org/10.1145/1255175.1255177},
	abstract = {The availability of map interfaces and location-aware devices makes a growing amount of unstructured, geo-referenced information available on the Web. This type of information can be valuable not only for browsing, finding and making sense of individual items, but also in aggregate form to help understand data trends and features. In particular, over twenty million geo-referenced photos are now available on Flickr, a photo-sharing website - the first major collection of its kind. These photos are often associated with user-entered unstructured text labels (i.e., tags). We show how we analyze the tags associated with the geo-referenced Flickr images to generate aggregate knowledge in the form of "representative tags" for arbitrary areas in the world. We use these tags to create a visualization tool, World Explorer, that can help expose the content of the data, using a map interface to display the derived tags and the original photo items. We perform a qualitative evaluation of World Explorer that outlines the visualization's benefits in browsing this type of content. We provide insights regarding the aggregate versus individual-item requirements in browsing digital geo-referenced material.},
	booktitle = {{JCDL} '07: {Proceedings} of the 2007 conference on {Digital} libraries},
	publisher = {ACM Press},
	author = {Ahern, Shane and Naaman, Mor and Nair, Rahul and Yang, Jeannie},
	year = {2007},
	keywords = {annotation, collaborative\_tagging, geocoding, location, tagging, web20},
	pages = {10, 1},
}

@misc{pouliquen_geocoding_2006,
	title = {Geocoding multilingual texts: {Recognition}, disambiguation and visualisation},
	url = {http://arxiv.org/abs/cs.CL/0609065},
	abstract = {We are presenting a method to recognise geographical references in free text.
Our tool must work on various languages with a minimum of language-dependent
resources, except a gazetteer. The main difficulty is to disambiguate these
place names by distinguishing places from persons and by selecting the most
likely place out of a list of homographic place names world-wide. The system
uses a number of language-independent clues and heuristics to disambiguate
place name homographs. The final aim is to index texts with the countries and
cities they mention and to automatically visualise this information on
geographical maps using various tools.},
	author = {Pouliquen, Bruno and Kimler, Marco and Steinberger, Ralf and Ignat, Camelia and Oellinger, Tamara and Blackler, Ken and Fuart, Flavio and Zaghouani, Wajdi and Widiger, Anna and Forslund, Ann-Charlotte and Best, Clive},
	year = {2006},
	keywords = {gazetteer, geocoding, georeferencing},
}

@inproceedings{martins_indexing_2005,
	title = {Indexing and ranking in {Geo}-{IR} systems},
	isbn = {1-59593-165-1},
	url = {http://dx.doi.org/10.1145/1096985.1096993},
	booktitle = {{GIR} '05: {Proceedings} of the 2005 workshop on {Geographic} information retrieval},
	publisher = {ACM Press},
	author = {Martins, Bruno and Silva, Mario and Andrade, Leonardo},
	year = {2005},
	keywords = {algorithm, analysis, gazetteer, geocoding, geography},
	pages = {34, 31},
}

@inproceedings{chen_location_2006,
	title = {A location annotation system for personal photos},
	isbn = {1-59593-369-7},
	url = {http://dx.doi.org/10.1145/1148170.1148339},
	booktitle = {{SIGIR} '06: {Proceedings} of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Chen, Chufeng and Oakes, Michael and Tait, John},
	year = {2006},
	keywords = {annotation, geocoding, georeferencing},
	pages = {726, 726},
}

@inproceedings{toyama_geographic_2003,
	title = {Geographic location tags on digital images},
	isbn = {1-58113-722-2},
	url = {http://dx.doi.org/10.1145/957013.957046},
	booktitle = {{MULTIMEDIA} '03: {Proceedings} of the eleventh {ACM} international conference on {Multimedia}},
	publisher = {ACM Press},
	author = {Toyama, Kentaro and Logan, Ron and Roseway, Asta},
	year = {2003},
	keywords = {geocoding, location, photos, tagging},
	pages = {166, 156},
}

@article{hill_core_2000,
	title = {Core {Elements} of {Digital} {Gazetteers}: {Placenames}, {Categories}, and {Footprints}},
	volume = {1923},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.5288},
	abstract = {The core elements of a digital gazetteer are the placename itself, the type

of place it labels, and a geographic footprint representing its location and possibly its

extent. Such gazetteer data is an important component of indirect geographic

referencing through placenames. Based on the gazetteer development work of the

Alexandria Digital Library, this paper presents the nature of placenames, and the

process of assigning categories to places based on the words in the placenames and

other...},
	journal = {Lecture Notes in Computer Science},
	author = {Hill, Linda},
	year = {2000},
	keywords = {gazetteer, geocoding, placenames},
	pages = {??, 280},
}

@inproceedings{rattenbury_towards_2007,
	title = {Towards extracting flickr tag semantics},
	isbn = {978-1-59593-654-7},
	url = {http://dx.doi.org/10.1145/1242572.1242811},
	booktitle = {{WWW} '07: {Proceedings} of the 16th international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Rattenbury, Tye and Good, Nathan and Naaman, Mor},
	year = {2007},
	keywords = {flickr, geocoding, semantic, tags},
	pages = {1288, 1287},
}

@inproceedings{heuer_towards_2007,
	title = {Towards a {Spatial} {Search} {Engine} {Using} {Geotags}},
	url = {http://www.gi-tage.de/downloads/acceptedPapers/heuer.pdf},
	abstract = {We introduce the idea of a spatial search engine based on geotags.
Geotags are keywords linked to a concrete position. User generated
geotags are available from Web 2.0 portals like Flickr1 or Google Maps2.
We collected a sample dataset of about 300.000 geotags. In the following
we explain a prototype implementation of a search engine and describe how
to compute the spatial relevance of a tag. The last section gives an outlook
about our research goals in this area and discusses the challenges and possible
benefits of integrating semantic information.},
	booktitle = {{GI}-{Days} 2007 - {Young} {Researchers} {Forum}},
	publisher = {IfGIprints},
	author = {Heuer, Jan and Dupke, Sören},
	collaborator = {Probst, Florian and Keßler, Carsten},
	year = {2007},
	keywords = {flickr, geocoding, search, spatial},
}

@inproceedings{naaman_automatic_2004,
	title = {Automatic organization for digital photographs with geographic coordinates},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1336098},
	abstract = {We describe PhotoCompas, a system that utilizes the time and location information embedded in digital photographs to automatically organize a personal photo collection. PhotoCompas produces browseable location and event hierarchies for the collection. These hierarchies are created using algorithms that interleave time and location to produce an organization that mimics the way people think about their photo collections. In addition, the algorithm annotates the generated hierarchy with geographical names. We tested our approach in case studies of three real-world collections and verified that the results are meaningful and useful for the collection owners.},
	booktitle = {Digital {Libraries}, 2004. {Proceedings} of the 2004 {Joint} {ACM}/{IEEE} {Conference} on},
	author = {Naaman, M and Song, YJ and Paepcke, A and Garcia-Molina, H},
	year = {2004},
	keywords = {automatic, clustering, geocoding, photos, space-time},
	pages = {62, 53},
}

@inproceedings{kang_geoddupe:_2007,
	title = {{GeoDDupe}: {A} {Novel} {Interface} for {Interactive} {Entity} {Resolution} in {Geospatial} {Data}},
	url = {http://www.cs.umd.edu/projects/linqs/geoddupe/IV07-Kang.pdf},
	abstract = {Due to the growing interest in geospatial data mining
and analysis, data cleaning and integration in geospatial
data is becoming an important issue. Geospatial entity
resolution is the process of reconciling multiple location
references to the same real world location within a single
data source (deduplication) or across multiple data
sources (integration). In this paper, we introduce an
interactive tool called GeoDDupe which effectively
combines automatic data mining algorithms for geospatial
entity resolution with a novel network visualization
supporting users’ resolution analysis and decisions. We
illustrate the GeoDDupe interface with an example
geospatial dataset and show how users can efficiently and
accurately resolve location entities. Finally, the case study
with two real-world geospatial datasets demonstrates the
potential of GeoDDupe.},
	booktitle = {Proceeding of {Information} {Visualisation}},
	author = {Kang, Hyunmo and Sehgal, Vivek and Getoor, Lise},
	month = jul,
	year = {2007},
	keywords = {entities, geocoding, geospatial, visualization},
}

@inproceedings{naaman_context_2004,
	title = {Context data in geo-referenced digital photo collections},
	isbn = {1-58113-893-8},
	url = {http://dx.doi.org/10.1145/1027527.1027573},
	booktitle = {{MULTIMEDIA} '04: {Proceedings} of the 12th annual {ACM} international conference on {Multimedia}},
	publisher = {ACM Press},
	author = {Naaman, Mor and Harada, Susumu and Wang, QianYing and Garcia-Molina, Hector and Paepcke, Andreas},
	year = {2004},
	keywords = {data, geocoding, photos},
	pages = {203, 196},
}

@incollection{schlieder_qualitative_2001,
	title = {Qualitative {Spatial} {Representation} for {Information} {Retrieval} by {Gazetteers}},
	url = {http://dx.doi.org/10.1007/3-540-45424-1_23},
	abstract = {Intelligent and efficient information retrieval becomes increasingly important. Analogous to thesauri in the realm of spatial concepts, gazetteers offer a controlled vocabulary that can be used for spatial queries. Gazetteers use geographic footprints to link place names to geographic locations. Which geographic footprint representation is chosen has a strong impact on the quality of spatial queries. However, the footprint representations currently used in standard gazetteers such as points, lines, grid cell representations, and bounding boxes do not offer enough topological information to support refined spatial queries. We propose a new type of spatial footprint that can be described as a qualitative representation of the spatial decomposition of geographic entities. It holds enough topological and ordinal information enable refined spatial queries without being subject to the constraints of exact polygon representations. The proposed spatial representation was developed to be combined with terminological reasoning techniques used in systems for intelligent information integration.},
	booktitle = {Spatial {Information} {Theory}},
	author = {Schlieder, C and Vögele, T and Visser, U},
	year = {2001},
	keywords = {gazetteer, geocoding, information-retrieval, spatial},
	pages = {351, 336},
}

@inproceedings{chen_efficient_2006,
	title = {Efficient query processing in geographic web search engines},
	isbn = {1-59593-434-0},
	url = {http://dx.doi.org/10.1145/1142473.1142505},
	booktitle = {{SIGMOD} '06: {Proceedings} of the 2006 {ACM} {SIGMOD} international conference on {Management} of data},
	publisher = {ACM Press},
	author = {Chen, Yen-Yu and Suel, Torsten and Markowetz, Alexander},
	year = {2006},
	keywords = {geocoding, geographic, query, search, web},
	pages = {288, 277},
}

@article{jones_geographic_2008,
	title = {Geographic intention and modification in web search},
	volume = {22},
	url = {http://dx.doi.org/10.1080/13658810701626186},
	abstract = {Web searchers signal their geographic intent by using place-names in search queries. They also indicate their flexibility about geographic specificity by reformulating their queries. By examining this data we can learn to understand web searcher flexibility with respect to geographic intent. We examine aggregated data of queries with locations, and locations identified from IP addresses, to identify overall distance preferences, as well as distance preferences by search topic. We also examine query rewriting: both deliberate query rewriting, conducted in web search sessions, and automated query rewriting, with manual relevance judgments of geo-modified queries. We find geo-specification in 12.7\% of user query rewrites in search sessions, and show the breakdown into sub-classes such as same-city, same-state, same-country and different-country. We also measure the dependence between US-state-name and distance-of-modified-location-from-original-location, finding that Vermont web searchers modify their locations greater distances than California web searchers. We find that automatically-modified queries are perceived as much more relevant when the geographic component is unchanged. We look at the relationship between the non-location part of a query and the distance from the user. We see that people search for child day-care near their locations and maps far from where they are located. We also give distance profiles for the top topics which cooccur with place-names in queries, which could be used to set document priors based on document proximity and query topic.},
	number = {3},
	journal = {International Journal of Geographical Information Science},
	author = {Jones, Rosie and Zhang, Wei and Rey, Benjamin and Jhala, Pradhuman and Stipp, Eugene},
	year = {2008},
	keywords = {geocoding, query, search, web},
	pages = {246, 229},
}

@inproceedings{mei_probabilistic_2006,
	title = {A probabilistic approach to spatiotemporal theme pattern mining on weblogs},
	isbn = {1-59593-323-9},
	url = {http://dx.doi.org/10.1145/1135777.1135857},
	booktitle = {{WWW} '06: {Proceedings} of the 15th international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Mei, Qiaozhu and Liu, Chao and Su, Hang and Zhai, ChengXiang},
	year = {2006},
	keywords = {blog, data-mining, geocoding, space-time},
	pages = {542, 533},
}

@inproceedings{pasley_geo-tagging_2007,
	title = {Geo-tagging for imprecise regions of different sizes},
	isbn = {978-1-59593-828-2},
	url = {http://dx.doi.org/10.1145/1316948.1316969},
	booktitle = {{GIR} '07: {Proceedings} of the 4th {ACM} workshop on {Geographical} information retrieval},
	publisher = {ACM},
	author = {Pasley, Robert and Clough, Paul and Sanderson, Mark},
	year = {2007},
	keywords = {geocoding, geography},
	pages = {82, 77},
}

@misc{anderson_spatialml:_2007,
	title = {\{{SpatialML}\}: {Annotation} {Scheme} for {Marking} {Spatial} {Expressions} in {Natural} {Language}},
	url = {http://mirror.optus.net/sourceforge/s/sp/spatialml/SpatialML-1.0-March30-2007.pdf},
	author = {Anderson, Dave and Goldstein-Stewart, Jade and Fayad-Beidas, Amal and Harris, Dave and Herath, Dulip and Hitzeman, Janet and Jang, Seok and Mani, Inderjeet and Pustejovsky, James and Richer, Justin},
	month = mar,
	year = {2007},
	keywords = {geocoding, geospatial, markup, nlp, standards, xml},
}

@article{noauthor_place_2008,
	title = {A place for everything},
	volume = {453},
	url = {http://dx.doi.org/10.1038/453002a},
	number = {7191},
	journal = {Nature},
	month = may,
	year = {2008},
	keywords = {data, editorial, geocoding, scientific},
	pages = {2, 2},
}

@article{jones_modelling_2008,
	title = {Modelling vague places with knowledge from the {Web}},
	volume = {22},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810701850547},
	number = {10},
	journal = {International Journal of Geographical Information Science},
	author = {Jones, CB and Purves, RS and Clough, PD and Joho, H},
	year = {2008},
	keywords = {gazetteer, geocoding, ontology, web},
	pages = {1065, 1045},
}

@article{janowicz_role_2008,
	title = {The role of ontology in improving gazetteer interaction},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810701851461},
	abstract = {Gazetteers are more than basic place name directories containing names and locations for named geographic places. Most of them contain additional information, including a categorization of gazetteer entries using a typing scheme. This paper focuses on the nature of these categorization schemes. We argue that gazetteers can benefit from an ontological approach to typing schemes, providing a formalization that will better support gazetteer applications, maintenance, interoperability, and semi-automatic feature annotation. We discuss the process of developing such an ontology as a modification of an existing feature type thesaurus; the difficulties in mapping from thesauri to ontologies are described in detail. To demonstrate the benefits of a categorization based on ontologies, a new gazetteer Web (and programming) interface is introduced and the impact on gazetteer interoperability is discussed.},
	journal = {International Journal of Geographical Information Science},
	author = {Janowicz, K and Kessler, C},
	year = {2008},
	keywords = {gazetteer, geocoding, geography, ontology, semantic, semanticweb},
	pages = {1157, 1129},
}

@inproceedings{lee_tag-geotag_2008,
	address = {Napa Valley, California, USA},
	title = {Tag-geotag correlation in social networks},
	isbn = {978-1-60558-258-0},
	url = {http://dx.doi.org/10.1145/1458583.1458595},
	abstract = {This paper presents an analysis of the correlation of annotated information unit (textual) tags and geographical identification metadata geotags. Despite the increased usage of geotagging in collaborative tagging systems, most current research focuses on textual tagging alone in solving the tag search problem. This may result in difficulties to search for precise and relevant information within the given tag space. For example, inconsistencies like polysemy, synonyms, and word inflections with plural forms complicate the tag search problem. Therefore, more work needs to be done to include geotag information with existing tagging information for analysis. In this paper, to make geotagging possible to be used in analysis with tagging, we prove that there is a strong correlation between tagging and geotagging information. Our approach uses tag similarity and geographical distribution similarity to determine inter-relationships among tags and geotags. From our initial experiments, we show that the power law is established between tag similarity and geographical distribution similarity: this means that tag similarity and geographical distribution similarity has a strong correlation and the correlation can be used to find more relevant tags in the tag space. The power law confirms that there is an increased relationship between tagging and geotagging and the increased relationship is scalable in size of tags and geotags. Also, using both geotagging and tagging information instead of only tagging, we show that the uncertainty between derived and actual similarities among tags is reduced.},
	booktitle = {{SSM} '08: {Proceeding} of the 2008 {ACM} workshop on {Search} in social media},
	publisher = {ACM},
	author = {Lee, Sang and Won, Dongwoo and McLeod, Dennis},
	year = {2008},
	keywords = {folksonomy, geocoding, social\_network, tagging},
	pages = {66, 59},
}

@inproceedings{rattenbury_towards_2007-1,
	title = {Towards automatic extraction of event and place semantics from flickr tags},
	isbn = {978-1-59593-597-7},
	url = {http://dx.doi.org/10.1145/1277741.1277762},
	booktitle = {{SIRIR} '07: {Proceedings} of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Rattenbury, Tye and Good, Nathaniel and Naaman, Mor},
	year = {2007},
	keywords = {flickr, geocoding, tags},
	pages = {110, 103},
}

@article{christel_interactive_2000,
	title = {Interactive {Maps} for a {Digital} {Video} {Library}},
	volume = {7},
	issn = {1070-986X},
	url = {http://dx.doi.org/10.1109/93.839312},
	abstract = {To improve library access, the Informedia Digital Video Library uses automatic processing to derive descriptors for video. A new extension to the video processing extracts geographic references from these descriptors. The operational library interface shows the geographic entities addressed in a given story, highlighting the regions discussed in the video through a map display synchronized with the video playback. The map can also serve as a query mechanism, allowing users to search the terabyte library for stories taking place in a selected area of interest.},
	number = {1},
	journal = {IEEE MultiMedia},
	author = {Christel, Michael and Olligschlaeger, Andreas and Huang, Chang},
	year = {2000},
	keywords = {geocoding, geography, georeferencing, mapping, multimedia},
	pages = {67, 60},
}

@inproceedings{turton_system_2008,
	address = {Napa Valley, California, USA},
	title = {A system for the automatic comparison of machine and human geocoded documents},
	isbn = {978-1-60558-253-5},
	url = {http://dx.doi.org/10.1145/1460007.1460012},
	abstract = {This paper describes an initial experiment in testing a geocoding system by comparing geocoded documents to locations assigned by human indexers as part of the MeSH indexing process of the PUBMED abstracting system. Preliminary results indicate that this is a useful check on the geocoding system and provides useful feed-back to developers of geocoding systems.},
	booktitle = {{GIR} '08: {Proceeding} of the 2nd international workshop on {Geographic} information retrieval},
	publisher = {ACM},
	author = {Turton, Ian},
	year = {2008},
	keywords = {geocoding, geocomputation, information-retrieval, pubmed},
	pages = {24, 23},
}

@inproceedings{frontiera_flamenco_2008,
	address = {Napa Valley, California, USA},
	title = {Flamenco + {Geo}: extending a hierarchical faceted metadata search interface with geographic capabilities},
	isbn = {978-1-60558-253-5},
	url = {http://dx.doi.org/10.1145/1460007.1460022},
	abstract = {This paper describes initial work on developing a geographic information retrieval system that provides both spatial and text-based functionality. The system is an extension to Flamenco, an open source browse and search interface framework based on hierarchical faceted metadata.},
	booktitle = {{GIR} '08: {Proceeding} of the 2nd international workshop on {Geographic} information retrieval},
	publisher = {ACM},
	author = {Frontiera, Patricia},
	year = {2008},
	keywords = {facets, geocoding, geography},
	pages = {56, 55},
}

@inproceedings{ahlers_retrieving_2008,
	address = {Napa Valley, California, USA},
	title = {Retrieving address-based locations from the web},
	isbn = {978-1-60558-253-5},
	url = {http://dx.doi.org/10.1145/1460007.1460015},
	abstract = {Geospatial search for the Web determines the relation of documents' contents to a location within a region. For some pedestrian scenarios, information at a higher granularity down to individual buildings is necessary. In this paper, we describe a process for the extraction and simultaneous verification of precise addresses on German Web pages by a validating parser. We describe how an address-level location extraction can be aided by an extensive use of previous geographic knowledge and the use of its structure. The analysis of address structure, components and dependencies leads to the design of a geoparser that determines valid addresses within unstructured Web content. We further discuss some noteworthy issues that arise within the process.},
	booktitle = {{GIR} '08: {Proceeding} of the 2nd international workshop on {Geographic} information retrieval},
	publisher = {ACM},
	author = {Ahlers, Dirk and Boll, Susanne},
	year = {2008},
	keywords = {data-mining, geocoding, web},
	pages = {34, 27},
}

@inproceedings{buyukkokten_exploiting_1999,
	title = {Exploiting geographical location information of web pages},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.4835},
	abstract = {Many information resources on the web are relevant primarily to limited geographical communities. For instance, web sites containing information on restaurants, theaters, and apartment rentals are relevant primarily to web users in geographical proximity to these locations. In contrast, other information resources are relevant to a broader geographical community. For instance, an on-line newspaper may be relevant to users across the United States. Unfortunately, the geographical scope of web resources is largely ignored by web search engines. We make the case for identifying and exploiting the geographical location information of web sites so that web search engines can rank resources in a geographically sensitive fashion, in addition to using more traditional information-retrieval strategies. In this paper, we first consider how to compute the geographical location of web pages. Subsequently, we consider how to exploit such information in one specific \&quot;proof-of-concept \&quot; application we implemented in JAVA, and discuss other examples as well. 1},
	booktitle = {In {Proceedings} of the {ACM} {SIGMOD} {Workshop} on the {Web} and {Databases} ({WebDB}’99},
	author = {Buyukkokten, Orkut and Cho, Junghoo and Garcia-Molina, Hector and Gravano, Luis and Shivakumar, Narayanan},
	year = {1999},
	keywords = {geocoding, geographic, information-retrieval, mapping, tagging, web},
	pages = {96, 91},
}

@inproceedings{sengar_robust_2007,
	address = {Seattle, Washington},
	title = {Robust location search from text queries},
	isbn = {978-1-59593-914-2},
	url = {http://dx.doi.org/10.1145/1341012.1341044},
	abstract = {Robust, global, address geocoding is challenging because there is no single address format that applies to all geographies, and in any case, users may not restrict themselves to well-formed addresses. Particularly in online mapping systems, users frequently enter queries with missing or conflicting information, misspellings, address transpositions, and other such variations.},
	booktitle = {{GIS} '07: {Proceedings} of the 15th annual {ACM} international symposium on {Advances} in geographic information systems},
	publisher = {ACM},
	author = {Sengar, Vibhuti and Joshi, Tanuja and Joy, Joseph and Prakash, Samarth and Toyama, Kentaro},
	year = {2007},
	keywords = {addresses, geocoding, geography, text},
	pages = {8, 1},
}

@inproceedings{campelo_geographic_2008,
	address = {Napa Valley, California, USA},
	title = {Geographic scope modeling for web documents},
	isbn = {978-1-60558-253-5},
	url = {http://dx.doi.org/10.1145/1460007.1460010},
	abstract = {Geographic Information Retrieval (GIR) has become a very attractive area of research. GIR is a specialization of a traditional information retrieval system, which may index and search Web documents based on their spatial footprints. Research in this new field may be categorized into crawling spatial-related documents, modeling the geographic scope of a document, indexing these documents using textual and spatial features, and the building of spatially-enabled searching and ranking. This paper presents a method for modeling the geographic scope. The proposed model is based on both the statistics collected from the detected references; and the spatial distribution of the places involved in a given document. This model aims to simplify the indexing and searching processes. Furthermore, the number of spatial operations is reduced, as a consequence the overall performance is improved.},
	booktitle = {{GIR} '08: {Proceeding} of the 2nd international workshop on {Geographic} information retrieval},
	publisher = {ACM},
	author = {Campelo, Cláudio and Baptista, Cláudio},
	year = {2008},
	keywords = {geocoding, gir, information-retrieval, web},
	pages = {18, 11},
}

@inproceedings{sallaberry_fuzzying_2008,
	address = {Napa Valley, California, USA},
	title = {Fuzzying \{{GIS}\} topological functions for \{{GIR}\} needs},
	isbn = {978-1-60558-253-5},
	url = {http://dx.doi.org/10.1145/1460007.1460008},
	abstract = {Natural Language 'schematizes' space; textual geographic information is usually a selection of certain aspects of a referent scene while neglecting others. Thus, an indexing process relying on such information obviously contains some degree of imprecision and uncertainty. The PIV prototype is a GIR system dedicated to geographic evocations tagging, geo-computing, indexing, querying and visualizing in wide corpora of travel books. The aim of this paper is to focus on the PIV spatial relationships management of vagueness for distance, direction and topology relationships. The proposed approach extends GIS operators with fuzzy spatial relationship functions like proximity and cardinal direction.},
	booktitle = {{GIR} '08: {Proceeding} of the 2nd international workshop on {Geographic} information retrieval},
	publisher = {ACM},
	author = {Sallaberry, Christian and Gaio, Mauro and Palacio, Damien and Lesbegueries, Julien},
	year = {2008},
	keywords = {fuzzy, geocoding, gir, gis, topology},
	pages = {8, 1},
}

@inproceedings{crane_challenge_2006,
	title = {The challenge of \{{V}\}irginia \{{B}\}anks: an evaluation of named entity analysis in a 19th-century newspaper collection},
	isbn = {1-59593-354-9},
	url = {http://dx.doi.org/10.1145/1141753.1141759},
	booktitle = {{JCDL} '06: {Proceedings} of the 6th {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {ACM},
	author = {Crane, Gregory and Jones, Alison},
	year = {2006},
	keywords = {geocoding, geography, ner, nlp},
	pages = {40, 31},
}

@article{frontiera_comparison_2008,
	title = {A comparison of geometric approaches to assessing spatial similarity for \{{GIR}\}},
	volume = {22},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810701626293},
	abstract = {This research compares the geographic information retrieval (GIR) performance of a set of logistic regression models with those of five non-probabilistic methods that compute a spatial similarity score for a query-document pair. All methods are applied to a test collection of queries and documents indexed spatially by two convex conservative geometric approximations: the minimum bounding box (MBB) and the convex hull. In the comparison, the tested logistic regression models outperform, in terms of standard information retrieval recall and precision measures, all of the non-probabilistic methods. The retrieval performance achieved by the logistic regression models on MBB approximations is similar to that achieved by the use of the non-probabilistic methods on convex hulls. Although these results are valid only for the test collection used in this study, they suggest that a logistic regression approach to GIR provides an alternative to the use of higher-quality geometric representations that are more difficult to obtain, implement, and process. Additionally, this research demonstrates the ability of a probabilistic approach to effectively incorporate information about geographic context in the spatial ranking process.},
	number = {3},
	journal = {Int. J. Geogr. Inf. Sci.},
	author = {Frontiera, Patricia and Larson, Ray and Radke, John},
	year = {2008},
	keywords = {geocoding, geography, gir},
	pages = {360, 337},
}

@article{yao_spatial_2006,
	title = {Spatial queries with qualitative locations in spatial information systems},
	volume = {30},
	issn = {01989715},
	url = {http://dx.doi.org/10.1016/j.compenvurbsys.2004.08.001},
	abstract = {We discuss locations as defined by their qualitative spatial relations to other features, dubbed qualitative locations (QL). We further propose a mechanism to handle queries with qualitative locations in geospatial information systems. For the realization of the mechanism for QL-based queries, we propose a conceptual framework that takes advantage of models of qualitative spatial reasoning to bridge the gap between conventional metric spatial information systems and the general public’s common-sense query of spatial relations in natural language.},
	number = {4},
	journal = {Computers, Environment and Urban Systems},
	author = {Yao, X and Thill, J},
	month = jul,
	year = {2006},
	keywords = {conceptual, fuzzy, geocoding, gis, location, nlp, qualitative, spatial\_cognition},
	pages = {502, 485},
}

@incollection{weaver_digital_2003,
	title = {A {Digital} {GeoLibrary}: {Integrating} {Keywords} {And} {Place} {Names}},
	url = {http://www.springerlink.com/content/183feg4gj7vag2km},
	abstract = {A digital library typically includes a set of keywords (or subject terms) for each document in its collection(s). For some applications, including natural resource management, geographic location (e.g., the place of a study or a project) is very important. The metadata for such documents needs to indicate the location(s) associated with a document - and users need to be able to search for documents by keyword as well as location. We have developed and implemented a digital library that supports - but does not require - georeferenceable documents (i.e., documents with reference to geography through the use of a textual place name). Because of their implicit spatial footprint, place names benefit from spatial reasoning and querying (e.g., to find all documents that describe work performed within a five-mile radius of a certain point) in addition to traditional keyword-based search. This paper presents the architecture for a digital library that combines spatial reasoning and selection with traditional (non-spatial) search. The contributions of this work are: (1) the use of a traditional geographic information system (GIS) for spatial processing rather than a specially tailored GIS system or a separate gazetteer and (2) the seamless integration of GIS with our thesaurus-based Metadata++ system, so users can easily take advantage of the strengths of both systems.},
	booktitle = {Research and {AdvancedTechnology} for {Digital} {Libraries}},
	author = {Weaver, Mathew and Delcambre, Lois and Shapiro, Leonard and Brewster, Jason and Gutema, Afrem and Tolle, Timothy},
	year = {2003},
	keywords = {digital, geocoding, geography, library, placenames},
	pages = {433, 422},
}

@inproceedings{angel_qualitative_2008,
	address = {Irvine, California},
	title = {Qualitative geocoding of persistent web pages},
	isbn = {978-1-60558-323-5},
	url = {http://dx.doi.org/10.1145/1463434.1463460},
	abstract = {Information and specifically Web pages may be organized, indexed, searched, and navigated using various metadata aspects, such as keywords, categories (themes), and also space. While categories and keywords are up for interpretation, space represents an unambiguous aspect to structure information. The basic problem of providing spatial references to content is solved by geocoding; a task that relates identifiers in texts to geographic co-ordinates. This work presents a methodology for the semiautomatic geocoding of persistent Web pages in the form of collaborative human intervention to improve on automatic geocoding results. While focusing on the Greek language and related Web pages, the developed techniques are universally applicable. The specific contributions of this work are (i) automatic geocoding algorithms for phone numbers, addresses and place name identifiers and (ii) a Web browser extension providing a map-based interface for manual geocoding and updating the automatically generated results. With the geocoding of a Web page being stored as respective annotations in a central repository, this overall mechanism is especially suited for persistent Web pages such as Wikipedia. To illustrate the applicability and usefulness of the overall approach, specific geocoding examples of Greek Web pages are presented.},
	booktitle = {{GIS} '08: {Proceedings} of the 16th {ACM} {SIGSPATIAL} international conference on {Advances} in geographic information systems},
	publisher = {ACM},
	author = {Angel, Albert and Lontou, Chara and Pfoser, Dieter and Efentakis, Alexandros},
	year = {2008},
	keywords = {geocoding, qualitative, web},
	pages = {10, 1},
}

@article{gaio_global_2008,
	title = {A global process to access documents’ contents from a geographical point of view},
	volume = {19},
	issn = {1045926X},
	url = {http://dx.doi.org/10.1016/j.jvlc.2007.08.010},
	abstract = {Local cultural heritage document repositories are characterized by contents strongly attached to a territory (i.e. geographical references). The user must be able to consider such repositories according to a focus, which takes into account his/her geographical interests, and which allows one to access the relevant document's contents from a geographical point of view. This paper presents the Virtual Itineraries in the Pyrenees (PIV) project. Spatial and temporal core models are proposed to give a formal representation of geographical information. The models take into account the characteristics of heterogeneous human modes of expression: written language and captures of drawings, maps, pictures, etc. Semantic processes have been built to automatically manage the spatial and temporal information from non-structured data. A “back office” prototype, which adds these processes to classic information extraction (IE) approaches, while associating a geographical information retrieval (GIR) service is proposed. This service searches for any links between formal representations of geographic information in document collections, and similar representations in a user's information query. Finally, the paper presents the design work, giving the details of the principles of result visualization and navigation, while proposing a “front office” first implementation of the system.},
	number = {1},
	journal = {Journal of Visual Languages \& Computing},
	author = {Gaio, M and Sallaberry, C and Etcheverry, P and Marquesuzaa, C and Lesbegueries, J},
	month = feb,
	year = {2008},
	keywords = {geocoding, geovisualization, gir, information-retrieval, qualitative, text-mining},
	pages = {23, 3},
}

@article{keller_expanding_2008,
	title = {Expanding a {Gazetteer}-based {Approach} for {Geo}-{Parsing} {Text} from {Media} {Reports} on {Global} {Disease} {Outbreaks}},
	volume = {5},
	url = {http://www.isdsjournal.org/article/viewArticle/3289},
	abstract = {Discovering in a text the geographic references
it may contain, is a task that human readers
perform using both their lexical and contextual
knowledge. Using a gazetteer to label such targeted
references in a dataset, this paper proposes
an approach to learning the context in which they
appear and by this means extending the prior
knowledge encoded in the gazetteer. The present
work was carried in the particular framework of a
system for disease outbreak alerts detection and
geo-indexing.},
	number = {3},
	journal = {Advances in Disease Surveillance},
	author = {Keller, Mikaela and Freifeld, Clark and Brownstein, John},
	year = {2008},
	keywords = {disease, gazetteer, geocoding, neuralnetworks, text-mining},
}

@inproceedings{arikawa_geocoding_2005,
	title = {Geocoding {Natural} {Route} {Descriptions} using {Sidewalk} {Network} {Databases}},
	isbn = {0-7695-2414-1},
	url = {http://portal.acm.org/citation.cfm?id=1106240},
	abstract = {People use descriptions referring to places through their daily life. These descriptions correspond to the region of the real world. We call such descriptions geo-referenced descriptions. We have studied a method of converting such descriptions like addresses and place names into their corresponding geographic coordinates, that is, tuples of longitude and latitude. The process of converting descriptions into coordinates is called geocoding. In this paper, we focus on natural route descriptions as a new type of target to geocode. We first propose Formal Route Statement (FRS) to represent and process natural route descriptions by means of a computer, and then explain a core schema of sidewalk network databases on the basis of a characteristic of natural route descriptions. Also, we present our prototype system to geocode natural route descriptions using sidewalk network databases based on our proposed framework.},
	booktitle = {{WIRI} '05: {Proceedings} of the {International} {Workshop} on {Challenges} in {Web} {Information} {Retrieval} and {Integration}},
	publisher = {IEEE Computer Society},
	author = {Arikawa, Masatoshi and Noaki, Kouzou},
	year = {2005},
	keywords = {geocoding, network, nlp, routing},
	pages = {144, 136},
}

@inproceedings{doan_global_2008,
	title = {Global {Health} {Monitor} - {A} {Web}-based {System} for {Detecting} and {Mapping} {Infectious} {Diseases}},
	url = {http://www.aclweb.org/anthology-new/I/I08/I08-2140.pdf},
	booktitle = {Proc. of {IJCNLP08}},
	author = {Doan, Son and Quochung-Ngo and Kawazoe, Ai and Collier, Nigel},
	year = {2008},
	keywords = {disease, geocoding, health, mapping, nlp, rss, text-mining, webmapping},
}

@inproceedings{buscaldi_using_2009,
	title = {Using {GeoWordNet} for {GeographicalInformation} {Retrieval}},
	url = {http://users.dsic.upv.es/~prosso/resources/BuscaldiRosso_GeoCLEF08revised.pdf},
	abstract = {We present a method that uses GeoWordNet for Geographical Information Retrieval. During the indexing phase, all places are disambiguated and assigned their coordinates on the world map. Documents are rst searched for by means of a term-based search method, and then re-ranked according to the geographical information. The results show that map-based re-ranking allows to improve the results obtained by the base system, which relies only on textual information.},
	booktitle = {Revised {Selected} {Papers} {CLEF}-2008,},
	publisher = {Springer-Verlag, LNCS},
	author = {Buscaldi, Davide and Rosso, Paolo},
	year = {2009},
	keywords = {geocoding, georeferencing, gir, wordnet},
}

@article{tomai_qualitative_2005,
	title = {Qualitative {Linguistic} {Terms} and {Geographic} {Concepts}: {Quantifiers} in {Definitions}},
	volume = {9},
	issn = {1361-1682},
	url = {http://dx.doi.org/10.1111/j.1467-9671.2005.00219.x},
	abstract = {Definitions of categories in existent geospatial ontologies are an invaluable source of information because they provide us with essential knowledge about concepts and their properties. A closer examination reveals that definitions also contain supplementary linguistic items, which are mainly qualitative expressions, such as quantifiers. This inclusion of modifiers in definitions affects the way values are assigned to the categories’ properties (semantic properties and relations). This paper introduces a methodology for: (1) representing the essence of qualitative information to clarify the identity relations among categories; and (2) assessing their semantic similarity in order to disambiguate the taxonomic structure of existent geospatial ontologies.},
	number = {3},
	journal = {Transactions in GIS},
	author = {Tomai, Eleni and Kavouras, Marinos},
	month = jun,
	year = {2005},
	keywords = {disambiguation, geographic, linguistic, ontology},
	pages = {290, 277},
}

@inproceedings{zong_assigning_2005,
	title = {On assigning place names to geography related web pages},
	isbn = {1-58113-876-8},
	url = {http://dx.doi.org/10.1145/1065385.1065464},
	booktitle = {{JCDL} '05: {Proceedings} of the 5th {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {ACM Press},
	author = {Zong, Wenbo and Wu, Dan and Sun, Aixin and Lim, Ee-Peng and Goh, Dion},
	year = {2005},
	keywords = {disambiguation, geocoding, geography, web},
	pages = {362, 354},
}

@inproceedings{martins_challenges_2005,
	title = {Challenges and resources for evaluating geographical {IR}},
	isbn = {1-59593-165-1},
	url = {http://dx.doi.org/10.1145/1096985.1097001},
	booktitle = {{GIR} '05: {Proceedings} of the 2005 workshop on {Geographic} information retrieval},
	publisher = {ACM Press},
	author = {Martins, Bruno and Silva, M{\textbackslash}\&{\textbackslash}\#225;rio and Chaves, Marcirio},
	year = {2005},
	keywords = {disambiguation, geocoding, geographic, information-retrieval, search, web},
	pages = {69, 65},
}

@article{montello_wheres_2003,
	title = {Where's {Downtown}?: {Behavioral} {Methods} for {Determining} {Referents} of {Vague} {Spatial} {Queries}},
	volume = {3},
	url = {http://dx.doi.org/10.1207/S15427633SCC032&3_06},
	abstract = {Humans think and talk about regions and spatial relations imprecisely, in terms of vague concepts that are fuzzy or probabilistic (e.g., downtown, near). The functionality of geographic information systems will be increased if they can interpret vague queries. We discuss traditional and newer approaches to defining and modeling spatial queries. Most of the research on vague concepts in information systems has focussed on mathematical and computational implementation. To complement this, we discuss behavioral-science methods for determining the referents of vague spatial terms, particularly vague regions. We present a study of the empirical determination of downtown Santa Barbara. We conclude with a discussion of prospects and problems for integrating vague concepts into geographic information systems.},
	number = {2-3},
	journal = {Spatial Cognition \& Computation},
	author = {Montello, Daniel and Goodchild, Michael and Gottsegen, Jonathon and Fohl, Peter},
	year = {2003},
	keywords = {analysis, artificial\_intelligence, cognitive, disambiguation},
	pages = {204, 185},
}

@article{duckham_formal_2001,
	title = {A formal approach to imperfection in geographic information},
	volume = {25},
	url = {http://dx.doi.org/10.1016/S0198-9715(00)00040-5},
	abstract = {Traditional computational models of geographic phenomena offer no room for imperfection. Underlying this tradition is the simplifying assumption that reality is certain, crisp, unambiguous, independent of context, and capable of quantitative representation. This paper reports on initial work which explicitly recognises that most geographic information is intrinsically imperfect. Based on an ontology of imperfection the paper explores a formal model of imperfect geographic information using multi-valued logic. The development of Java software able to assist with a geodemographic retail site assessment application is used to illustrate the utility of a formal approach.},
	number = {1},
	journal = {Computers, Environment and Urban Systems},
	author = {Duckham, M and Mason, K and Stell, J and Worboys, M},
	month = jan,
	year = {2001},
	keywords = {disambiguation, extraction, geography, gis},
	pages = {103, 89},
}

@inproceedings{smith_disambiguating_2001,
	title = {Disambiguating {Geographic} {Names} in a {Historical} {Digital} {Library}},
	isbn = {3-540-42537-3},
	url = {http://portal.acm.org/citation.cfm?id=699911},
	booktitle = {{ECDL} '01: {Proceedings} of the 5th {European} {Conference} on {Research} and {Advanced} {Technology} for {Digital} {Libraries}},
	publisher = {Springer-Verlag},
	author = {Smith, David and Crane, Gregory},
	year = {2001},
	keywords = {disambiguation, geocoding},
	pages = {136, 127},
}

@inproceedings{wang_detecting_2005,
	title = {Detecting dominant locations from search queries},
	isbn = {1-59593-034-5},
	url = {http://dx.doi.org/10.1145/1076034.1076107},
	booktitle = {{SIGIR} '05: {Proceedings} of the 28th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Wang, Lee and Wang, Chuang and Xie, Xing and Forman, Josh and Lu, Yansheng and Ma, Wei-Ying and Li, Ying},
	year = {2005},
	keywords = {disambiguation, geocoding, search},
	pages = {431, 424},
}

@inproceedings{wang_web_2005,
	title = {Web resource geographic location classification and detection},
	isbn = {1-59593-051-5},
	url = {http://dx.doi.org/10.1145/1062745.1062907},
	booktitle = {{WWW} '05: {Special} interest tracks and posters of the 14th international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Wang, Chuang and Xie, Xing and Wang, Lee and Lu, Yansheng and Ma, Wei-Ying},
	year = {2005},
	keywords = {disambiguation, geocoding},
	pages = {1139, 1138},
}

@inproceedings{clough_extracting_2005,
	title = {Extracting metadata for spatially-aware information retrieval on the internet},
	isbn = {1-59593-165-1},
	url = {http://dx.doi.org/10.1145/1096985.1096992},
	booktitle = {{GIR} '05: {Proceedings} of the 2005 workshop on {Geographic} information retrieval},
	publisher = {ACM Press},
	author = {Clough, Paul},
	year = {2005},
	keywords = {disambiguation, geocoding, information, retrieval},
	pages = {30, 25},
}

@inproceedings{wang_detecting_2005-1,
	title = {Detecting geographic locations from web resources},
	isbn = {1-59593-165-1},
	url = {http://dx.doi.org/10.1145/1096985.1096991},
	booktitle = {{GIR} '05: {Proceedings} of the 2005 workshop on {Geographic} information retrieval},
	publisher = {ACM Press},
	author = {Wang, Chuang and Xie, Xing and Wang, Lee and Lu, Yansheng and Ma, Wei-Ying},
	year = {2005},
	keywords = {disambiguation, geocoding, gir, information, retrieval},
	pages = {24, 17},
}

@incollection{martins_assigning_2005,
	title = {Assigning {Geographical} {Scopes} {To} {Web} {Pages}},
	volume = {3408},
	url = {http://dx.doi.org/10.1007/b107096},
	abstract = {Finding automatic ways of attaching geographical scopes to on-line resources, also called "geo-referencing" documents, is a challenging problem, getting increasing attention [1,5,3]. Here we present a system architecture and a process for identifying the geographical scope of Web pages, defining a scope as the region where more people than average would find that page relevant. We rely on typical Web IR heuristics (i.e. feature weighting, hypertext topic locality, anchor description) and assumptions on how people use geographical references in documents. The method involves three major steps. First, geographical named entities are identified in the text. Next, we propagate the found named entities through the Web linkage graph. Finally, a geographical ontology is used to disambiguate among the named entities associated to a document, this way selecting the most likely scope. In the future, we plan on using scopes in new location-aware search tools.},
	booktitle = {Advances in {Information} {Retrieval}},
	publisher = {Springer Berlin / Heidelberg},
	author = {Martins, Bruno and Chaves, Marcirio and Silva, Mario},
	collaborator = {Losada, David and Fernández-Luna, Juan},
	year = {2005},
	keywords = {disambiguation, geocoding, information, retrieval},
	pages = {564},
}

@inproceedings{delboni_geographic_2005,
	title = {Geographic web search based on positioning expressions},
	isbn = {1-59593-165-1},
	url = {http://dx.doi.org/10.1145/1096985.1097000},
	booktitle = {{GIR} '05: {Proceedings} of the 2005 workshop on {Geographic} information retrieval},
	publisher = {ACM Press},
	author = {Delboni, Tiago and Borges, Karla and Laender, Alberto},
	year = {2005},
	keywords = {disambiguation, geocoding, gir, information, retrieval},
	pages = {64, 61},
}

@article{schutze_automatic_1998,
	title = {Automatic {Word} {Sense} {Discrimination}},
	volume = {24},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.6026},
	abstract = {This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the...},
	number = {1},
	journal = {Computational Linguistics},
	author = {Schütze, Hinrich},
	year = {1998},
	keywords = {disambiguation, information-retrieval, search, web},
	pages = {123, 97},
}

@misc{gale_discrimination_nodate,
	title = {Discrimination {Decisions} for 100,000-{Dimensional} {Spaces}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.8646},
	abstract = {Discrimination decisions arise in many natural language processing tasks. Three classical tasks are discriminating texts by their authors (author identification), discriminating documents by their relevance to some query (information retrieval), and discriminating multi-meaning words by their meanings (sense discrimination). Many other discrimination tasks arise regularly, such as determining whether a particular proper noun represents a person or a place, or whether a given word from some...},
	author = {Gale, William and Church, Kenneth and Yarowsky, David},
	keywords = {disambiguation, entities, geocoding, information-retrieval, statistical},
}

@inproceedings{han_two_2004,
	title = {Two supervised learning approaches for name disambiguation in author citations},
	isbn = {1-58113-832-6},
	url = {http://dx.doi.org/10.1145/996350.996419},
	booktitle = {Proceedings of the 4th {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {ACM Press},
	author = {Han, Hui and Giles, Lee and Zha, Hongyuan and Li, Cheng and Tsioutsiouliklis, Kostas},
	year = {2004},
	keywords = {disambiguation, entities, knowledge-management},
	pages = {305, 296},
}

@misc{vestavik_geographic_nodate,
	title = {Geographic {Information} {Retrieval}: {An} {Overview}},
	url = {http://www.idi.ntnu.no/~oyvindve/article.pdf},
	abstract = {Geographic Information Retrieval can be seen as a
specialized branch of traditional Information Retrieval. It
includes all of the research areas that have traditionally
made up the core of research into Information Retrieval,
but in addition has an emphasis on spatial and geographic
indexing and retrieval. This article present some of the
challenges in Geographic Information Retrieval like the
ambiguities involved in the use of place names and phrases
linking a document to a location and the influence of
human perception of and reasoning about space. It also
explaines the role of gazetteers, thesauri and ontologies of
place names in Geographic Information Retrieval and
reviews some relevant projects.},
	author = {Vestavik, Øyvind},
	keywords = {disambiguation, geocoding, geographic, information-retrieval},
}

@article{silva_adding_2006,
	title = {Adding geographic scopes to web resources},
	volume = {30},
	url = {http://dx.doi.org/10.1016/j.compenvurbsys.2005.08.003},
	abstract = {Many web pages are rich in geographic information and primarily relevant to geographically limited communities. However, existing IR systems only recently began to offer local services and largely ignore geo-spatial information. This paper presents our work on automatically identifying the geographical scope of web documents, which provides the means to develop retrieval tools that take the geographical context into consideration. Our approach makes extensive use of an ontology of geographical concepts, and includes a system architecture for extracting geographic information from large collections of web documents. The proposed method involves recognising geographical references over the documents and assigning geographical scopes through a graph ranking algorithm. Initial evaluation results are encouraging, indicating the viability of this approach.},
	number = {4},
	journal = {Computers, Environment and Urban Systems},
	author = {Silva, Mario and Martins, Bruno and Chaves, Marcirio and Afonso, Ana and Cardoso, Nuno},
	month = jul,
	year = {2006},
	keywords = {disambiguation, geocoding, information-retrieval},
	pages = {399, 378},
}

@article{leidner_evaluation_2006,
	title = {An evaluation dataset for the toponym resolution task},
	volume = {30},
	url = {http://dx.doi.org/10.1016/j.compenvurbsys.2005.07.003},
	abstract = {Toponym resolution is the task of linking place name instances in a text with spatial footprints, given the context in which they occur. Whereas a lot of work on the evaluation of temporal resolution is ongoing (e.g. [Setzer, A., \& Gaizauskas, R. (2000). On the importance of annotating temporal event-event relations in text. In LREC 2000 Workshop on annotation standards for temporal information in natural language, Vol. 3 (pp. 1281-1286). Athens, Greece]), to date no reference resource is available to evaluate competing algorithms for toponym resolution. It is thus argued that a shareable, reusable evaluation resource is necessary.To this end, a new proposal for the markup of toponyms in text corpora with their referents and an associated tool data methodology are presented: the Toponym Resolution Markup Language (TRML) is an XML-based markup language, and TAME, the toponym annotation markup editor, is a tool that implements it. A novel evaluation resource is described which comprises a large-scale reference gazetteer server and a human-annotated news corpus in which toponyms are associated with latitude/longitude coordinates of the location they refer to. The reliability of the annotation task is established by determining inter-annotator agreement of the human annotators.},
	number = {4},
	journal = {Computers, Environment and Urban Systems},
	author = {Leidner, Jochen},
	month = jul,
	year = {2006},
	keywords = {disambiguation, gazetteer, geocoding},
	pages = {417, 400},
}

@article{naaman_assigning_2006,
	title = {Assigning textual names to sets of geographic coordinates},
	volume = {30},
	url = {http://dx.doi.org/10.1016/j.compenvurbsys.2006.02.001},
	abstract = {NameSet is a system that translates a set of geographic coordinates into a textual name based on the geographic regions where the coordinates occur. One possible application of NameSet is to concisely present the geographical scope of a set of geo-referenced observations to a human user. Another application is to generate text to depict a set of coordinates that appear on a web site--text that could later be used for information retrieval applications. NameSet's computation is based on a simple algorithm, using off-the-shelf and web-based data sources. The system was proven effective in an application that automatically organizes and names sets of geo-referenced digital photographs.},
	number = {4},
	journal = {Computers, Environment and Urban Systems},
	author = {Naaman, Mor and Song, Yee and Paepcke, Andreas and Garcia-Molina, Hector},
	month = jul,
	year = {2006},
	keywords = {disambiguation, geocoding},
	pages = {435, 418},
}

@misc{thierry_geographic_nodate,
	title = {Geographic {Reference} {Analysis} for {Geographic} {Document} {Querying}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.2449},
	abstract = {Retrieval from geographical documents,
i.e. documents with a major geographic
component. The final aim, in response to an
informational query of the user, is to return a
ranked list of relevant passages in selected documents,
allowing text browsing within them.
We consider in this paper the spatial component
of the texts and the queries. The idea is to perform
an off-line linguistic analysis of the document,
extracting spatial expressions (i.e. expressions
denoting geographical localisations)....},
	author = {Thierry, Frdrik},
	keywords = {automated, disambiguation, geocoding, georeferencing, information-retrieval, query},
}

@misc{southall_defining_nodate,
	title = {Defining and {Identifying} the {Roles} of {Geographic} {References} {Within} {Text}: {Examples} {From} {The} {Great} {Britain} {GIS} {Project}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.3.2375},
	abstract = {Reliably recognizing, disambiguating,
normalizing, storing, and displaying
geographic names poses many challenges.
However, associating each name with a
geographical point location cannot be the final
stage. We also need to understand each nameÕs
role within the document, and its association
with adjacent text. The paper develops these
points through a discussion of two different
types of historical texts, both rich in
geographic names: descriptive gazetteer entries
and travellersÕ narratives. It concludes by
discussing the limitations of existing mark-up
systems in this area.},
	author = {Southall, Humphrey},
	keywords = {data-mining, disambiguation, gazetteer, geocoding, geographic, geospatial, information-retrieval},
}

@inproceedings{rauch_confidence-based_2003,
	title = {A confidence-based framework for disambiguating geographic terms},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.5.3619},
	abstract = {We describe a purely confidence-based geographic term disambiguation system that crucially relies on the notion of "positive" and "negative" context and methods for combining confidence-based disambiguation with measures of relevance to a user's query.},
	booktitle = {{HLT}-{NAACL} 2003 {Workshop} on {Analysis} of {Geographic} {References}},
	author = {Rauch, E and Bukatin, M and Baker, K},
	year = {2003},
	keywords = {disambiguation, gazetteer, geocoding, geographic},
}

@misc{uryupina_semi-supervised_nodate,
	title = {Semi-{Supervised} {Learning} of {Geographical} {Gazetteers} {From} the {Internet}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.7816},
	abstract = {In this paper we present an approach to the acquisition of geographical gazetteers. Instead of creating these resources manually, we propose to extract gazetteers from the World Wide Web, using Data Mining techniques.},
	author = {Uryupina, Olga},
	keywords = {ai, disambiguation, gazetteer, geocoding, geographic},
}

@inproceedings{han_name_2005,
	title = {Name disambiguation in author citations using a {K}-way spectral clustering method},
	isbn = {1-58113-876-8},
	url = {http://dx.doi.org/10.1145/1065385.1065462},
	booktitle = {{JCDL} '05: {Proceedings} of the 5th {ACM}/{IEEE}-{CS} joint conference on {Digital} libraries},
	publisher = {ACM Press},
	author = {Han, Hui and Zha, Hongyuan and Giles, Lee},
	year = {2005},
	keywords = {citations, disambiguation, document, information-retrieval, knowledge-management, publishing},
	pages = {343, 334},
}

@inproceedings{bagga_entity-based_1998,
	title = {Entity-based cross-document coreferencing using the {Vector} {Space} {Model}},
	url = {http://portal.acm.org/citation.cfm?id=980859},
	booktitle = {Proceedings of the 17th international conference on {Computational} linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Bagga, Amit and Baldwin, Breck},
	year = {1998},
	keywords = {citations, data-mining, disambiguation},
	pages = {85, 79},
}

@inproceedings{volz_towards_2007,
	title = {Towards ontology based disambiguation of geographical identifiers},
	url = {http://raphaelvolz.de/blog/wp-content/uploads/2007/03/paper.pdf},
	booktitle = {{WWW2007}},
	author = {Volz, Raphael and Kleb, Joachim and Mueller, Wolfgang},
	month = may,
	year = {2007},
	keywords = {disambiguation, geocoding, information-retrieval, knowledge-discovery, ontology, semantic},
}

@misc{manov_experiments_nodate,
	title = {Experiments with {Geographic} {Knowledge} for {Information} {Extraction}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.1975},
	abstract = {Here we present work on using spatial knowledge

in conjunction with information extraction

(IE). Considerable volume of location data

was imported in a knowledge base (KB) with

entities of general importance used for semantic

annotation, indexing, and retrieval of text.},
	author = {Manov, Dimitar and Kiryakov, Atanas and Popov, Borislav and Bontcheva, Kalina and Maynard, Diana and Cunningham, Hamish},
	keywords = {disambiguation, gazetteer, geocoding, ontology},
}

@misc{le_grand_visualisation_nodate,
	title = {Visualisation of the {Semantic} {Web}: {Topic} {Maps} {Visualisation}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.873},
	abstract = {knowledge representation and information management
by building a structured semantic network above
information resources. Our research at LIP6 aims at
visualizing this semantic layer efficiently, which is a
critical issue as Topic Maps may contain millions of
elements.},
	author = {Le Grand, Bénédicte and Soto, Michel},
	keywords = {classification, clustering, disambiguation, entities, infovis, openstandards, semanticweb, visualization},
}

@incollection{widdows_mathematical_2003,
	title = {A {Mathematical} {Model} for {Context} and {Word}-{Meaning}},
	url = {http://www.springerlink.com/content/hl2c6gmc3jj6w1nl},
	abstract = {Context is vital for deciding which of the possible senses of a word is being used in a particular situation, a task known as disambiguation. Motivated by a survey of disambiguation techniques in natural language processing, this paper presents a mathematical model describing the relationship between words, meanings and contexts, giving examples of how context-groups can be used to distinguish different senses of ambiguous words. Many aspects of this model have interesting similarities with quantum theory.},
	booktitle = {Modeling and {Using} {Context}},
	author = {Widdows, Dominic},
	year = {2003},
	keywords = {context, disambiguation, model},
	pages = {1027, 1027},
}

@inproceedings{nadeau_unsupervised_2006,
	title = {Unsupervised {Named}-{Entity} {Recognition}: {Generating} {Gazetteers} and {Resolving} {Ambiguity}},
	url = {http://cogprints.org/5025/},
	abstract = {In this paper, we propose a named-entity recognition (NER) system that addresses two major limitations frequently discussed in the field. First, the system requires no human intervention such as manually labeling training data or creating gazetteers. Second, the system can handle more than the three classical named-entity types (person, location, and organization). We describe the system’s architecture and compare its performance with a supervised system. We experimentally evaluate the system on a standard corpus, with the three classical named-entity types, and also on a new corpus, with a new named-entity type (car brands).},
	booktitle = {Proc. {Canadian} {Conference} on {Artificial} {Intelligence}.},
	author = {Nadeau, David and Turney, Peter and Matwin, Stan},
	year = {2006},
	keywords = {disambiguation, entities, gazetteer},
}

@inproceedings{turton_geographic_2007,
	title = {Geographic {Information} {Retrieval} from {Disparate} {Data} {Sources}},
	url = {http://ncg.nuim.ie/geocomputation/sessions/3B/3B5.pdf},
	booktitle = {{GeoComputation}'07},
	author = {Turton, Ian and Gahegan, Mark and Jaiswal, Anuj},
	year = {2007},
	keywords = {algorithm, disambiguation, exploration, gate, geocoding, geocomputation, nlp, pubmed},
}

@article{mani_disambiguating_2005,
	title = {Disambiguating {Toponyms} in {News}},
	url = {http://dx.doi.org/10.3115/1220575.1220621},
	abstract = {This research is aimed at the problem of disambiguating toponyms (place names) in terms of a classification derived by merging information from two publicly available gazetteers. To establish the difficulty of the problem, we measured the degree of ambiguity, with respect to a gazetteer, for toponyms in news. We found that 67.82\% of the toponyms found in a corpus that were ambiguous in a gazetteer lacked a local discriminator in the text. Given the scarcity of human annotated data, our method used unsuper- vised machine learning to develop disam- biguation rules. Toponyms were automatically tagged with information about them found in a gazetteer. A toponym that was ambiguous in the gazetteer was automatically disambiguated based on preference heuristics. This automatically tagged data was used to train a machine learner, which disambiguated toponyms in a human-annotated news corpus at 78.5\% accuracy.},
	journal = {HLT '05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing},
	author = {Mani, Inderjeet and Garbin, Eric},
	year = {2005},
	keywords = {data-mining, disambiguation, geocoding, news},
	pages = {370, 363},
}

@misc{li_infoxtract_2003,
	title = {{InfoXtract} location normalization: a hybrid approach to geographic references in information extraction},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.3.6065},
	abstract = {Ambiguity is very high for location names. For

example, there are 23 cities named `Buffalo' in the

U.S. Based on our previous work, this paper presents

a refined hybrid approach to geographic references

using our information extraction engine InfoXtract.},
	author = {Li, H and Srihari, K and Niu, C and Li, W},
	year = {2003},
	keywords = {disambiguation, geocoding, geography, information},
}

@inproceedings{leidner_grounding_2003,
	title = {Grounding spatial named entities for information extraction and question answering},
	url = {http://dx.doi.org/10.3115/1119394.1119399},
	booktitle = {Proceedings of the {HLT}-{NAACL} 2003 workshop on {Analysis} of geographic references},
	publisher = {Association for Computational Linguistics},
	author = {Leidner, Jochen and Sinclair, Gail and Webber, Bonnie},
	year = {2003},
	keywords = {disambiguation, geocoding, information-retrieval, nlp, spatial},
	pages = {38, 31},
}

@misc{clough_proposal_2004,
	title = {A proposal for comparative evaluation of automatic annotation for geo-referenced documents},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.1.4524},
	abstract = {INTRODUCTION

Organised evaluation campaigns such as the Text Retrieval
Conference (TREC) for information retrieval [9], the Document
Understanding Conference (DUC) for document summarization
[7], and the Message Understanding Conference (MUC) for
information extraction [3] have not only proven to be an
important and effective stimulus for research, but also served to
bring together members of the academic and industrial research
communities. These campaigns have resulted in large-scale...},
	author = {Clough, P and Sanderson, M},
	year = {2004},
	keywords = {annotation, disambiguation, evaluation, geocoding},
}

@misc{leidner_toponym_2006,
	title = {Toponym {Resolution}: {A} {First} {Large}-{Scale} {Comparative} {Evaluation}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.7200},
	abstract = {Toponym resolution (TR) is the task of mapping the name of a location to a spatial representation of the location referred to, such as the centroid of the location, given as latitude/longitude. While a number of systems for automating the task have been described in the literature, to date no comparative evaluation study has existed, mainly for lack of a standard benchmark (i.e., gazetteer and evaluation corpus).},
	author = {Leidner, Jochen},
	year = {2006},
	keywords = {disambiguation, evaluation, geocoding},
}

@inproceedings{leidner_toponym_2004,
	title = {Toponym resolution in text: "which \{{S}\}heffield is it?"},
	isbn = {1-58113-881-4},
	url = {http://dx.doi.org/10.1145/1008992.1009147},
	booktitle = {{SIGIR} '04: {Proceedings} of the 27th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Leidner, Jochen},
	year = {2004},
	keywords = {disambiguation, geocoding},
	pages = {602, 602},
}

@inproceedings{hays_im2gps:_2008,
	title = {im2gps: estimating geographic information from a single image},
	url = {http://graphics.cs.cmu.edu/projects/im2gps/im2gps.pdf},
	booktitle = {Proceedings of the \{{IEEE}\} {Conf}. on {Computer} {Vision} and {Pattern} {Recognition} (\{{CVPR}\})},
	author = {Hays, James and Efros, Alexei},
	year = {2008},
	keywords = {data-mining, disambiguation, flickr, geocoding, geography},
}

@incollection{asadi_pattern-based_2008,
	title = {Pattern-{Based} {Extraction} of {Addresses} from {Web} {Page} {Content}},
	url = {http://dx.doi.org/10.1007/978-3-540-78849-2_41},
	abstract = {Extraction of addresses and location names from Web pages is a challenging task for search engines. Traditional information extraction and natural processing models remain unsuccessful in the context of the Web because of the uncontrolled heterogenous nature of the Web resources as well as the effects of HTML and other markup tags. We describe a new pattern-based approach for extraction of addresses from Web pages. Both HTML and vision-based segmentations are used to increase the quality of address extraction. The proposed system uses several address patterns and a small table of geographic knowledge to hit addresses and then itemize them into smaller components. The experiments show that this model can extract and itemize different addresses effectively without large gazetteers or human supervision.},
	booktitle = {Progress in {WWW} {Research} and {Development}},
	author = {Asadi, Saeid and Yang, Guowei and Zhou, Xiaofang and Shi, Yuan and Zhai, Boxuan and Jiang, Wendy},
	year = {2008},
	keywords = {disambiguation, geocoding, ner, text, text-mining, web},
	pages = {418, 407},
}

@inproceedings{mikheev_knowledge-free_1999,
	address = {College Park, Maryland},
	title = {A knowledge-free method for capitalized word disambiguation},
	isbn = {1558606093},
	url = {http://dx.doi.org/10.3115/1034678.1034710},
	booktitle = {Proceedings of the 37th annual meeting of the {Association} for {Computational} {Linguistics} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Mikheev, Andrei},
	year = {1999},
	keywords = {disambiguation, ner, nlp, text, text-mining},
	pages = {166, 159},
}

@article{lee_exploring_2007,
	title = {Exploring phrasal context and error correction heuristics in bootstrapping for geographic named entity annotation},
	volume = {32},
	url = {http://dx.doi.org/10.1016/j.is.2006.03.001},
	abstract = {Geographic named entities can be classified into many sub-types that are useful for applications such as information extraction and question answering. In this paper, we present a high-performance bootstrapping algorithm with error correction heuristics and location normalization for the task of geographic named entity annotation with seven sub-types. Location normalization additionally resolves ambiguities of entities with same name and sub-types. In the initial stage, we annotate a raw corpus using a large set of seeds which is automatically selected from a gazetteer so that its quality does not depend on a specific training corpus. From the initial annotation, boundary patterns reflecting phrasal context are learned and applied to the corpus again to obtain new annotation which passes through error correction heuristics. As the bootstrapping loop proceeds, the annotated instances are gradually increased and the learned boundary patterns become gradually richer and more accurate. Through experiments, we explore inter/intra-phrasal context which reflects syntactic constraints of a named entity and several heuristic knowledge for correcting annotation errors introduced by incomplete boundary patterns. The experiments show the effect of the strategies on the learning curve. When our bootstrapping approach was applied to a newspaper corpus, it could achieve 89 F1 value. And the method suggested for location normalization could achieve 95\% accuracy at instance level.},
	number = {4},
	journal = {Information Systems},
	author = {Lee, Seungwoo and Lee, Gary},
	month = jun,
	year = {2007},
	keywords = {disambiguation, geocoding, ner, nlp, text, text-mining},
	pages = {592, 575},
}

@inproceedings{smith_bootstrapping_2003,
	title = {Bootstrapping toponym classifiers},
	url = {http://dx.doi.org/10.3115/1119394.1119401},
	booktitle = {Proceedings of the {HLT}-{NAACL} 2003 workshop on {Analysis} of geographic references},
	publisher = {Association for Computational Linguistics},
	author = {Smith, David and Mann, Gideon},
	year = {2003},
	keywords = {disambiguation, ner, nlp, text, text-mining},
	pages = {49, 45},
}

@inproceedings{cimiano_gimme_2005,
	title = {Gimme' the context: context-driven automatic semantic annotation with {C}-{PANKOW}},
	isbn = {1-59593-046-9},
	url = {http://dx.doi.org/10.1145/1060745.1060796},
	booktitle = {{WWW} '05: {Proceedings} of the 14th international conference on {World} {Wide} {Web}},
	publisher = {ACM Press},
	author = {Cimiano, Philipp and Ladwig, G{\textbackslash}\&{\textbackslash}\#252;nter and Staab, Steffen},
	year = {2005},
	keywords = {disambiguation, geocoding, ner, nlp, text, text-mining},
	pages = {341, 332},
}

@inproceedings{yarowsky_unsupervised_1995,
	title = {Unsupervised word sense disambiguation rivaling supervised methods},
	url = {http://dx.doi.org/10.3115/981658.981684},
	booktitle = {Proceedings of the 33rd annual meeting on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Yarowsky, David},
	year = {1995},
	keywords = {disambiguation, ner, nlp, text, text-mining},
	pages = {196, 189},
}

@article{bikel_algorithm_1999,
	title = {An {Algorithm} that {Learns} {What}'s in a {Name}},
	volume = {34},
	url = {http://dx.doi.org/10.1023/A:1007558221122},
	abstract = {In this paper, we present IdentiFinderTM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder's performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90\% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.},
	number = {1},
	journal = {Machine Learning},
	author = {Bikel, Daniel and Schwartz, Richard and Weischedel, Ralph},
	month = feb,
	year = {1999},
	keywords = {disambiguation, ner, nlp, text, text-mining},
	pages = {231, 211},
}

@article{sanderson_retrieving_2000,
	title = {Retrieving with {Good} {Sense}},
	volume = {2},
	issn = {1386-4564},
	url = {http://dx.doi.org/10.1023/A:1009933700147},
	abstract = {Although always present in text, word sense ambiguity only recently became regarded as a problem
to information retrieval which was potentially solvable. The growth of interest in word senses resulted from new
directions taken in disambiguation research. This paper first outlines this research and surveys the resulting efforts
in information retrieval. Although the majority of attempts to improve retrieval effectiveness were unsuccessful,
much was learnt from the research. Most notably a notion of under what circumstance disambiguation may prove
of use to retrieval.},
	number = {1},
	journal = {Inf. Retr.},
	author = {Sanderson, Mark},
	month = feb,
	year = {2000},
	keywords = {disambiguation, ner, nlp, text, text-mining},
	pages = {69, 49},
}

@article{cucerzan_large-scale_2007,
	title = {Large-{Scale} {Named} {Entity} {Disambiguation} {Based} on {Wikipedia} {Data}},
	url = {http://acl.ldc.upenn.edu/D/D07/D07-1074.pdf},
	abstract = {This paper presents a large-scale system for the
recognition and semantic disambiguation of
named entities based on information extracted
from a large encyclopedic collection and Web
search results. It describes in detail the disambiguation
paradigm employed and the information
extraction process from Wikipedia. Through a
process of maximizing the agreement between the
contextual information extracted from Wikipedia
and the context of a document, as well as the
agreement among the category tags associated
with the candidate entities, the implemented system
shows high disambiguation accuracy on both
news stories and Wikipedia articles.},
	journal = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
	author = {Cucerzan, Silviu},
	month = jun,
	year = {2007},
	keywords = {disambiguation, ner, nlp, text, text-mining},
	pages = {716, 708},
}

@article{stokes_empirical_2008,
	title = {An empirical study of the effects of \{{NLP}\} components on {Geographic} \{{IR}\} performance},
	volume = {22},
	url = {http://dx.doi.org/10.1080/13658810701626210},
	abstract = {Natural language processing (NLP) techniques, such as toponym detection and resolution, are an integral part of most geographic information retrieval (GIR) architectures. Without these components, synonym detection, ambiguity resolution and accurate toponym expansion would not be possible. However, there are many important factors affecting the success of an NLP approach to GIR, including toponym detection errors, toponym resolution errors and query overloading. The aim of this paper is to determine how severe these errors are in state-of-the-art systems, and to what extent they affect GIR performance. We show that a careful choice of weighting schemes in the IR engine can minimize the negative impact of these errors on GIR accuracy. We provide empirical evidence from the GeoCLEF 2005 and 2006 datasets to support our observations.},
	number = {3},
	journal = {International Journal of Geographical Information Science},
	author = {Stokes, Nicola and Li, Yi and Moffat, Alistair and Rong, Jiawen},
	year = {2008},
	keywords = {disambiguation, gir, nlp, text, text-mining},
	pages = {264, 247},
}

@article{buscaldi_conceptual_2008,
	title = {A conceptual density-based approach for the disambiguation of toponyms},
	volume = {22},
	url = {http://dx.doi.org/10.1080/13658810701626251},
	abstract = {Nowadays, a huge quantity of information is stored in digital format. A great portion of this information is constituted by textual and unstructured documents, where geographical references are usually given by means of place names. A common problem with textual information retrieval is represented by polysemous words, that is, words can have more than one sense. This problem is present also in the geographical domain: place names may refer to different locations in the world. In this paper we investigate the use of our word sense disambiguation technique in the geographical domain, with the aim of resolving ambiguous place names. Our technique is based on WordNet conceptual density. Due to the lack of a reference corpus tagged with WordNet senses, we carried out the experiments over a set of 1,210 place names extracted from the SemCor corpus that we named GeoSemCor and made publicly available. We compared our method with the most-frequent baseline and the enhanced-Lesk method, which previously has not been tested in large contexts. The results show that a better precision can be achieved by using a small context (phrase level), whereas a greater coverage can be obtained by using large contexts (document level). The proposed method should be tested with other corpora, due to the fact that our experiments evidenced the excessive bias towards the most-frequent sense of the GeoSemCor.},
	number = {3},
	journal = {International Journal of Geographical Information Science},
	author = {Buscaldi, Davide and Rosso, Paulo},
	year = {2008},
	keywords = {disambiguation, geography, gir, wordnet},
	pages = {313, 301},
}

@inproceedings{amitay_web--where:_2004,
	title = {Web-a-where: geotagging web content},
	isbn = {1-58113-881-4},
	shorttitle = {Web-a-where},
	url = {http://dx.doi.org/10.1145/1008992.1009040},
	abstract = {We describe Web-a-Where, a system for associating geography
with Web pages. Web-a-Where locates mentions of
places and determines the place each name refers to. In addition,
it assigns to each page a geographic focus — a locality
that the page discusses as a whole. The tagging process
is simple and fast, aimed to be applied to large collections
of Web pages and to facilitate a variety of location-based
applications and data analyses.
Geotagging involves arbitrating two types of ambiguities:
geo/non-geo and geo/geo. A geo/non-geo ambiguity occurs
when a place name also has a non-geographic meaning, such
as a person name (e.g., Berlin) or a common word (Turkey).
Geo/geo ambiguity arises when distinct places have the same
name, as in London, England vs. London, Ontario.
An implementation of the tagger within the framework
of the WebFountain data mining system is described, and
evaluated on several corpora of real Web pages. Precision of
up to 82\% on individual geotags is achieved. We also evaluate
the relative contribution of various heuristics the tagger
employs, and evaluate the focus-finding algorithm using a
corpus pretagged with localities, showing that as many as
91\% of the foci reported are correct up to the country level.},
	urldate = {2009-07-14},
	booktitle = {{SIGIR} '04: {Proceedings} of the 27th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Amitay, Einat and Har'El, Nadav and Sivan, Ron and Soffer, Aya},
	year = {2004},
	keywords = {information-retrieval},
	pages = {280, 273},
}

@inproceedings{weinberger_resolving_2008,
	title = {Resolving {Tag} {Ambiguity}},
	url = {http://research.yahoo.com/node/2335},
	abstract = {Tagging is an important way for users to succinctly describe the content they upload to the Internet. However, most tag-suggestion systems recommend words that are highly correlated with the existing tag set, and thus add little information to a user's contribution. This paper describes a means to determine the ambiguity of a set of (user-contributed) tags and suggests new tags that disambiguate the original tags. We introduce a probabilistic framework that allows us to find two tags that appear in different contexts but are both likely to co-occur with the original tag set. If such tags can be found, the current description is considered ``ambiguous'' and the two tags are recommended to the user for further clarification. In contrast to previous work, we only query the user when information is most needed and good suggestions are available. We verify the efficacy of our approach using geographical, temporal and semantic metadata, and a user study. We built our system using statistics from a large (100M) database of images and their tags.},
	booktitle = {{ACM} {Multimedia}},
	author = {Weinberger, K and Slaney, M and Zwol, R},
	year = {2008},
	keywords = {disambiguation, flickr, folksonomy, semantic, tagging, wordnet},
}

@incollection{brun_intertwining_2004,
	title = {Intertwining {Deep} {Syntactic} {Processing} and {Named} {Entity} {Detection}},
	url = {http://www.springerlink.com/content/w94fdey98t07au5d},
	abstract = {In this paper, we present a robust incremental architecture for natural language processing centered around syntactic analysis but allowing at the same time the description of specialized modules, like named entity recognition. We show that the flexibility of our approach allows us to intertwine general and specific processing, which has a mutual improvement effect on their respective results: for example, syntactic analysis clearly benefits from named entity recognition as a pre-processing step, but named entity recognition can also take advantage of deep syntactic information.},
	booktitle = {Advances in {Natural} {Language} {Processing}},
	author = {Brun, Caroline and Hagège, Caroline},
	year = {2004},
	keywords = {disambiguation, ner, nlp, text},
	pages = {206, 195},
}

@inproceedings{ourioupina_extracting_2002,
	title = {Extracting geographical knowledge from the internet},
	url = {http://www.coli.uni-saarland.de/~ourioupi/ouri_fin.pdf},
	abstract = {This paper describes an algorithm for knowledge acquisition in the Geography domain. We apply Text Mining procedures to the Internet in order to classify places into different location types (e.g., Maebashi is a CITY, Honshu is an ISLAND) and to determine for a given place name, where the place is (e.g. Maebashi is in Japan, Honshu is in the Pacific ocean). At the moment we distinguish 6 location types.
We conducted three series of experiments: with a manually tuned system, using the TiMBL Memory Based Learner, and using the C4.5 Decision Tree Induction Algorithm. The results obtained so far are quite promising: all three algorithms scored in the high eighties for all but one (CITY) class. All systems were significantly better than the baseline. That leads us to the conclusion that the approach may successfully be used to automatically create gazetteers for Named Entities Recognition tools.},
	booktitle = {International {Workshop} on {Active} {Mining}, {ACDM}-{AM}},
	author = {Ourioupina, O},
	year = {2002},
	keywords = {disambiguation, internet},
}

@article{overell_using_2008,
	title = {Using co-occurrence models for placename disambiguation},
	volume = {22},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810701626236},
	abstract = {This paper describes the generation of a model capturing information on how placenames co-occur together. The advantages of the co-occurrence model over traditional gazetteers are discussed and the problem of placename disambiguation is presented as a case study. We begin by outlining the problem of ambiguous placenames. We demonstrate how analysis of Wikipedia can be used in the generation of a co-occurrence model. The accuracy of our model is compared to a handcrafted ground truth; then we evaluate alternative methods of applying this model to the disambiguation of placenames in free text (using the GeoCLEF evaluation forum). We conclude by showing how the inclusion of placenames in both the text and geographic parts of a query provides the maximum mean average precision and outline the benefits of a co-occurrence model as a data source for the wider field of geographic information retrieval (GIR).},
	number = {3},
	journal = {Int. J. Geogr. Inf. Sci.},
	author = {Overell, Simon and R{\textbackslash}"uger, Stefan},
	year = {2008},
	keywords = {disambiguation, gir},
	pages = {287, 265},
}

@article{guo_georeferencing_2008,
	title = {Georeferencing locality descriptions and computing associated uncertainty using a probabilistic approach},
	volume = {22},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810701851420},
	number = {10},
	journal = {International Journal of Geographical Information Science},
	author = {Guo, Q and Liu, Y and Wieczorek, J},
	year = {2008},
	keywords = {disambiguation, uncertainty, web},
	pages = {1090, 1067},
}

@inproceedings{jones_geographical_2001,
	title = {Geographical {Information} {Retrieval} with {Ontologies} of {Place}},
	isbn = {3-540-42613-2},
	url = {http://portal.acm.org/citation.cfm?id=678826},
	booktitle = {{COSIT} 2001: {Proceedings} of the {International} {Conference} on {Spatial} {Information} {Theory}},
	publisher = {Springer-Verlag},
	author = {Jones, Christopher and Alani, Harith and Tudhope, Douglas},
	year = {2001},
	keywords = {disambiguation, gis, ontology},
	pages = {335, 322},
}

@article{leveling_metonymy_2008,
	title = {On metonymy recognition for geographic information retrieval},
	volume = {22},
	issn = {1365-8816},
	url = {http://dx.doi.org/10.1080/13658810701626244},
	abstract = {Metonymically used location names (toponyms) refer to other, related entities and thus possess a meaning different from their literal, geographic sense. Metonymic uses are to be treated differently to improve the performance of geographic information retrieval (GIR). Statistics on toponym senses show that 75.06\% of all location names are used in their literal sense, 17.05\% are used metonymically, and 7.89\% have a mixed sense. This article presents a method for disambiguating location names in texts between literal and metonymic senses, based on shallow features. The evaluation of this method is two-fold. First, we use a memory-based learner (TiMBL) to train a classifier and determine standard evaluation measures such as F-score and accuracy. The classifier achieved an F-score of 0.842 and an accuracy of 0.846 for identifying toponym senses in a subset of the CoNLL (Conference on Natural Language Learning) data. Second, we perform retrieval experiments based on the GeoCLEF data (newspaper article corpus and queries) from 2005 and 2006. We compare searching location names in a database index containing both their literal and metonymic senses with searching in an index containing their literal senses only. Evaluation results indicate that removing metonymic senses from the index yields a higher mean average precision (MAP) for GIR. In total, we observed a significant gain in MAP: an increase from 0.0704 to 0.0715 MAP for the GeoCLEF 2005 data, and an increase from 0.1944 to 0.2100 MAP for the GeoCLEF 2006 data.},
	number = {3},
	journal = {Int. J. Geogr. Inf. Sci.},
	author = {Leveling, Johannes and Hartrumpf, Sven},
	year = {2008},
	keywords = {disambiguation, gir},
	pages = {299, 289},
}

@inproceedings{andogah_gir_2005,
	title = {{GIR} {Experimentation}},
	url = {http://www.clef-campaign.org/2006/presentat/andogahCLEF2006-Paper.pdf},
	abstract = {Geographic named entities (GNEs) are ambiguous in two ways - a name may refer to various locations (i.e. Groningen, the Netherlands or Groningen, Germany), and a location may have many names (Holland and the Netherlands). In addition to the first type ambiguity, a name may have various senses (i.e. Groningen can refer to a city and a province) and may also refer to non-geographic entities (people and organizations). In this work we are concerned with "one name to many location" and "one name to many senses" ambiguity resolution. To resolve the first type of ambiguity, we determine in which geographical hierarchy a name can be placed (i.e. Groningen, the Netherlands, Europe vs. Groningen, Baden-Wurttemburg, Germany, Europe, ...). We assign a hierarchy on the basis of other (non-ambiguous) GNEs in context, and default locations (where population size can be used to determine the default location). The sense ambiguity is resolved on the basis of key-phrases in context (province, city), and the sense of other GNEs in context. We report on the result of preliminary experiment to test the hypothesis.},
	booktitle = {The 16th {Meeting} of {Computational} {Linguistics} in the {Netherlands} {Amsterdam}},
	author = {Andogah, Geoffrey},
	year = {2005},
	keywords = {disambiguation, ner, nlp},
}

@phdthesis{hafernik_automatic_2005,
	title = {Automatic {Methods} {To} {Disambiguate} {Geospatial} {Queries}},
	url = {http://hdl.handle.net/10090/4908},
	abstract = {Today an unprecedented amount of digital information is available, but locating information of interest can be difficult. Information Retrieval (IR) is the area of Computer Science that aims to locate information by automatically organizing, storing, and managing it. IR systems search large databases, separating non-relevant items from the relevant items, and return a list of the documents likely to match the information need as described by the user’s query. Geographical Information Retrieval (GIR) aims to develop IR systems with spatial awareness and exploit geospatial information to improve retrieval effectiveness. This research explores GIR, as in GeoCLEF, and uses geospatial information to automatically disambiguate geospatial terms. The hypothesis is that automatically disambiguating geospatial terms will improve retrieval effectiveness. Two challenges to IR and GIR are language ambiguity and improving retrieval automatically, instead of manually, by query modification. Language ambiguity can be problematic for both queries and documents because there are many ways to describe concepts or ideas. Separate documents describe the same information differently. Similarly, independent users looking for the same information may use different words in their queries. A query describing a concept in one way will fail to retrieve relevant documents that use different vocabulary. Users often fail to precisely specify information they require, which may cause the system to miss some important relevant documents. Manual approaches to query modification require a person to determine other concepts and words which not only better describe the needed information, but that others may have chosen for the same topic. This is not realistic in a real world situation. Often there is not enough time for individuals to modify the queries themselves and they might not know how to improve queries. Thus automatic methods, which can be done without a human, are needed in order for IR or GIR methods to be practical. This work aims to exploit geospatial information in queries to improve retrieval by automatically disambiguating geospatial terms within the queries using outside geospatial knowledge gathered from the internet, including city names, countries, regions, parts of countries and location information. Our approach combines simple linguistic analysis with query modification via the addition of geospatial information. Geospatial terms were chosen in several different ways. First, terms were added from retrieved documents assumed to be relevant. Another method gave higher weight to more important query words. A third procedure added terms selected from a geographic thesaurus. Finally, attempts were made to perform spatial disambiguation by using longitude and latitude to infer an upper bound on distance terms like “near.”},
	school = {Mount Holyoke},
	author = {Hafernik, Carolyn},
	year = {2005},
	keywords = {disambiguation, gir},
}

@inproceedings{schockaert_neighborhood_2007,
	title = {Neighborhood restrictions in geographic {IR}},
	isbn = {978-1-59593-597-7},
	url = {http://dx.doi.org/10.1145/1277741.1277772},
	urldate = {2009-07-14},
	booktitle = {{SIGIR} '07: {Proceedings} of the 30th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {ACM Press},
	author = {Schockaert, Steven and De Cock, Martine},
	year = {2007},
	keywords = {geocoding},
	pages = {174, 167},
}

@book{leidner_toponym_2008,
	title = {Toponym {Resolution} in {Text}: {Annotation}, {Evaluation} and {Applications} of {Spatial} {Grounding} of {Place} {Names}},
	isbn = {1-58112-384-1},
	shorttitle = {Toponym {Resolution} in {Text}},
	url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20&path=ASIN/1581123841},
	abstract = {The problem of automatic toponym resolution, or computing the mapping from
occurrences of names for places as found in a text to an unambiguous spatial
footprint of the location referred to, such as a geographic latitude/longitude
centroid is difficult to automate due to insufficient and error-prone
geographic databases, and a large degree of place name ambiguity: common words
need to be distinguished from proper names (geo/non-geo ambiguity), and the
mapping between names and locations is ambiguous (London can refer to the
capital of the UK or to London, Ontario, Canada, or to about forty other
Londons on earth). This thesis investigates how referentially ambiguous
spatial named entities can be grounded, or resolved, with respect to an
extensional coordinate model robustly on open-domain news text by collecting a
repertoire of linguistic heuristics and extra-linguistic knowledge sources
such as population. I then investigate how to combine these sources of
evidence to obtain a superior method. Noise effects introduced by the named
entity tagging that toponym resolution relies on are also studied. While few
attempts have been made to solve toponym resolution, these were either not
evaluated, or evaluation was done by manual inspection of system output
instead of creating a re-usable reference corpus. A systematic comparison
leads to an inventory of heuristics and other sources of evidence. In order to
carry out a comparative evaluation procedure, an evaluation resource is
required, so a reference gazetteer and an associated novel reference corpus
with human-labelled referent annotation were created for this thesis, to be
used to benchmark a selection of the reconstructed algorithms and a novel re-
combination of the heuristics catalogued in the inventory. Performance of the
same resolution algorithms is compared under different conditions, namely
applying it to the output of human named entity annotation and automatic
annotation using an existing Maximum Entropy sequence tagging model.},
	urldate = {2009-07-14},
	publisher = {Dissertation.Com},
	author = {Leidner, Jochen},
	month = jan,
	year = {2008},
	keywords = {geocoding, placenames, spatial, toponym},
}

@inproceedings{warren_teaching_2005,
	address = {Newcastle, New South Wales, Australia},
	title = {Teaching patterns and software design},
	isbn = {1-920682-24-4},
	url = {http://portal.acm.org/citation.cfm?id=1082424.1082430},
	abstract = {In this paper we describe our experiences with reengineering an undergraduate course in software design. The course's learning outcomes require that students can model, design and implement software. These are inherently practical skills and rely on functioning knowledge. To facilitate a learning environment in which students can acquire the necessary deep level of understanding, we have designed the course by applying the educational theory of constructive alignment and a number of proven techniques for teaching, learning, and assessment. Fundamentally, we have embraced the active learning paradigm that recognises that student activity is critical to the learning process. In this paper, we describe several active learning techniques that we have used including role play, problem solving and peer learning. We also describe two novel assessment techniques we have developed, holistic assessment and formative examination. In addition we describe how students work with JUnit, a popular unit testing tool, not as users but as developers by applying design patterns to extend it with new functionality. Finally, we report on student assessment results and relay student feedback.},
	urldate = {2009-07-13},
	booktitle = {Proceedings of the 7th {Australasian} conference on {Computing} education - {Volume} 42},
	publisher = {Australian Computer Society, Inc.},
	author = {Warren, Ian},
	year = {2005},
	keywords = {active and peer learning, constructive alignment, design patterns, formative and holistic assessment, junit, uml},
	pages = {39--49},
}

@article{nelson_teaching_2000,
	title = {Teaching computer networking using open source software},
	volume = {32},
	url = {http://portal.acm.org/citation.cfm?id=343056},
	doi = {10.1145/353519.343056},
	abstract = {For the past seven years we have taught a subject entitled Network Software and Management (NSM) for both computer science and electrical engineering students. We discuss the evolution of this subject syllabus in response to the changing requirements of the workplace environment, ever improving technology and the need to combine theory and practice in teaching subjects such as this. We used open source software exclusively in our laboratory exercises and we provide the rationale behind our choice of specific software packages.},
	number = {3},
	urldate = {2009-07-13},
	journal = {SIGCSE Bull.},
	author = {Nelson, Daniel and Ng, Yau Man},
	year = {2000},
	pages = {13--16},
}

@article{nieh_experiences_2005,
	title = {Experiences teaching operating systems using virtual platforms and linux},
	volume = {37},
	url = {http://portal.acm.org/citation.cfm?id=1047124.1047508},
	doi = {10.1145/1047124.1047508},
	abstract = {Operating system courses teach students much more when they provide hands-on kernel-level project experience with a real operating system. However, enabling a large class of students to do kernel development can be difficult. To address this problem, we created a virtual kernel development environment in which operating systems can be developed, debugged, and rebooted in a shared computer facility without affecting other users. Using virtual machines and remote display technology, our virtual kernel development laboratory enables even distance learning students at remote locations to participate in kernel development projects with on-campus students. We have successfully deployed and used our virtual kernel development environment together with the open-source Linux kernel to provide kernel-level project experiences for over nine hundred students in the introductory operating system course at Columbia University.},
	number = {1},
	urldate = {2009-07-13},
	journal = {SIGCSE Bull.},
	author = {Nieh, Jason and Vaill, Chris},
	year = {2005},
	keywords = {computer science education, open-source software, operating systems, virtual machines, virtualization},
	pages = {520--524},
}

@article{cataloglu_open_2006,
	title = {Open {Source} {Software} in {Teaching} {Physics}: {A} {Case} {Study} on {Vector} {Algebra} and {Visual} {Representations}},
	volume = {5},
	url = {http://eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?_nfpb=true&_&ERICExtSearch_SearchValue_0=ED494627&ERICExtSearch_SearchType_0=no&accno=ED494627},
	abstract = {This study aims to report the effort on teaching vector algebra using free open source software (FOSS). Recent studies showed that students have difficulties in learning basic physics concepts. Constructivist learning theories suggest the use of visual and hands-on activities in learning. We will report on the software used for this purpose. The effect of FOSS on students understanding of vector algebra was determined by a non-equivalent control group design. A total number of 113 freshman students from two classes of introductory level physics courses were involved. The experimental group's students learning processes were supplemented by instruction utilizing FOSS while the control group was taught in traditional manner. A significant difference in students' performance was found that could be attributed to the treatment. Consequently, visualization of vector and related concepts by FOSS simulations helped students to understand them well and contributed to shorten the time needed to learn these concepts.},
	number = {1},
	urldate = {2009-07-13},
	journal = {Turkish Online Journal of Educational Technology},
	author = {Cataloglu, Erdat},
	month = jan,
	year = {2006},
}

@article{jiang_selection_nodate,
	title = {Selection of {Streets} from a {Network} {Using} {Self}-{Organizing} {Maps}},
	volume = {8},
	doi = {10.1111/j.1467-9671.2004.00186.x},
	number = {3},
	journal = {Transactions in GIS},
	author = {Jiang, Bin},
	pages = {335--350},
}

@article{hornbaek_navigation_nodate,
	title = {Navigation patterns and usability of zoomable user interfaces with and without an overview},
	volume = {9},
	issn = {1073-0516},
	doi = {10.1145/586081.586086},
	number = {4},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Hornbæk, Kasper},
	pages = {362--389},
}

@article{knutson_applying_nodate,
	title = {Applying the {Small}-{Area} {Estimation} {Method} to {Estimate} a {Population} {Eligible} for {Breast} {Cancer} {Detection} {Services}},
	volume = {5},
	number = {1},
	journal = {Preventing Chronic Disease},
	author = {Knutson, Kirsten},
}

@article{suarez_shaping_nodate,
	title = {Shaping the {Future} of {GI} {Science} in {Europe}},
	volume = {11},
	issn = {1361-1682},
	doi = {10.1111/j.1467-9671.2007.01082.x},
	number = {6},
	journal = {Transactions in GIS},
	author = {Suarez, Juan},
	pages = {795--797},
}

@article{zhang_text_nodate,
	title = {Text {Categorization} {Based} on {Regularized} {Linear} {Classification} {Methods}},
	volume = {4},
	number = {1},
	journal = {Information Retrieval},
	author = {Zhang, Tong},
	pages = {5--31},
}

@article{parker_multi-agent_nodate,
	title = {Multi-{Agent} {Systems} for the {Simulation} of {Land}-{Use} and {Land}-{Cover} {Change}: {A} {Review}},
	volume = {93},
	shorttitle = {Multi-{Agent} {Systems} for the {Simulation} of {Land}-{Use} and {Land}-{Cover} {Change}},
	doi = {10.1111/1467-8306.9302004},
	number = {2},
	journal = {Annals of the Association of American Geographers},
	author = {Parker, Dawn},
	pages = {314--337},
}

@article{smith_disease_nodate,
	title = {Disease {Cluster} {Detection} {Methods}: {The} {Impact} of {Choice} of {Shape} on the {Power} of {Statistical} {Tests}},
	shorttitle = {Disease {Cluster} {Detection} {Methods}},
	author = {Smith, Geoffrey},
}
